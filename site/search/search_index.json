{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OSLLM.ai","text":"<p>Welcome to the official documentation for OSLLM.ai. This documentation covers everything you need to know about our suite of tools and services.</p>"},{"location":"#our_products","title":"Our Products","text":""},{"location":"#indox","title":"Indox","text":"<p>Our comprehensive document processing and analysis toolkit.</p>"},{"location":"#indoxjudge","title":"IndoxJudge","text":"<p>Advanced evaluation and benchmarking system for LLMs and RAG systems.</p>"},{"location":"#indoxgen","title":"IndoxGen","text":"<p>Powerful data synthesis and generation tools.</p>"},{"location":"#indoxminer","title":"IndoxMiner","text":"<p>Intelligent data extraction and classification system.</p>"},{"location":"#getting_started","title":"Getting Started","text":"<p>Choose a product from the navigation menu to get started with detailed documentation, tutorials, and examples.</p> <p>For quick start guides: - Indox Quick Start - IndoxJudge Overview - IndoxGen Basics - IndoxMiner Guide </p>"},{"location":"license/","title":"License","text":""},{"location":"license/#osllmai_software_license","title":"OSLLM.ai Software License","text":"<p>Copyright (c) 2024 OSLLM.ai</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. </p>"},{"location":"indoxArcg/","title":"IndoxArcg","text":"<p>IndoxArcg is an innovative application designed to streamline information extraction from a wide range of document types, including text files, PDF, HTML, Markdown, and LaTeX. Whether structured or unstructured, Indox provides users with a powerful toolset to efficiently extract relevant data.</p> <p>Indox Retrieval Augmentation is an innovative application designed to streamline information extraction from a wide range of document types, including text files, PDF, HTML, Markdown, and LaTeX. Whether structured or unstructured, Indox provides users with a powerful toolset to efficiently extract relevant data. One of its key features is the ability to intelligently cluster primary chunks to form more robust groupings, enhancing the quality and relevance of the extracted information. With a focus on adaptability and user-centric design, Indox aims to deliver future-ready functionality with more features planned for upcoming releases. Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs.</p>"},{"location":"indoxArcg/#join_us","title":"Join Us","text":"<p>Join us in exploring how Indox can revolutionize your document processing workflow, bringing clarity and organization to your data retrieval needs.</p>"},{"location":"indoxArcg/embedding_models/","title":"Embedding Models","text":"<p>indoxArcg supports multiple state-of-the-art embedding models for text representation. This guide provides detailed instructions for configuring and using each supported model.</p>"},{"location":"indoxArcg/embedding_models/#table_of_contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Supported Models</li> <li>Model Configuration Guides</li> <li>OpenAI</li> <li>Azure OpenAI</li> <li>Hugging Face</li> <li>NerdToken</li> <li>Mistral</li> <li>Clarifai</li> <li>Cohere</li> <li>Elasticsearch</li> <li>GPT4All</li> <li>Ollama</li> <li>Future Development</li> </ul>"},{"location":"indoxArcg/embedding_models/#prerequisites","title":"Prerequisites","text":"<p>Before using any embedding models:</p> <ol> <li>Python 3.8+ installed</li> <li>Install required packages:    <pre><code>pip install python-dotenv\n</code></pre></li> </ol> <p>Create a <code>.env</code> file in your project root with relevant API keys:</p> <pre><code>OPENAI_API_KEY=your_key_here\nNERDTOKEN_API_KEY=your_key_here\nMISTRAL_API_KEY=your_key_here\n# Add other API keys as needed\n</code></pre>"},{"location":"indoxArcg/embedding_models/#supported_models","title":"Supported Models","text":"# Model Provider Class Name Requirements 1 OpenAI OpenAiEmbedding pip install openai 2 Azure OpenAI AzureOpenAIEmbeddings pip install openai 3 Hugging Face HuggingFaceEmbedding pip install sentence-transformers 4 NerdToken NerdTokenEmbedding API key required 5 Mistral MistralEmbedding pip install mistralai 6 Clarifai ClarifaiEmbeddings pip install clarifai 7 Cohere CohereEmbeddings pip install cohere 8 Elasticsearch ElasticsearchEmbeddings Elasticsearch instance running 9 GPT4All GPT4AllEmbeddings pip install gpt4all 10 Ollama OllamaEmbeddings Local Ollama server running"},{"location":"indoxArcg/embedding_models/#model_configuration_guides","title":"Model Configuration Guides","text":""},{"location":"indoxArcg/embedding_models/#1_openai_embedding_model","title":"1. OpenAI Embedding Model","text":"<p>Recommended Models: text-embedding-3-small, text-embedding-3-large</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxArcg.embeddings import OpenAiEmbedding\n\nload_dotenv()\n\nembeddings = OpenAiEmbedding(\n    model=\"text-embedding-3-small\",\n    api_key=os.getenv('OPENAI_API_KEY') # Required\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#2_azure_openai_embedding_model","title":"2. Azure OpenAI Embedding Model","text":"<p>Required Parameters:</p> <ul> <li>azure_endpoint: Your Azure deployment endpoint</li> <li>deployment: Your model deployment name</li> <li>api_version: API version (e.g., \"2023-05-15\")</li> </ul> <pre><code>from indoxArcg.embeddings import AzureOpenAIEmbeddings\n\nazure_embeddings = AzureOpenAIEmbeddings(\n    azure_endpoint=\"https://your-resource.openai.azure.com\",\n    deployment=\"your-deployment-name\",\n    api_version=\"2023-05-15\",\n    api_key=os.getenv('AZURE_OPENAI_KEY'),\n    model=\"text-embedding-3-small\"\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#3_hugging_face_embedding_models","title":"3. Hugging Face Embedding Models","text":"<p>Popular Models:</p> <ul> <li>sentence-transformers/all-MiniLM-L6-v2</li> <li>multi-qa-mpnet-base-cos-v1</li> </ul> <pre><code>pip install sentence-transformers\n</code></pre> <pre><code>from indoxArcg.embeddings import HuggingFaceEmbedding\n\nhf_embeddings = HuggingFaceEmbedding(\n    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    api_key=os.getenv('HUGGING_FACE_API_KEY')  # Optional\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#4_nerdtoken_embedding_models","title":"4. NerdToken Embedding Models","text":"<pre><code>from indoxArcg.embeddings import NerdTokenEmbedding\n\nnt_embeddings = NerdTokenEmbedding(\n    api_key=os.getenv('NERDTOKEN_API_KEY'),\n    model=\"text-embedding-3-small\",\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#5_mistral_embedding_model","title":"5. Mistral Embedding Model","text":"<p>Current Model: mistral-embed (1280-dimension embeddings)</p> <pre><code>from indoxArcg.embeddings import MistralEmbedding\n\nmistral_embeddings = MistralEmbedding(\n    api_key=os.getenv('MISTRAL_API_KEY'),\n    model=\"mistral-embed\",\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#6_clarifai_embedding_model","title":"6. Clarifai Embedding Model","text":"<p>Finding Model IDs:</p> <ol> <li>Log in to Clarifai Portal</li> <li>Navigate to your application</li> <li>Copy model ID from model details</li> </ol> <pre><code>from indoxArcg.embeddings import ClarifaiEmbeddings\n\nclarifai_embeddings = ClarifaiEmbeddings(\n    pat=os.getenv('CLARIFAI_PAT'),\n    model_id=\"your-clarifai-model-id\",\n    user_id=\"clarifai-user-id\",\n    app_id=\"your-application-id\"\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#7_cohere_embedding_model","title":"7. Cohere Embedding Model","text":"<p>Recommended Models: embed-english-v3.0, embed-multilingual-v3.0</p> <pre><code>from indoxArcg.embeddings import CohereEmbeddings\n\ncohere_embeddings = CohereEmbeddings(\n    api_key=os.getenv('COHERE_API_KEY'),\n    model_name=\"embed-english-v3.0\",\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#8_elasticsearch_embedding_model","title":"8. Elasticsearch Embedding Model","text":"<p>Prerequisites:</p> <ul> <li>Running Elasticsearch cluster (version 8.0+)</li> <li>Deployed embedding model via Eland</li> </ul> <pre><code>pip install elasticsearch\n</code></pre> <pre><code>from elasticsearch import Elasticsearch\nfrom indoxArcg.embeddings import ElasticsearchEmbeddings\n\nes = Elasticsearch(\n    hosts=[\"http://localhost:9200\"],\n    basic_auth=(\"username\", \"password\")\n)\n\nes_embeddings = ElasticsearchEmbeddings(\n    client=es,\n    model_id=\".multilingual-e5-small\"  # Example model ID\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#9_gpt4all_embedding_model","title":"9. GPT4All Embedding Model","text":"<p>Available Models:</p> <ul> <li>all-MiniLM-L6-v2</li> <li>gpt4all-lora</li> </ul> <pre><code>from indoxArcg.embeddings import GPT4AllEmbeddings\n\ngpt4all_embeddings = GPT4AllEmbeddings(\n    model_name=\"all-MiniLM-L6-v2\",\n    device=\"nvidia\"  # or \"cpu\", \"amd\", \"intel\"\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#10_ollama_embedding_model","title":"10. Ollama Embedding Model","text":"<p>Setup:</p> <pre><code>curl -fsSL https://ollama.ai/install.sh | sh\nollama pull llama2\n</code></pre> <pre><code>from indoxArcg.embeddings import OllamaEmbeddings\n\nollama_embeddings = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=\"llama2\",\n    temperature=0.3  # Control randomness\n)\n</code></pre>"},{"location":"indoxArcg/embedding_models/#future_development","title":"Future Development","text":""},{"location":"indoxArcg/embedding_models/#future-development","title":"Future Development","text":"<p>Planned enhancements include:</p> <ul> <li>Integration with Google Vertex AI embedding models</li> <li>Support for multimodal embeddings (image+text)</li> <li>Batch embedding generation optimizations</li> <li>Dynamic model selection based on content type</li> <li>Enhanced error handling and retry mechanisms</li> </ul> <p>For feature requests or issues, please visit our GitHub repository.</p>"},{"location":"indoxArcg/llms/","title":"Language Models","text":"<p>indoxArcg provides unified access to state-of-the-art LLMs through a consistent interface. All models implement common methods for question answering, document grading, and hallucination checking.</p>"},{"location":"indoxArcg/llms/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Supported Models</li> <li>Common Interface</li> <li>Model Configuration Guides</li> <li>OpenAI</li> <li>Mistral</li> <li>Hugging Face</li> <li>Google AI</li> <li>Ollama</li> <li>DeepSeek</li> <li>NerdToken</li> <li>Azure OpenAI</li> <li>Local Inference</li> <li>Troubleshooting</li> <li>Future Development</li> </ul>"},{"location":"indoxArcg/llms/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.8+ environment</li> <li>Install base package:    <pre><code>pip install python-dotenv\n</code></pre></li> </ol>"},{"location":"indoxArcg/llms/#env_file_with_api_keys","title":".env file with API keys:","text":"<pre><code>OPENAI_API_KEY=your_key\nMISTRAL_API_KEY=your_key\nGOOGLE_API_KEY=your_key\nHF_API_KEY=your_key\n</code></pre>"},{"location":"indoxArcg/llms/#supported-models","title":"Supported Models","text":"# Provider Class Name Requirements 1 OpenAI OpenAi pip install openai 2 Mistral Mistral pip install mistralai 3 Hugging Face HuggingFaceAPIModel pip install requests 4 Google AI GoogleAi pip install google-generativeai 5 Ollama Ollama Local Ollama server 6 DeepSeek DeepSeek pip install openai 7 NerdToken NerdToken API key required 8 Azure OpenAI AzureOpenAi Azure deployment"},{"location":"indoxArcg/llms/#common-interface","title":"Common Interface","text":"<p>All LLM classes implement these core methods:</p> <pre><code>class BaseLLM:\n    def answer_question(self, context: str, question: str) -&gt; str: ...\n    def get_summary(self, documentation: str) -&gt; str: ...\n    def grade_docs(self, context: list, question: str) -&gt; list: ...\n    def check_hallucination(self, context: str, answer: str) -&gt; str: ...\n    def chat(self, prompt: str, system_prompt: str) -&gt; str: ...\n</code></pre>"},{"location":"indoxArcg/llms/#model-configuration-guides","title":"Model Configuration Guides","text":""},{"location":"indoxArcg/llms/#1-openai","title":"1. OpenAI","text":"<p>Recommended Models: gpt-4-turbo, gpt-3.5-turbo</p> <pre><code>from indoxArcg.llms import OpenAi\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nllm = OpenAi(\n    api_key=os.getenv('OPENAI_API_KEY'),\n    model=\"gpt-4-turbo\",\n)\n\nresponse = llm.answer_question(\n    context=\"Climate change refers to...\",\n    question=\"What are main causes of global warming?\"\n    temperature=0.3\n)\n</code></pre>"},{"location":"indoxArcg/llms/#2-mistral","title":"2. Mistral","text":"<p>Recommended Models: mistral-large-latest, codestral-latest</p> <pre><code>from indoxArcg.llms import Mistral\n\nmistral_llm = Mistral(\n    api_key=os.getenv('MISTRAL_API_KEY'),\n    model=\"mistral-large-latest\",\n)\n</code></pre>"},{"location":"indoxArcg/llms/#3-hugging-face","title":"3. Hugging Face","text":"<pre><code>from indoxArcg.llms import HuggingFaceAPIModel\n\nhf_llm = HuggingFaceAPIModel(\n    api_key=os.getenv('HF_API_KEY'),\n    model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n    prompt_template=\"[INST] {context}\\nQuestion: {question} [/INST]\"\n)\n</code></pre>"},{"location":"indoxArcg/llms/#4-google-ai","title":"4. Google AI","text":"<pre><code>from indoxArcg.llms import GoogleAi\n\ngemini = GoogleAi(\n    api_key=os.getenv('GOOGLE_API_KEY'),\n    model=\"gemini-1.5-flash\",\n)\n</code></pre>"},{"location":"indoxArcg/llms/#5-ollama","title":"5. Ollama","text":"<p>Setup:</p> <pre><code>curl -fsSL https://ollama.ai/install.sh | sh\nollama pull llama2\n</code></pre> <pre><code>from indoxArcg.llms import Ollama\n\nlocal_llm = Ollama(\n    model=\"llama2:13b\",\n)\n</code></pre>"},{"location":"indoxArcg/llms/#6-deepseek","title":"6. DeepSeek","text":"<pre><code>from indoxArcg.llms import DeepSeek\n\nds_llm = DeepSeek(\n    api_key=\"your_deepseek_key\",\n    model=\"deepseek-chat\",\n)\n</code></pre>"},{"location":"indoxArcg/llms/#7-nerdtoken","title":"7. NerdToken","text":"<pre><code>from indoxArcg.llms import NerdToken\n\nnt_llm = NerdToken(\n    api_key=os.getenv('NERDTOKEN_API_KEY'),\n    model=\"openai/gpt-4o-turbo\",\n)\n</code></pre>"},{"location":"indoxArcg/llms/#8-azure-openai","title":"8. Azure OpenAI","text":"<pre><code>from indoxArcg.llms import AzureOpenAi\n\nazure_llm = AzureOpenAi(\n    api_key=os.getenv('AZURE_OPENAI_KEY'),\n    endpoint=\"https://your-resource.openai.azure.com\",\n    deployment_name=\"gpt-4-turbo-deployment\",\n    api_version=\"2024-02-01\"\n)\n</code></pre>"},{"location":"indoxArcg/llms/#9-local-inference-with-huggingface","title":"9. Local Inference with HuggingFace","text":"<p>For private/offline use with 4-bit quantization:</p> <pre><code>from indoxArcg.llms import HuggingFaceLocalModel\n\nlocal_llm = HuggingFaceLocalModel(\n    hf_model_id=\"BioMistral/BioMistral-7B\",\n    device_map=\"auto\",\n    bnb_4bit_quant_type=\"nf4\",\n    max_new_tokens=512\n)\n\nresponse = local_llm.answer_question(\n    context=\"Mitochondria are...\",\n    question=\"What is the function of mitochondria?\"\n)\n</code></pre> <p>Requirements:</p> <pre><code>pip install torch transformers accelerate bitsandbytes\n</code></pre>"},{"location":"indoxArcg/llms/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues:</p> <ul> <li>APIError: Invalid API Key: Verify .env file loading and key permissions</li> <li>Timeout Errors: Increase timeout parameter for local models</li> <li>CUDA Out of Memory: Reduce max_new_tokens or use smaller model</li> <li>Formatting Issues: Adjust prompt_template for model-specific formats</li> </ul>"},{"location":"indoxArcg/llms/#future-development","title":"Future Development","text":"<p>Planned enhancements:</p> <ul> <li>Streaming response support for all models</li> <li>Automatic model fallback strategies</li> <li>Integrated token counting</li> <li>Advanced caching mechanisms</li> <li>Multi-modal capabilities (image+text)</li> </ul>"},{"location":"indoxArcg/quick_start/","title":"Quick Start","text":""},{"location":"indoxArcg/quick_start/#quick_start","title":"Quick Start","text":""},{"location":"indoxArcg/quick_start/#overview","title":"Overview","text":"<p>This documentation provides a detailed explanation of how to use the <code>IndoxRetrievalAugmentation</code> package for QA model and embedding selection, document splitting, and storing in a vector store.</p>"},{"location":"indoxArcg/quick_start/#setup","title":"Setup","text":""},{"location":"indoxArcg/quick_start/#install_the_required_packages","title":"Install the Required Packages","text":"<pre><code>!pip install indoxArcg\n!pip install openai\n!pip install chromadb\n</code></pre>"},{"location":"indoxArcg/quick_start/#load_environment_variables","title":"Load Environment Variables","text":"<p>To start, you need to load your API keys from the environment.</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre>"},{"location":"indoxArcg/quick_start/#import_indoxarcg_package","title":"Import indoxArcg Package","text":"<p>Import the necessary classes from the indoxArcg package.</p> <pre><code>from indoxArcg.pipelines.rag import RAG\n</code></pre>"},{"location":"indoxArcg/quick_start/#importing_llm_and_embedding_models","title":"Importing LLM and Embedding Models","text":"<pre><code>from indoxArcg.llms import OpenAi\n</code></pre> <pre><code>from indoxArcg.embeddings import OpenAiEmbedding\n</code></pre>"},{"location":"indoxArcg/quick_start/#initialize_indoxarcg","title":"Initialize indoxArcg","text":"<pre><code>openai_qa = OpenAiQA(api_key=OPENAI_API_KEY,model=\"gpt-3.5-turbo-0125\")\nopenai_embeddings = OpenAiEmbedding(model=\"text-embedding-3-small\",openai_api_key=OPENAI_API_KEY)\n</code></pre> <pre><code>file_path = \"sample.txt\"\n</code></pre> <p>In this section, we take advantage of the <code>unstructured</code> library to load documents and split them into chunks by title. This method helps in organizing the document into manageable sections for further processing.</p> <pre><code>from indoxArcg.data_loader_splitter import UnstructuredLoadAndSplit\n</code></pre> <pre><code>loader_splitter = UnstructuredLoadAndSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <pre><code>Starting processing...\nEnd Chunking process.\n</code></pre> <p>Storing document chunks in a vector store is crucial for enabling efficient retrieval and search operations. By converting text data into vector representations and storing them in a vector store, you can perform rapid similarity searches and other vector-based operations.</p> <pre><code>from indoxArcg.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(collection_name=\"sample\",embedding=embed_openai)\nindoxArcg.connect_to_vectorstore(db)\nindoxArcg.store_in_vectorstore(docs)\n</code></pre> <pre><code>2024-05-14 15:33:04,916 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n2024-05-14 15:33:12,587 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-05-14 15:33:13,574 - INFO - Document added successfully to the vector store.\n\nConnection established successfully.\n\n&lt;indoxArcg.vectorstore.ChromaVectorStore at 0x28cf9369af0&gt;\n</code></pre>"},{"location":"indoxArcg/quick_start/#quering","title":"Quering","text":"<pre><code>query = \"how cinderella reach her happy ending?\"\n</code></pre> <pre><code>retriever = RAG(vector_database=db,llm=openai_qa)\nretriever.infer(query,,top_k=5)\n</code></pre> <pre><code>2024-05-14 15:34:55,380 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n2024-05-14 15:35:01,917 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n'Cinderella reached her happy ending by enduring mistreatment from her step-family, finding solace and help from the hazel tree and the little white bird, attending the royal festival where the prince recognized her as the true bride, and ultimately fitting into the golden shoe that proved her identity. This led to her marrying the prince and living happily ever after.'\n</code></pre> <pre><code>\n</code></pre>"},{"location":"indoxArcg/data_connectors/","title":"Data Connectors in indoxArcg","text":"<p>This documentation covers integrations with external platforms and services, organized by domain.</p>"},{"location":"indoxArcg/data_connectors/#categories","title":"Categories","text":""},{"location":"indoxArcg/data_connectors/#1_social_communication","title":"1. Social &amp; Communication","text":"<p>Real-time interaction platforms - Twitter - Social media trends - Discord - Community chat - Google Chat - Team messaging  </p>"},{"location":"indoxArcg/data_connectors/#2_academic_research","title":"2. Academic Research","text":"<p>Scholarly content sources - arXiv - Scientific papers - Wikipedia - Crowdsourced knowledge - Gutenberg - Classic literature  </p>"},{"location":"indoxArcg/data_connectors/#3_google_workspace","title":"3. Google Workspace","text":"<p>Google ecosystem integration - Google Docs - Collaborative writing - Google Drive - File management - Google Sheets - Tabular data  </p>"},{"location":"indoxArcg/data_connectors/#4_multimedia_content","title":"4. Multimedia Content","text":"<p>Rich media handling - YouTube - Video analysis - Maps Search - Location data  </p>"},{"location":"indoxArcg/data_connectors/#5_development_tools","title":"5. Development Tools","text":"<p>Code collaboration - GitHub - Repository management  </p>"},{"location":"indoxArcg/data_connectors/#comparison_guide","title":"Comparison Guide","text":"Category Data Freshness Rate Limits Auth Complexity indoxArcg Use Cases Social &amp; Communication Real-time Strict OAuth2 Trend analysis, moderation Academic Research Static None Low Literature reviews, Q&amp;A Google Workspace Live sync Quotas GCP Auth Collaborative RAG Multimedia Varies API Keys Medium Video analysis, geospatial Development Event-driven Strict PAT Tokens Codebase analysis"},{"location":"indoxArcg/data_connectors/#implementation_workflow","title":"Implementation Workflow","text":"<ol> <li>Select Platform: Choose connector category</li> <li>Configure Auth: Set up API keys/OAuth</li> <li>Define Scope: Select data types (posts/files/issues)</li> <li>Sync Data: Continuous or batch ingestion</li> </ol>"},{"location":"indoxArcg/data_connectors/#key_considerations","title":"Key Considerations","text":"<ul> <li>Rate Limits: Social/media APIs have strict quotas</li> <li>Auth Requirements: Google/GitHub need token management</li> <li>Data Formats: YouTube requires video/text processing</li> </ul>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/","title":"Academic &amp; Research Connectors in indoxArcg","text":"<p>This guide covers integrations with academic databases and research repositories for scholarly content ingestion.</p>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#supported_connectors","title":"Supported Connectors","text":""},{"location":"indoxArcg/data_connectors/Academic-Connectors/#1_arxivreader","title":"1. ArxivReader","text":"<p>arXiv preprint paper retrieval</p>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#features","title":"Features","text":"<ul> <li>Paper metadata extraction</li> <li>Abstract/content fetching</li> <li>Citation context analysis</li> </ul> <pre><code>from indoxArcg.data_connectors import ArxivReader\n\nreader = ArxivReader()\ndocs = reader.load_data(\n    paper_ids=[\"2201.08239\", \"2203.02155\"],\n    include_full_text=True\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#installation","title":"Installation","text":"<pre><code>pip install arxiv\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#2_wikipediareader","title":"2. WikipediaReader","text":"<p>Encyclopedic knowledge base access</p>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#features_1","title":"Features","text":"<ul> <li>Page content extraction</li> <li>Disambiguation handling</li> <li>Multilingual support</li> </ul> <pre><code>from indoxArcg.data_connectors import WikipediaReader\n\nreader = WikipediaReader()\ndocs = reader.load_data(\n    pages=[\"Large language model\", \"Transformer (machine learning model)\"],\n    lang=\"en\"\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#installation_1","title":"Installation","text":"<pre><code>pip install wikipedia\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#3_gutenbergreader","title":"3. GutenbergReader","text":"<p>Public domain literary works</p>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#features_2","title":"Features","text":"<ul> <li>Full-text book retrieval</li> <li>Metadata-aware search</li> <li>Classic literature corpus</li> </ul> <pre><code>from indoxArcg.data_connectors import GutenbergReader\n\nreader = GutenbergReader()\n# Get book by ID\nalice = reader.get_book(\"11\")\n# Search by query\nresults = reader.search_gutenberg(\"Shakespeare\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#installation_2","title":"Installation","text":"<pre><code>pip install requests beautifulsoup4\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#comparison_table","title":"Comparison Table","text":"Connector Content Type Access Method Format Rate Limits ArxivReader Research Papers API (arXiv IDs) PDF/TeX 1 req/3 seconds WikipediaReader Encyclopedia API (Page Titles) HTML/Markdown 500 req/minute GutenbergReader Books Web Scraping Plain Text 5 req/second"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#common_operations","title":"Common Operations","text":""},{"location":"indoxArcg/data_connectors/Academic-Connectors/#bulk_paper_fetching","title":"Bulk Paper Fetching","text":"<pre><code># arXiv bulk download\npaper_ids = [f\"2305.{10000+i}\" for i in range(50)]\nchunk_size = 10\ndocs = []\nfor i in range(0, len(paper_ids), chunk_size):\n    docs += reader.load_data(paper_ids[i:i+chunk_size])\n    time.sleep(30)  # Respect rate limits\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#literature_search","title":"Literature Search","text":"<pre><code># Gutenberg advanced search\nresults = reader.search_gutenberg(\n    query=\"19th century astronomy\",\n    language=\"en\",\n    file_type=[\"txt\", \"html\"]\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#metadata_enrichment","title":"Metadata Enrichment","text":"<pre><code># Extract references from papers\nfor doc in docs:\n    references = re.findall(r'\\bdoi:\\d+\\.\\d+/\\S+', doc.content)\n    doc.metadata['references'] = references\n</code></pre>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>arXiv Rate Limits <pre><code>import time\ntry:\n    docs = reader.load_data(...)\nexcept HTTPError as e:\n    if e.status == 403:\n        time.sleep(300)  # Wait 5 minutes\n</code></pre></p> </li> <li> <p>Wikipedia Disambiguation <pre><code>from wikipedia import DisambiguationError\ntry:\n    docs = reader.load_data(pages=[\"AI\"])\nexcept DisambiguationError as e:\n    print(f\"Possible options: {e.options}\")\n</code></pre></p> </li> <li> <p>Gutenberg Parsing Issues <pre><code># Raw content handling\nbook = reader.get_book(\"11\", raw=True)\nclean_text = re.sub(r'\\n{3,}', '\\n\\n', book.content)\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#best_practices","title":"Best Practices","text":"<ol> <li>arXiv</li> <li>Use arXiv IDs instead of titles for precision</li> <li>Cache responses to avoid redundant fetches</li> <li> <p>Combine with PDF parsers for full-text analysis</p> </li> <li> <p>Wikipedia</p> </li> <li>Specify language codes for multilingual content</li> <li>Use page sections for chunked processing</li> <li> <p>Handle redirects with <code>auto_suggest=False</code></p> </li> <li> <p>Gutenberg</p> </li> <li>Prefer TXT format over HTML for cleaner text</li> <li>Use metadata filters for era/genre selection</li> <li>Combine with NLP models for classical text analysis</li> </ol>"},{"location":"indoxArcg/data_connectors/Academic-Connectors/#security_considerations","title":"Security Considerations","text":"<ul> <li>Avoid storing raw API responses with sensitive metadata</li> <li>Use caching mechanisms to reduce API calls</li> <li>Validate user inputs to prevent injection attacks</li> <li>Respect Project Gutenberg's terms of service ```</li> </ul>"},{"location":"indoxArcg/data_connectors/Development-Connectors/","title":"Development &amp; Codebase Connectors in indoxArcg","text":"<p>This guide covers integrations with development platforms and version control systems for codebase analysis and documentation.</p>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#supported_connectors","title":"Supported Connectors","text":""},{"location":"indoxArcg/data_connectors/Development-Connectors/#1_githubrepositoryreader","title":"1. GithubRepositoryReader","text":"<p>GitHub repository content ingestion</p>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#features","title":"Features","text":"<ul> <li>File/folder filtering</li> <li>Branch selection</li> <li>Code metadata extraction</li> <li>Size-aware chunking</li> </ul> <pre><code>from indoxArcg.data_connectors import GithubClient, GithubRepositoryReader\n\n# Authenticate with fine-grained token\nclient = GithubClient(github_token=os.getenv(\"GH_TOKEN\"))\n\n# Configure repository reader\nrepo_reader = GithubRepositoryReader(\n    github_client=client,\n    owner=\"openai\",\n    repo=\"gpt-4\",\n    filter_directories=([\"src\"], \"INCLUDE\"),\n    filter_file_extensions=([\".py\", \".md\"], \"INCLUDE\")\n)\n\n# Load from main branch\ndocs = repo_reader.load_data(branch=\"main\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#installation","title":"Installation","text":"<pre><code>pip install PyGithub\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#key_capabilities","title":"Key Capabilities","text":"Feature Description File Types Code, Docs, Configs Access Scope Public/Private Repos Authentication Personal Access Tokens Rate Limits 5000 req/hour (authenticated) Max File Size 100MB (API limit) Version Control Branch/Tag selection"},{"location":"indoxArcg/data_connectors/Development-Connectors/#authentication_setup","title":"Authentication Setup","text":"<ol> <li>Create GitHub Personal Access Token:</li> <li>Scopes: <code>repo</code> (full control of private repos)</li> <li>Fine-grained: <code>Contents: Read-only</code></li> <li>Set environment variable: <pre><code>export GH_TOKEN=\"your_token_here\"\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_connectors/Development-Connectors/#filtering_strategies","title":"Filtering Strategies","text":"<pre><code># Include only test directories\nfilter_dirs=([\"tests\", \"spec\"], \"INCLUDE\")\n\n# Exclude binary files\nfilter_ext=([\".png\", \".jar\"], \"EXCLUDE\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#handling_large_repos","title":"Handling Large Repos","text":"<pre><code>repo_reader = GithubRepositoryReader(\n    max_file_size=50000,  # 50KB\n    parallel_fetch=True,\n    workers=4\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#common_operations","title":"Common Operations","text":""},{"location":"indoxArcg/data_connectors/Development-Connectors/#code_analysis","title":"Code Analysis","text":"<pre><code># Find TODO comments across codebase\ntodos = [doc for doc in docs if \"TODO:\" in doc.content]\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#documentation_processing","title":"Documentation Processing","text":"<pre><code># Extract all Markdown docs\ndocs = [doc for doc in docs if doc.metadata['file_path'].endswith(\".md\")]\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#commit_correlation","title":"Commit Correlation","text":"<pre><code># Add commit history metadata\nfor doc in docs:\n    doc.metadata['last_commit'] = get_commit_history(doc.metadata['file_path'])\n</code></pre>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_connectors/Development-Connectors/#common_issues","title":"Common Issues","text":"<ol> <li>Authentication Failures</li> <li>Verify token has <code>repo</code> scope</li> <li>Check token expiration date</li> <li> <p>Use fine-grained tokens for better security</p> </li> <li> <p>Rate Limits <pre><code>from github import RateLimitExceededException\n\ntry:\n    docs = repo_reader.load_data()\nexcept RateLimitExceededException:\n    print(\"API rate limit exceeded - wait before retrying\")\n</code></pre></p> </li> <li> <p>Large File Handling <pre><code>GithubRepositoryReader(\n    skip_large_files=True,\n    size_threshold=100000  # 100KB\n)\n</code></pre></p> </li> <li> <p>Private Repo Access</p> </li> <li>Ensure token has proper permissions</li> <li>Verify org SSO is enabled if required</li> </ol>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#security_best_practices","title":"Security Best Practices","text":"<ol> <li>Use fine-grained access tokens</li> <li>Never commit tokens to code</li> <li>Limit token permissions to read-only</li> <li>Rotate tokens quarterly</li> <li>Use .env files for local development</li> </ol>"},{"location":"indoxArcg/data_connectors/Development-Connectors/#performance_tips","title":"Performance Tips","text":"<ul> <li>Use directory filtering to reduce scope</li> <li>Enable parallel fetching for large repos</li> <li>Cache frequently accessed repositories</li> <li>Combine with code parsers (AST analysis)</li> <li>Utilize GitHub's Content API pagination</li> </ul>"},{"location":"indoxArcg/data_connectors/Google-Connectors/","title":"Google Workspace Connectors in indoxArcg","text":"<p>This guide covers integrations with Google Workspace services for document management and collaboration.</p>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#supported_connectors","title":"Supported Connectors","text":""},{"location":"indoxArcg/data_connectors/Google-Connectors/#1_googledoc","title":"1. GoogleDoc","text":"<p>Google Docs content retrieval</p>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#features","title":"Features","text":"<ul> <li>Full document text extraction</li> <li>Paragraph-level metadata</li> <li>Real-time collaboration awareness</li> </ul> <pre><code>from indoxArcg.data_connectors import GoogleDoc\n\ndoc = GoogleDoc(creds_file='token.json')\ncontent = doc.read(\"1aBcDeFgHiJkLmNoPqRsTuVwXyZ\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#installation","title":"Installation","text":"<pre><code>pip install google-api-python-client google-auth\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#2_googledrive","title":"2. GoogleDrive","text":"<p>Cloud file storage integration</p>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#features_1","title":"Features","text":"<ul> <li>Multi-format support (Docs, Sheets, PDF, etc.)</li> <li>File metadata extraction</li> <li>Version history access</li> </ul> <pre><code>from indoxArcg.data_connectors import GoogleDrive\n\ndrive = GoogleDrive()\ndrive.read(\"0B9qHj-hJ5W-rdVJXRlZzVUJtWVE\", mime_type=\"application/pdf\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#installation_1","title":"Installation","text":"<pre><code>pip install PyPDF2 python-pptx python-docx beautifulsoup4\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#3_googlesheet","title":"3. GoogleSheet","text":"<p>Spreadsheet data processing</p>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#features_2","title":"Features","text":"<ul> <li>Range-based data extraction</li> <li>Formula resolution</li> <li>Tabular metadata preservation</li> </ul> <pre><code>from indoxArcg.data_connectors import GoogleSheet\n\nsheet = GoogleSheet()\ndata = sheet.read(\"1f2qu3NGL-kU_RLSvN1O3rTXi-NpAZRtQ7B0trI5xH-U\", \n                range=\"Q2!A1:Z1000\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#installation_2","title":"Installation","text":"<pre><code>pip install pandas google-auth-oauthlib\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#comparison_table","title":"Comparison Table","text":"Feature GoogleDoc GoogleDrive GoogleSheet Content Type Text Documents 100+ File Types Spreadsheet Data Auth Scope docs.readonly drive.readonly sheets.readonly Rate Limits 300 req/min 1000 req/100s 500 req/100s Data Structure Hierarchical File Tree Tabular Version Control \u2705 \u2705 \u274c"},{"location":"indoxArcg/data_connectors/Google-Connectors/#common_setup","title":"Common Setup","text":""},{"location":"indoxArcg/data_connectors/Google-Connectors/#authentication_workflow","title":"Authentication Workflow","text":"<ol> <li>Enable APIs in Google Cloud Console:</li> <li>Google Docs API</li> <li>Google Drive API</li> <li>Google Sheets API</li> <li>Download <code>credentials.json</code></li> <li>First-run OAuth flow: <pre><code># Shared auth pattern\nfrom google.oauth2.credentials import Credentials\n\ncreds = Credentials.from_authorized_user_file('token.json')\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#advanced_operations","title":"Advanced Operations","text":""},{"location":"indoxArcg/data_connectors/Google-Connectors/#batch_processing","title":"Batch Processing","text":"<pre><code># Process multiple Google Docs\ndoc_ids = [\"doc1_id\", \"doc2_id\", \"doc3_id\"]\nfor doc_id in doc_ids:\n    content = GoogleDoc().read(doc_id)\n    process_content(content)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#drive_search","title":"Drive Search","text":"<pre><code>from googleapiclient.discovery import build\n\nservice = build('drive', 'v3', credentials=creds)\nresults = service.files().list(\n    q=\"name contains 'report' and mimeType='application/pdf'\",\n    pageSize=10\n).execute()\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#sheet_data_transformation","title":"Sheet Data Transformation","text":"<pre><code># Convert to pandas DataFrame\nimport pandas as pd\n\nvalues = sheet.read(spreadsheet_id, range=\"Sales!A1:Z1000\")\ndf = pd.DataFrame(values[1:], columns=values[0])\n</code></pre>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_connectors/Google-Connectors/#common_issues","title":"Common Issues","text":"<ol> <li>Authentication Errors</li> <li>Verify <code>credentials.json</code> exists</li> <li>Check OAuth consent screen configuration</li> <li> <p>Ensure redirect URIs match</p> </li> <li> <p>Rate Limits <pre><code>from time import sleep\nfrom googleapiclient.errors import HttpError\n\ntry:\n    doc.read(doc_id)\nexcept HttpError as e:\n    if e.resp.status == 429:\n        sleep(60)  # Backoff 1 minute\n</code></pre></p> </li> <li> <p>File Permissions</p> </li> <li>Share documents with service account email</li> <li>Enable domain-wide delegation for workspace accounts</li> </ol>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#security_best_practices","title":"Security Best Practices","text":"<ol> <li>Use least-privilege scopes:    <pre><code>SCOPES = [\n    'https://www.googleapis.com/auth/documents.readonly',\n    'https://www.googleapis.com/auth/drive.metadata.readonly'\n]\n</code></pre></li> <li>Store tokens encrypted</li> <li>Rotate credentials quarterly</li> <li>Monitor API usage in Cloud Console</li> </ol>"},{"location":"indoxArcg/data_connectors/Google-Connectors/#performance_tips","title":"Performance Tips","text":"<ul> <li>Enable batch processing for bulk operations</li> <li>Use fields parameter to limit response size</li> <li>Cache frequently accessed documents</li> <li>Prefer document IDs over title searches</li> </ul>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/","title":"Multimedia &amp; Geospatial Connectors in indoxArcg","text":"<p>This guide covers integrations with multimedia platforms and location services for video and geospatial data processing.</p>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#supported_connectors","title":"Supported Connectors","text":""},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#1_youtubetranscriptreader","title":"1. YoutubeTranscriptReader","text":"<p>YouTube video transcript extraction</p>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#features","title":"Features","text":"<ul> <li>Multi-language caption support</li> <li>Timestamp metadata</li> <li>Automatic translation handling</li> </ul> <pre><code>from indoxArcg.data_connectors import YoutubeTranscriptReader\n\nreader = YoutubeTranscriptReader(languages=(\"en\", \"es\"))\ndocs = reader.load_data(\n    ytlinks=[\"https://youtu.be/dQw4w9WgXcQ\"],\n    include_timestamps=True\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#installation","title":"Installation","text":"<pre><code>pip install youtube-transcript-api\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#2_mapstextsearch","title":"2. MapsTextSearch","text":"<p>Geocoding and location data retrieval</p>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#features_1","title":"Features","text":"<ul> <li>Address \u2192 coordinates conversion</li> <li>Reverse geocoding support</li> <li>Multiple result ranking</li> </ul> <pre><code>from indoxArcg.data_connectors import MapsTextSearch\n\nsearcher = MapsTextSearch(user_agent=\"my-geo-app/1.0\")\nlocations = searcher.search_address(\"Eiffel Tower, Paris\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#installation_1","title":"Installation","text":"<pre><code>pip install geopy\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#comparison_table","title":"Comparison Table","text":"Feature YoutubeTranscriptReader MapsTextSearch Data Type Video Captions Location Metadata API Source YouTube Data API OpenStreetMap Rate Limits 1M chars/day 1 req/second Authentication None User Agent Required Output Format Timed Text GeoJSON Precision Control Language Selection Result Thresholding"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#common_operations","title":"Common Operations","text":""},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#video_content_processing","title":"Video Content Processing","text":"<pre><code># Merge multiple video transcripts\ncombined = \"\\n\".join([doc.content for doc in docs])\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#location_data_enrichment","title":"Location Data Enrichment","text":"<pre><code># Convert to GeoJSON format\nimport geojson\n\nfeature = geojson.Feature(\n    geometry=geojson.Point((location.longitude, location.latitude)),\n    properties={\"address\": location.address}\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#temporal_analysis","title":"Temporal Analysis","text":"<pre><code># Extract timing data from YouTube transcripts\ntimed_content = [\n    (entry['start'], entry['text']) \n    for entry in doc.metadata['timed_transcript']\n]\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#common_issues","title":"Common Issues","text":"<ol> <li> <p>Missing Transcripts <pre><code>YoutubeTranscriptReader(\n    fallback_ASR=True  # Use automatic speech recognition\n).load_data(links)\n</code></pre></p> </li> <li> <p>Geocoding Ambiguity <pre><code>MapsTextSearch(\n    exactly_one=False  # Get multiple results\n).search_address(\"Springfield\")\n</code></pre></p> </li> <li> <p>API Rate Limits <pre><code>from time import sleep\n\nfor video in playlist:\n    docs = reader.load_data([video])\n    sleep(2)  # Respect YouTube API limits\n</code></pre></p> </li> <li> <p>Encoding Issues <pre><code>YoutubeTranscriptReader(\n    languages=(\"en\",),\n    preserve_formatting=False\n)\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#security_best_practices","title":"Security Best Practices","text":"<ul> <li>YouTube: Monitor API usage through Google Cloud Console</li> <li>Maps: </li> <li>Obfuscate exact coordinates in logs</li> <li>Cache frequently queried locations</li> <li>Comply with OSM's Tile Usage Policy</li> <li>General:</li> <li>Rotate user agents regularly</li> <li>Store API keys in encrypted secrets</li> <li>Limit PII exposure in location data</li> </ul>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#performance_optimization","title":"Performance Optimization","text":""},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#batch_processing","title":"Batch Processing","text":"<pre><code># Bulk geocoding with rate limiting\naddresses = [\"Paris\", \"London\", \"New York\"]\nresults = []\nfor addr in addresses:\n    results.append(searcher.search_address(addr))\n    time.sleep(1.1)  # Stay under OSM limits\n</code></pre>"},{"location":"indoxArcg/data_connectors/Multimedia-Connectors/#selective_loading","title":"Selective Loading","text":"<pre><code># YouTube partial transcript loading\nreader.load_data(\n    ytlinks=[video_url],\n    start_time=60,  # Start at 1 minute\n    end_time=300    # End at 5 minutes\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/","title":"Social &amp; Communication Connectors in indoxArcg","text":"<p>This guide covers integrations with social platforms and team communication tools for real-time data ingestion.</p>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#supported_connectors","title":"Supported Connectors","text":""},{"location":"indoxArcg/data_connectors/Social-Connectors/#1_twittertweetreader","title":"1. TwitterTweetReader","text":"<p>Social media content ingestion from Twitter</p>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#features","title":"Features","text":"<ul> <li>User timeline extraction</li> <li>Hashtag/mention tracking</li> <li>Tweet metadata preservation</li> </ul> <pre><code>from indoxArcg.data_connectors import TwitterTweetReader\n\n# Initialize with bearer token\nreader = TwitterTweetReader(bearer_token=os.getenv(\"TWITTER_BEARER\"))\n\n# Load tweets from handles\ndocs = reader.load_data(\n    twitterhandles=[\"OpenAI\", \"DeepMind\"],\n    num_tweets=200\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#authentication","title":"Authentication","text":"<ol> <li>Create Twitter Developer account</li> <li>Generate Bearer Token</li> <li>Set environment variable: <pre><code>export TWITTER_BEARER=\"your-api-token\"\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#2_discordchannelreader","title":"2. DiscordChannelReader","text":"<p>Community chat data extraction</p>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#features_1","title":"Features","text":"<ul> <li>Channel history retrieval</li> <li>Message threading support</li> <li>User reaction tracking</li> </ul> <pre><code>from indoxArcg.data_connectors import DiscordChannelReader\n\n# Initialize with bot token\nreader = DiscordChannelReader(bot_token=os.getenv(\"DISCORD_TOKEN\"))\n\n# Load channel messages\ndocs = reader.load_data(\n    channel_ids=[\"1234567890\"],\n    num_messages=500\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#authentication_1","title":"Authentication","text":"<ol> <li>Create Discord Bot at Developer Portal</li> <li>Add bot to server with <code>read_messages</code> permission</li> <li>Set environment variable: <pre><code>export DISCORD_TOKEN=\"your-bot-token\"\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#3_googlechat","title":"3. GoogleChat","text":"<p>Enterprise team communication integration</p>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#features_2","title":"Features","text":"<ul> <li>Space/room listing</li> <li>Threaded conversation export</li> <li>Google Workspace integration</li> </ul> <pre><code>from indoxArcg.data_connectors import GoogleChat\n\n# OAuth2 authentication\nchat = GoogleChat()\n\n# List available spaces\nspaces = chat.list_spaces()\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#authentication_2","title":"Authentication","text":"<ol> <li>Enable Google Chat API in Cloud Console</li> <li>Download OAuth2 <code>credentials.json</code></li> <li>First run initiates browser authentication flow</li> </ol>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#comparison_table","title":"Comparison Table","text":"Platform Data Type Rate Limits History Depth Media Support Setup Complexity Twitter Public Posts 900 tweets/15min 7 days Images/GIFs Medium Discord Private Chats 50 reqs/second Unlimited* Files/Embeds High GoogleChat Team Spaces 600 reqs/minute 25k messages Google Docs Medium <p>*Discord history limited by server retention policies</p>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#common_operations","title":"Common Operations","text":""},{"location":"indoxArcg/data_connectors/Social-Connectors/#filtering_content","title":"Filtering Content","text":"<pre><code># Twitter - Exclude retweets\ndocs = [doc for doc in docs if \"RT @\" not in doc.content]\n\n# Discord - Filter bot messages\nclean_docs = [doc for doc in docs if not doc.metadata.get('is_bot')]\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#handling_pagination","title":"Handling Pagination","text":"<pre><code># Twitter - Iterative loading\nfor page in range(0, 3):\n    docs += reader.load_data(\n        twitterhandles=[\"AI_Research\"],\n        num_tweets=100,\n        start_page=page\n    )\n</code></pre>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#troubleshooting","title":"Troubleshooting","text":"<ol> <li> <p>Twitter Rate Limits <pre><code>import time\ntry:\n    docs = reader.load_data(...)\nexcept RateLimitError:\n    time.sleep(900)  # Wait 15 minutes\n</code></pre></p> </li> <li> <p>Discord Permission Issues</p> </li> <li>Ensure bot has <code>View Channel</code> and <code>Read Message History</code> permissions</li> <li> <p>Check server role hierarchy</p> </li> <li> <p>Google OAuth Errors</p> </li> <li>Verify <code>credentials.json</code> exists</li> <li>Ensure redirect URI matches in Google Cloud Console</li> </ol>"},{"location":"indoxArcg/data_connectors/Social-Connectors/#security_best_practices","title":"Security Best Practices","text":"<ul> <li>Use environment variables for tokens</li> <li>Store credentials in encrypted vaults</li> <li>Limit Discord bot permissions</li> <li>Rotate Twitter bearer tokens quarterly ```</li> </ul>"},{"location":"indoxArcg/data_connectors/document/","title":"Document Class Reference","text":"<p>The <code>Document</code> class is the fundamental data structure for representing content and metadata in indoxArcg's data pipeline. It serves as the standardized format for: - Input from data connectors - Processing in RAG/CAG workflows - Storage in vector databases</p> <pre><code>from indoxArcg.data_connectors import Document\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#core_attributes","title":"Core Attributes","text":"Attribute Type Description Example <code>id_</code> <code>str</code> Auto-generated UUIDv4 identifier \"550e8400-e29b-41d4-a716-446655440000\" <code>source</code> <code>str</code> Origin system/format identifier \"wikipedia\", \"youtube_transcript\" <code>content</code> <code>str</code> Primary textual content (UTF-8 encoded) \"Large language models...\" <code>metadata</code> <code>Dict[str, Any]</code> Contextual information about the content <code>{\"author\": \"John Doe\", \"timestamp\": \"2024-03-15\"}</code>"},{"location":"indoxArcg/data_connectors/document/#key_methods","title":"Key Methods","text":""},{"location":"indoxArcg/data_connectors/document/#_init_source_str_content_str_metadata_optionaldictstr_any_none","title":"<code>__init__(source: str, content: str, metadata: Optional[Dict[str, Any]] = None)</code>","text":"<p>Initializes a new Document instance</p> <p>Parameters: - <code>source</code> (str): Origin identifier for tracking document provenance   - Format: <code>[system]_[type]</code> (e.g., <code>arxiv_paper</code>, <code>youtube_transcript</code>)   - Required: Yes - <code>content</code> (str): Main textual payload (minimum 10 characters)   - Required: Yes - <code>metadata</code> (dict): Additional context (default: empty dict)</p> <pre><code>doc = Document(\n    source=\"arxiv_paper\",\n    content=\"We present a novel approach...\",\n    metadata={\n        \"doi\": \"10.1234/abcd.56789\",\n        \"authors\": [\"Smith, J.\", \"Lee, R.\"],\n        \"publication_date\": \"2024-03-01\"\n    }\n)\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#serialization_methods","title":"Serialization Methods","text":""},{"location":"indoxArcg/data_connectors/document/#to_dict_-_dictstr_any","title":"<code>to_dict() -&gt; Dict[str, Any]</code>","text":"<p>Converts document to portable dictionary format</p> <pre><code>doc_dict = doc.to_dict()\n# {\n#     \"id_\": \"550e8400-e29b-41d4-a716-446655440000\",\n#     \"source\": \"arxiv_paper\",\n#     \"content\": \"We present...\",\n#     \"metadata\": {...}\n# }\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#from_dictdata_dictstr_any_-_document","title":"<code>from_dict(data: Dict[str, Any]) -&gt; Document</code>","text":"<p>Reconstructs document from dictionary representation</p> <pre><code>reconstructed_doc = Document.from_dict(doc_dict)\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#usage_guidelines","title":"Usage Guidelines","text":""},{"location":"indoxArcg/data_connectors/document/#best_practices","title":"Best Practices","text":"<ol> <li> <p>Source Identification <pre><code># Good - specific source\nDocument(source=\"wikipedia_llm_article\", ...)\n\n# Avoid - vague source\nDocument(source=\"website\", ...)\n</code></pre></p> </li> <li> <p>Metadata Standards <pre><code>recommended_metadata = {\n    \"author\": str,          # Content creator\n    \"created_date\": str,    # ISO 8601 format\n    \"source_url\": str,      # Original location\n    \"language\": str,        # BCP-47 code\n    \"confidence\": float     # 0.0-1.0 for AI-generated content\n}\n</code></pre></p> </li> <li> <p>Content Validation <pre><code># Minimum viable content check\nif len(doc.content) &lt; 10:\n    raise ValueError(\"Content too short for meaningful processing\")\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_connectors/document/#advanced_usage","title":"Advanced Usage","text":""},{"location":"indoxArcg/data_connectors/document/#batch_processing","title":"Batch Processing","text":"<pre><code>def process_documents(docs: List[Document]) -&gt; List[Document]:\n    \"\"\"Add processing timestamp to all documents\"\"\"\n    return [\n        Document(\n            source=doc.source,\n            content=doc.content,\n            metadata={**doc.metadata, \"processed_at\": datetime.now().isoformat()}\n        )\n        for doc in docs\n    ]\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#error_handling","title":"Error Handling","text":"<pre><code>try:\n    doc = Document(source=\"\", content=\"Invalid document\") \nexcept ValueError as e:\n    print(f\"Validation error: {e}\")\n\ninvalid_dict = {\"source\": \"test\", \"content\": \"\"}\ntry:\n    Document.from_dict(invalid_dict)\nexcept KeyError as e:\n    print(f\"Missing required field: {e}\")\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#integration_points","title":"Integration Points","text":""},{"location":"indoxArcg/data_connectors/document/#with_data_connectors","title":"With Data Connectors","text":"<pre><code>from indoxArcg.data_connectors import WikipediaReader\n\nreader = WikipediaReader()\ndocs = reader.load_data(pages=[\"Large language models\"])\n# Returns List[Document] instances\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#with_vector_stores","title":"With Vector Stores","text":"<pre><code>from indoxArcg.vector_stores import PineconeVectorStore\n\nvector_store = PineconeVectorStore()\nvector_store.add(documents=docs)  # Accepts List[Document]\n</code></pre>"},{"location":"indoxArcg/data_connectors/document/#performance_considerations","title":"Performance Considerations","text":"<ol> <li>Content Size</li> <li>Optimal: 500-1500 characters per document</li> <li> <p>Maximum: 10,000 characters (split larger content)</p> </li> <li> <p>Metadata Efficiency <pre><code># Preferred - flat structure\n{\"author\": \"Alice\", \"page_count\": 12}\n\n# Avoid - nested data\n{\"details\": {\"author\": {\"name\": \"Alice\"}}}\n</code></pre></p> </li> <li> <p>Bulk Operations <pre><code># Process 1000 documents at a time\nbatch_size = 1000\nfor i in range(0, len(docs), batch_size):\n    process_batch(docs[i:i+batch_size])\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_connectors/document/#security_notes","title":"Security Notes","text":"<ol> <li> <p>Sensitive Data <pre><code># Never store raw PII in content/metadata\nDocument(\n    source=\"customer_service_chat\",\n    content=\"[REDACTED]\",\n    metadata={\"user_id\": \"hash_abc123\"}\n)\n</code></pre></p> </li> <li> <p>Validation Filters <pre><code>from html import escape\n\nsanitized_content = escape(raw_content)\ndoc = Document(source=\"web\", content=sanitized_content)\n</code></pre> ```</p> </li> </ol>"},{"location":"indoxArcg/data_loader/","title":"Data Loaders Overview","text":"<p>indoxArcg provides a variety of data loaders to help you load different types of documents and data sources:</p> <ul> <li>Office Loaders - Load Microsoft Office documents (Word, Excel, PowerPoint)</li> <li>PDF Loaders - Load PDF documents with different backends</li> <li>Scientific Loaders - Load scientific papers and research documents</li> <li>Structured Data Loaders - Load structured data formats like CSV, JSON</li> <li>Web Loaders - Load web pages and HTML content</li> <li>Text Loaders - Load plain text files and documents</li> </ul> <p>This documentation organizes supported data loaders into logical categories based on file formats and use cases. Each loader handles specific document types within indoxArcg's RAG/CAG pipeline.</p>"},{"location":"indoxArcg/data_loader/#categories_overview","title":"Categories Overview","text":""},{"location":"indoxArcg/data_loader/#1_pdf_processors","title":"1. PDF Processors","text":"<p>For parsing text and metadata from PDF documents - <code>pdfminer</code>: Text extraction from PDFs with complex layouts - <code>pdfplumber</code>: Advanced PDF text and table extraction - <code>pypdf2</code>: Basic PDF text extraction and manipulation - <code>pypdf4</code>: Enhanced PDF features support</p>"},{"location":"indoxArcg/data_loader/#2_office_document_loaders","title":"2. Office Document Loaders","text":"<p>Microsoft Office file format handlers - <code>docx</code>: Word document processing - <code>pptx</code>: PowerPoint presentation extraction - <code>openpyxl</code>: Excel spreadsheet parsing</p>"},{"location":"indoxArcg/data_loader/#3_structured_data_formats","title":"3. Structured Data Formats","text":"<p>Tabular and structured data processors - <code>csv</code>: Comma-separated values parsing - <code>json</code>: JSON document processing - <code>sql</code>: SQL query results handling - <code>md</code>: Markdown document processing</p>"},{"location":"indoxArcg/data_loader/#4_web_markup_formats","title":"4. Web &amp; Markup Formats","text":"<p>HTML/XML and rich text processors - <code>bs4</code>: BeautifulSoup HTML/XML parsing - <code>rtf</code>: Rich Text Format processing</p>"},{"location":"indoxArcg/data_loader/#5_scientific_data_formats","title":"5. Scientific Data Formats","text":"<p>Specialized scientific data handlers - <code>scipy</code>: Scientific data file support (.mat, .wav) - <code>joblib</code>: Python object serialization files</p>"},{"location":"indoxArcg/data_loader/#6_text_miscellaneous","title":"6. Text &amp; Miscellaneous","text":"<p>Basic text processing and fallback options - <code>txt</code>: Plain text file processing</p>"},{"location":"indoxArcg/data_loader/#loader_comparison","title":"Loader Comparison","text":"Category Loaders Text Extraction Metadata Images Tables Installation Complexity PDF Processors pdfminer, pypdf* \u2705 \u2705 \u274c \u2705 Medium Office Documents docx, pptx, openpyxl \u2705 \u2705 \u2705 \u2705 Low Structured Data csv, json, sql \u2705 \u2705 \u274c \u2705 Low Web &amp; Markup bs4, rtf \u2705 \u2705 \u274c \u274c Medium Scientific Data scipy, joblib \u274c \u2705 \u2705 \u2705 High Text &amp; Miscellaneous txt \u2705 \u274c \u274c \u274c None"},{"location":"indoxArcg/data_loader/#quick_start_guide","title":"Quick Start Guide","text":""},{"location":"indoxArcg/data_loader/#installation","title":"Installation","text":"<pre><code># PDF Processors\npip install pdfminer.six pdfplumber pypdf2 pypdf4\n\n# Office Documents\npip install python-docx python-pptx openpyxl\n\n# Web Formats\npip install beautifulsoup4 striprtf\n\n# Scientific Data\npip install scipy joblib\n</code></pre>"},{"location":"indoxArcg/data_loader/#basic_usage","title":"Basic Usage","text":"<pre><code>from indoxArcg.data_loaders import PDFLoader, DocxLoader\n\n# Load PDF document\npdf_loader = PDFLoader(pdf_processor='pdfplumber')\npdf_content = pdf_loader.load(\"document.pdf\")\n\n# Load Word document\ndocx_loader = DocxLoader()\ndocx_content = docx_loader.load(\"document.docx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Missing Text in PDFs</li> <li>Try different PDF processors: <code>pdfplumber</code> handles complex layouts better</li> <li> <p>Check encrypted files: Use <code>pypdf2</code> for password-protected PDFs</p> </li> <li> <p>Office Document Formatting Issues</p> </li> <li>Ensure Microsoft Office file versions are supported</li> <li> <p>Use <code>openpyxl</code> for Excel files with advanced features</p> </li> <li> <p>HTML Parsing Errors</p> </li> <li>Use <code>bs4</code> with specific parsers (lxml/html5lib)</li> <li> <p>Handle malformed HTML with <code>BeautifulSoup(features=\"html.parser\")</code></p> </li> <li> <p>Scientific Data Loading</p> </li> <li>Verify package versions match file formats</li> <li>Use <code>joblib</code> for Python-specific serialization</li> </ol>"},{"location":"indoxArcg/data_loader/#detailed_guides","title":"Detailed Guides","text":"<ol> <li>PDF Processing Techniques</li> <li>Office Document Handling</li> <li>Structured Data Parsing</li> <li>Web Content Extraction</li> <li>Scientific Data Loading</li> <li>Text Processing Basics ```</li> </ol>"},{"location":"indoxArcg/data_loader/Office-Loaders/","title":"Office Document Loaders in indoxArcg","text":"<p>This guide covers Microsoft Office file processors supported in indoxArcg, organized by file type and capability.</p>"},{"location":"indoxArcg/data_loader/Office-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/Office-Loaders/#1_docx_loader","title":"1. Docx Loader","text":"<p>Best for: Word document text extraction with paragraph analysis</p>"},{"location":"indoxArcg/data_loader/Office-Loaders/#features","title":"Features","text":"<ul> <li>Paragraph-based page estimation</li> <li>Style-aware text extraction</li> <li>Metadata preservation</li> </ul> <pre><code>from indoxArcg.data_loaders import Docx\n\nloader = Docx(paragraphs_per_page=25)  # Customize page estimation\ndocs = loader.load(\"report.docx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#2_pptx_loader","title":"2. Pptx Loader","text":"<p>Best for: PowerPoint presentation content extraction</p>"},{"location":"indoxArcg/data_loader/Office-Loaders/#features_1","title":"Features","text":"<ul> <li>Slide-level text extraction</li> <li>Speaker notes inclusion</li> <li>Slide metadata tracking</li> </ul> <pre><code>from indoxArcg.data_loaders import Pptx\n\nloader = Pptx(include_notes=True)\ndocs = loader.load(\"presentation.pptx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#3_openpyxl_loader","title":"3. OpenPyXl Loader","text":"<p>Best for: Excel spreadsheet data processing</p>"},{"location":"indoxArcg/data_loader/Office-Loaders/#features_2","title":"Features","text":"<ul> <li>Multi-sheet handling</li> <li>Pandas DataFrame integration</li> <li>Cell formatting awareness</li> </ul> <pre><code>from indoxArcg.data_loaders import OpenPyXl\n\nloader = OpenPyXl(sheet_names=[\"Sales\", \"Inventory\"])\ndocs = loader.load(\"data.xlsx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#comparison_table","title":"Comparison Table","text":"Feature Docx Pptx OpenPyXl Text Extraction \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 Metadata Handling Page estimates Slide numbers Sheet names Format Preservation Paragraphs Slide order Table formats Image Extraction \u274c \u2705 (Beta) \u274c Chart Data \u274c \u274c \u2705 Version Support 2007+ 2007+ .xls/.xlsx"},{"location":"indoxArcg/data_loader/Office-Loaders/#installation","title":"Installation","text":"<pre><code>pip install python-docx python-pptx openpyxl pandas\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/Office-Loaders/#word_document_processing","title":"Word Document Processing","text":"<pre><code>from indoxArcg.data_loaders import Docx\n\ndocs = Docx().load(\"contract.docx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#powerpoint_analysis","title":"PowerPoint Analysis","text":"<pre><code>from indoxArcg.data_loaders import Pptx\n\nloader = Pptx(include_hidden=True)  # Process hidden slides\npresentation_docs = loader.load(\"deck.pptx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#excel_data_handling","title":"Excel Data Handling","text":"<pre><code>from indoxArcg.data_loaders import OpenPyXl\n\nloader = OpenPyXl(max_rows=1000)  # Limit row processing\nspreadsheet_docs = loader.load(\"dataset.xlsx\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/Office-Loaders/#custom_metadata_handling","title":"Custom Metadata Handling","text":"<pre><code>def word_metadata(paragraphs, page_num):\n    return {\"author\": \"AI Team\", \"version\": 1.2}\n\nloader = Docx(metadata_fn=word_metadata)\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#excel_data_processing","title":"Excel Data Processing","text":"<pre><code>from indoxArcg.data_loaders import OpenPyXl\n\nloader = OpenPyXl(\n    skip_empty=True,\n    data_format=\"json\"  # Alternative: \"csv\" or \"markdown\"\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#powerpoint_image_extraction","title":"PowerPoint Image Extraction","text":"<pre><code>from indoxArcg.data_loaders import Pptx\n\nloader = Pptx(\n    extract_images=True,\n    image_dir=\"./presentation_images\"\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Office-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/Office-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li> <p>Corrupted Files <pre><code>try:\n    docs = Docx().load(\"file.docx\")\nexcept CorruptedFileError:\n    print(\"Invalid Office file format\")\n</code></pre></p> </li> <li> <p>Missing Content</p> </li> <li>For Excel: Enable formatting awareness      <pre><code>OpenPyXl(preserve_formatting=True)\n</code></pre></li> <li> <p>For PowerPoint: Check slide layouts      <pre><code>Pptx(process_master_slides=True)\n</code></pre></p> </li> <li> <p>Large File Handling <pre><code>Docx(chunk_size=500)  # Process in 500-paragraph chunks\n</code></pre></p> </li> <li> <p>Version Compatibility</p> </li> <li>Use <code>compat_mode=True</code> for legacy formats:      <pre><code>OpenPyXl(compat_mode=True).load(\"old_data.xls\")\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_loader/PDF-Loaders/","title":"PDF Loaders in indoxArcg","text":"<p>This guide covers PDF processors supported in indoxArcg, organized by capability and use case.</p>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/PDF-Loaders/#1_pdfminer","title":"1. PdfMiner","text":"<p>Best for: Complex layouts with precise text positioning</p>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#features","title":"Features","text":"<ul> <li>Accurate text extraction from multi-column layouts</li> <li>Handles PDFs with non-standard encoding</li> <li>Page-level metadata tracking</li> </ul> <pre><code>from indoxArcg.data_loaders import PdfMiner\n\nloader = PdfMiner()\ndocs = loader.load(\"research_paper.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#2_pdfplumber","title":"2. PdfPlumber","text":"<p>Best for: Table extraction and visual debugging</p>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#features_1","title":"Features","text":"<ul> <li>Table data extraction with spatial recognition</li> <li>Visual debugging with page images</li> <li>Text bounding box analysis</li> </ul> <pre><code>from indoxArcg.data_loaders import PdfPlumber\n\nloader = PdfPlumber(extract_tables=True)\ndocs = loader.load(\"financial_report.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#3_pypdf2","title":"3. PyPDF2","text":"<p>Best for: Basic extraction and encryption handling</p>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#features_2","title":"Features","text":"<ul> <li>Password-protected PDF support</li> <li>Fast text extraction</li> <li>Metadata preservation</li> </ul> <pre><code>from indoxArcg.data_loaders import PyPDF2\n\nloader = PyPDF2(password=\"secure123\")\ndocs = loader.load(\"encrypted.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#4_pypdf4","title":"4. PyPDF4","text":"<p>Best for: Advanced PDF features and standards compliance</p>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#features_3","title":"Features","text":"<ul> <li>PDF/A standard support</li> <li>Embedded media handling</li> <li>Advanced metadata extraction</li> </ul> <pre><code>from indoxArcg.data_loaders import PyPDF4 \n\nloader = PyPDF4()\ndocs = loader.load(\"specification.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#comparison_table","title":"Comparison Table","text":"Feature PdfMiner PdfPlumber PyPDF2 PyPDF4 Text Accuracy \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 Table Extraction \u274c \u2705 \u274c \u2705 Password Support \u274c \u274c \u2705 \u2705 Layout Preservation \u2705 \u2705 \u274c \u274c Speed Medium Slow Fast Medium PDF/A Compliance \u274c \u274c \u274c \u2705"},{"location":"indoxArcg/data_loader/PDF-Loaders/#installation","title":"Installation","text":"<pre><code>pip install pdfminer.six pdfplumber pypdf2 pypdf4\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/PDF-Loaders/#simple_text_extraction","title":"Simple Text Extraction","text":"<pre><code>from indoxArcg.data_loaders import PdfPlumber\n\nloader = PdfPlumber()\ndocuments = loader.load(\"document.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#encrypted_pdf_handling","title":"Encrypted PDF Handling","text":"<pre><code>from indoxArcg.data_loaders import PyPDF4\n\nloader = PyPDF4(password=\"company@123\")\ndocuments = loader.load(\"secured_doc.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#table_extraction","title":"Table Extraction","text":"<pre><code>from indoxArcg.data_loaders import PdfPlumber\n\nloader = PdfPlumber(\n    extract_tables=True,\n    table_output_format=\"markdown\"  # or \"pandas\"\n)\ndocuments = loader.load(\"data_report.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/PDF-Loaders/#custom_metadata_handling","title":"Custom Metadata Handling","text":"<pre><code>from indoxArcg.data_loaders import PdfMiner\n\ndef custom_metadata(page_num, text):\n    return {\"page\": page_num + 1, \"chars\": len(text)}\n\nloader = PdfMiner(metadata_fn=custom_metadata)\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#parallel_processing","title":"Parallel Processing","text":"<pre><code>from indoxArcg.data_loaders import PyPDF4\n\nloader = PyPDF4(thread_count=4)  # Use 4 CPU cores\ndocuments = loader.load(\"large_document.pdf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/PDF-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li>Missing Text Content</li> <li>Try PdfPlumber for complex layouts</li> <li> <p>Enable layout preservation: <code>PdfMiner(laparams={\"all_texts\": True})</code></p> </li> <li> <p>Encrypted PDF Errors <pre><code># For PyPDF2/PyPDF4\nloader = PyPDF4(password=\"correct_password\")\n</code></pre></p> </li> <li> <p>Corrupted Files <pre><code>try:\n    docs = loader.load(\"file.pdf\")\nexcept PDFSyntaxError:\n    print(\"Invalid PDF structure\")\n</code></pre></p> </li> <li> <p>Table Extraction Failures</p> </li> <li>Verify installation: <code>pip install pdfplumber[csv]</code></li> <li>Adjust table detection thresholds:      <pre><code>PdfPlumber(table_settings={\"snap_tolerance\": 4})\n</code></pre></li> </ol>"},{"location":"indoxArcg/data_loader/PDF-Loaders/#further_reading","title":"Further Reading","text":"<ul> <li>PDF Processing Best Practices</li> <li>Advanced Table Extraction Guide</li> <li>PDF Security Features ```</li> </ul>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/","title":"Scientific Data Loaders in indoxArcg","text":"<p>This guide covers scientific data format processors supported in indoxArcg, focusing on numerical datasets and serialized objects.</p>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/Scientific-Loaders/#1_scipy_mat_loader","title":"1. Scipy MAT Loader","text":"<p>Best for: MATLAB data file interoperability</p>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#features","title":"Features","text":"<ul> <li>MATLAB v7.3+ format support</li> <li>Variable-wise document generation</li> <li>Metadata filtering</li> </ul> <pre><code>from indoxArcg.data_loaders import Scipy\n\nloader = Scipy(exclude_vars=[\"__header__\", \"__version__\"])\ndocs = loader.load(\"experiment.mat\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#2_joblib_loader","title":"2. Joblib Loader","text":"<p>Best for: Python object serialization</p>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#features_1","title":"Features","text":"<ul> <li>Large NumPy array handling</li> <li>Compression support</li> <li>Safe deserialization</li> </ul> <pre><code>from indoxArcg.data_loaders import Joblib\n\nloader = Joblib(compression=\"zlib\", safe_mode=True)\ndocs = loader.load(\"model.pkl\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#comparison_table","title":"Comparison Table","text":"Feature Scipy MAT Joblib File Formats .mat .pkl, .joblib Data Types MATLAB variables Python objects Compression \u274c \u2705 (multiple) Security Basic Sandboxed Max File Size 2GB 10GB Parallel Loading \u274c \u2705"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#installation","title":"Installation","text":"<pre><code>pip install scipy joblib indoxArcg\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/Scientific-Loaders/#matlab_data_handling","title":"MATLAB Data Handling","text":"<pre><code>from indoxArcg.data_loaders import Scipy\n\n# Load specific variables\ndocs = Scipy(var_names=[\"sensor_data\", \"timestamps\"]).load(\"lab.mat\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#serialized_object_loading","title":"Serialized Object Loading","text":"<pre><code>from indoxArcg.data_loaders import Joblib\n\n# Load with memory mapping\ndocs = Joblib(mmap_mode=\"r\").load(\"large_array.npy\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/Scientific-Loaders/#custom_variable_processing","title":"Custom Variable Processing","text":"<pre><code>def mat_processor(var_name, var_data):\n    return f\"{var_name}: {var_data.shape}\"\n\nloader = Scipy(var_transform=mat_processor)\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#safe_deserialization","title":"Safe Deserialization","text":"<pre><code>from indoxArcg.data_loaders import Joblib\n\nloader = Joblib(\n    allowed_classes=[\"numpy.ndarray\", \"pandas.DataFrame\"],\n    max_buffer_size=1e9  # 1GB limit\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Scientific-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/Scientific-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li> <p>MATLAB Version Mismatch <pre><code>Scipy(matlab_compat=\"v7.3\").load(\"legacy.mat\")\n</code></pre></p> </li> <li> <p>Unsafe Pickle Files <pre><code>Joblib(safe_mode=True).load(\"untrusted.pkl\")\n</code></pre></p> </li> <li> <p>Large MAT Files <pre><code>Scipy(chunk_size=\"1GB\").load(\"big_data.mat\")\n</code></pre></p> </li> <li> <p>Compression Errors <pre><code>Joblib(compression=\"lz4\", fix_imports=False).load(\"compressed.joblib\")\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/","title":"Structured Data Loaders in indoxArcg","text":"<p>This guide covers structured data format processors supported in indoxArcg, organized by file type and processing capability.</p>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#1_csv_loader","title":"1. CSV Loader","text":"<p>Best for: Tabular data processing and row-based analysis</p>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#features","title":"Features","text":"<ul> <li>Row-wise document generation</li> <li>Custom metadata injection</li> <li>Encoding auto-detection</li> </ul> <pre><code>from indoxArcg.data_loaders import CSV\n\nloader = CSV(delimiter=\";\", skip_rows=1)\ndocs = loader.load(\"data.csv\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#2_json_loader","title":"2. JSON Loader","text":"<p>Best for: Nested data structures and API responses</p>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#features_1","title":"Features","text":"<ul> <li>Key-value pair extraction</li> <li>Nested field flattening</li> <li>Size-aware chunking</li> </ul> <pre><code>from indoxArcg.data_loaders import Json\n\nloader = Json(max_depth=3, flatten_nested=True)\ndocs = loader.load(\"api_response.json\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#3_sql_loader","title":"3. SQL Loader","text":"<p>Best for: Database query files and schema analysis</p>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#features_2","title":"Features","text":"<ul> <li>Query validation</li> <li>Parameterized query support</li> <li>Execution plan metadata</li> </ul> <pre><code>from indoxArcg.data_loaders import Sql\n\nloader = Sql(include_execution_plan=True)\ndocs = loader.load(\"query.sql\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#4_md_loader","title":"4. MD Loader","text":"<p>Best for: Markdown documentation processing</p>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#features_3","title":"Features","text":"<ul> <li>Section-aware splitting</li> <li>Frontmatter extraction</li> <li>Code block preservation</li> </ul> <pre><code>from indoxArcg.data_loaders import Md\n\nloader = Md(extract_frontmatter=True)\ndocs = loader.load(\"documentation.md\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#comparison_table","title":"Comparison Table","text":"Feature CSV JSON SQL MD Nested Data Support \u274c \u2705 \u274c \u274c Large File Handling Streaming Chunking \u274c \u274c Metadata Extraction Column Headers Key Paths Query Stats Frontmatter Max File Size 10GB 2GB 100MB 50MB Encoding Support Auto-detect UTF-8/16 UTF-8 UTF-8"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#installation","title":"Installation","text":"<pre><code>pip install indoxArcg[structured]\n# Or individual packages\npip install pandas python-jsonlogger sqlparse markdown\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#csv_processing","title":"CSV Processing","text":"<pre><code>from indoxArcg.data_loaders import CSV\n\ndocs = CSV(include_headers=True).load(\"dataset.csv\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#json_handling","title":"JSON Handling","text":"<pre><code>from indoxArcg.data_loaders import Json\n\nloader = Json(json_path=\"$.items[*]\")  # JSONPath support\ndocs = loader.load(\"nested_data.json\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#sql_analysis","title":"SQL Analysis","text":"<pre><code>from indoxArcg.data_loaders import Sql\n\ndocs = Sql(highlight_syntax=True).load(\"schema.sql\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#markdown_processing","title":"Markdown Processing","text":"<pre><code>from indoxArcg.data_loaders import Md\n\ndocs = Md(split_sections=True).load(\"README.md\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#custom_csv_metadata","title":"Custom CSV Metadata","text":"<pre><code>def csv_metadata(row):\n    return {\"row_id\": row[\"id\"], \"source\": \"legacy_system\"}\n\nloader = CSV(metadata_fn=csv_metadata)\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#json_streaming","title":"JSON Streaming","text":"<pre><code>from indoxArcg.data_loaders import Json\n\nloader = Json(\n    stream=True,\n    chunk_size=1000  # Process 1000 items at a time\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#sql_parameterization","title":"SQL Parameterization","text":"<pre><code>from indoxArcg.data_loaders import Sql\n\nloader = Sql(\n    parameters={\"date\": \"2024-01-01\"},\n    dialect=\"postgresql\"\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/Structured-Data-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li> <p>CSV Encoding Errors <pre><code>CSV(encoding=\"latin-1\").load(\"legacy.csv\")\n</code></pre></p> </li> <li> <p>Malformed JSON <pre><code>Json(strict=False).load(\"partial_data.json\")\n</code></pre></p> </li> <li> <p>Large SQL Files <pre><code>Sql(chunk_queries=True).load(\"large_dump.sql\")\n</code></pre></p> </li> <li> <p>Markdown Formatting <pre><code>Md(clean_extra_spaces=True).load(\"messy.md\")\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_loader/Web-Loaders/","title":"Web &amp; Rich Text Loaders in indoxArcg","text":"<p>This guide covers web content and rich text format processors supported in indoxArcg, organized by format type and extraction capability.</p>"},{"location":"indoxArcg/data_loader/Web-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/Web-Loaders/#1_html_loader_beautifulsoup","title":"1. HTML Loader (BeautifulSoup)","text":"<p>Best for: Web content extraction and DOM analysis</p>"},{"location":"indoxArcg/data_loader/Web-Loaders/#features","title":"Features","text":"<ul> <li>Tag-aware content extraction</li> <li>Automatic encoding detection</li> <li>Clean HTML formatting</li> </ul> <pre><code>from indoxArcg.data_loaders import Bs4\n\nloader = Bs4(exclude_tags=[\"script\", \"style\"])\ndocs = loader.load(\"webpage.html\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#2_rtf_loader","title":"2. RTF Loader","text":"<p>Best for: Legacy rich text document processing</p>"},{"location":"indoxArcg/data_loader/Web-Loaders/#features_1","title":"Features","text":"<ul> <li>Basic formatting preservation</li> <li>Font style recognition</li> <li>Simple metadata extraction</li> </ul> <pre><code>from indoxArcg.data_loaders import Rtf\n\nloader = Rtf(preserve_formatting=True)\ndocs = loader.load(\"document.rtf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#comparison_table","title":"Comparison Table","text":"Feature HTML Loader RTF Loader Text Accuracy \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 Format Preservation Semantic Basic Styles Encoding Handling Auto-detect Latin-1 Default Metadata Extraction DOM Structure Limited Max File Size 100MB 50MB Complex Layouts Tables/CSS Simple Formatting"},{"location":"indoxArcg/data_loader/Web-Loaders/#installation","title":"Installation","text":"<pre><code>pip install beautifulsoup4 pyth indoxArcg\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/Web-Loaders/#html_processing","title":"HTML Processing","text":"<pre><code>from indoxArcg.data_loaders import Bs4\n\n# Extract specific elements\nloader = Bs4(include_selectors=[\".article-content\", \"#main-heading\"])\ndocs = loader.load(\"news.html\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#rtf_processing","title":"RTF Processing","text":"<pre><code>from indoxArcg.data_loaders import Rtf\n\n# Preserve basic formatting\ndocs = Rtf(keep_bold=True, keep_italic=True).load(\"legacy.rtf\")\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/Web-Loaders/#custom_html_metadata","title":"Custom HTML Metadata","text":"<pre><code>def html_metadata(soup):\n    return {\n        \"title\": soup.title.string,\n        \"word_count\": len(soup.get_text().split())\n    }\n\nloader = Bs4(metadata_fn=html_metadata)\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#rtf_format_conversion","title":"RTF Format Conversion","text":"<pre><code>from indoxArcg.data_loaders import Rtf\n\nloader = Rtf(\n    conversion_mode=\"markdown\",  # Convert RTF to Markdown\n    image_dir=\"./rtf_images\"\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/Web-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/Web-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li> <p>HTML Encoding Errors <pre><code>Bs4(encoding=\"windows-1252\").load(\"legacy_page.html\")\n</code></pre></p> </li> <li> <p>Malformed RTF Files <pre><code>Rtf(strict_parsing=False).load(\"corrupted.rtf\")\n</code></pre></p> </li> <li> <p>Missing Web Content <pre><code>Bs4(retry_count=3, timeout=10).load(\"https://example.com\")\n</code></pre></p> </li> <li> <p>Formatting Loss <pre><code>Rtf(preserve_layout=True).load(\"formatted.rtf\")\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_loader/text-Loaders/","title":"Text Loaders in indoxArcg","text":"<p>This guide covers basic text file processing capabilities in indoxArcg, focusing on simple text extraction and handling.</p>"},{"location":"indoxArcg/data_loader/text-Loaders/#supported_loaders","title":"Supported Loaders","text":""},{"location":"indoxArcg/data_loader/text-Loaders/#txt_loader","title":"Txt Loader","text":"<p>Best for: Raw text file processing and ingestion</p>"},{"location":"indoxArcg/data_loader/text-Loaders/#features","title":"Features","text":"<ul> <li>Encoding auto-detection</li> <li>Large file streaming</li> <li>Basic metadata tracking</li> </ul> <pre><code>from indoxArcg.data_loaders import Txt\n\nloader = Txt(encoding=\"utf-8\", chunk_size=4096)\ndocs = loader.load(\"novel.txt\")\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#key_capabilities","title":"Key Capabilities","text":"Feature Description File Formats .txt, .log, .md Encoding Support UTF-8/16, ASCII, Latin-1 Max File Size 10GB+ (with streaming) Metadata Extraction File stats, basic headers Special Handling CR/LF normalization Performance High-speed sequential reading"},{"location":"indoxArcg/data_loader/text-Loaders/#installation","title":"Installation","text":"<pre><code>pip install indoxArcg\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#basic_usage","title":"Basic Usage","text":""},{"location":"indoxArcg/data_loader/text-Loaders/#simple_text_loading","title":"Simple Text Loading","text":"<pre><code>from indoxArcg.data_loaders import Txt\n\ndocs = Txt().load(\"document.txt\")\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#large_file_handling","title":"Large File Handling","text":"<pre><code>loader = Txt(buffer_size=1048576)  # 1MB chunks\ndocs = loader.load(\"large_log.log\")\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#advanced_configuration","title":"Advanced Configuration","text":""},{"location":"indoxArcg/data_loader/text-Loaders/#custom_metadata","title":"Custom Metadata","text":"<pre><code>def text_metadata(file_path, content):\n    return {\n        \"line_count\": len(content.split('\\n')),\n        \"language\": detect_language(content)\n    }\n\nloader = Txt(metadata_fn=text_metadata)\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#pattern-based_processing","title":"Pattern-based Processing","text":"<pre><code>loader = Txt(\n    preprocess=lambda text: re.sub(r'\\s+', ' ', text),  # Remove extra whitespace\n    skip_lines_matching=r'^#.*'  # Skip comment lines\n)\n</code></pre>"},{"location":"indoxArcg/data_loader/text-Loaders/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/data_loader/text-Loaders/#common_issues","title":"Common Issues","text":"<ol> <li> <p>Encoding Errors <pre><code>Txt(encoding=\"latin-1\").load(\"legacy.txt\")\n</code></pre></p> </li> <li> <p>Memory Constraints <pre><code>Txt(stream=True, chunk_size=524288).load(\"huge_file.txt\")\n</code></pre></p> </li> <li> <p>Line Ending Conflicts <pre><code>Txt(normalize_newlines=True).load(\"mixed_endings.txt\")\n</code></pre></p> </li> <li> <p>Binary File Detection <pre><code>Txt(strict_text_check=True).load(\"possibly_binary.dat\")\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/data_loader_and_splitter/clustered_split/","title":"ClusteredSplit","text":"<p>The <code>ClusteredSplit</code> function creates leaf chunks from the text and adds extra clustered chunks to these leaf chunks. The clustering continues until no new clusters are available, growing like a tree: starting from leaf chunks, then clustering between the last clustered chunks, and so on.</p> <pre><code>def __init__(self, file_path: str, embeddings, re_chunk: bool = False, remove_sword: bool = False,\n             chunk_size: Optional[int] = 100, overlap: Optional[int] = 0, threshold: float = 0.1, dim: int = 10,\n             use_openai_summary: bool = False, max_len_summary: int = 100, min_len_summary: int = 30)\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/clustered_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the plain text file or PDF file to be processed.</li> <li>embeddings: The embeddings to be used for clustering.</li> <li>re_chunk (bool): If True, re-chunk the text after initial chunking. Default is False.</li> <li>remove_sword (bool): If True, remove stopwords during the chunking process. Default is False.</li> <li>chunk_size (Optional[int]): The size of each chunk in characters. Default is 100.</li> <li>overlap (Optional[int]): The number of characters to overlap between chunks. Default is 0.</li> <li>threshold (float): The similarity threshold for creating clusters. Default is 0.1.</li> <li>dim (int): The dimensionality of the embeddings. Default is 10.</li> <li>use_openai_summary (bool, optional): Whether to use OpenAI summary for summarizing the chunks. Default is False.</li> <li>max_len_summary (int, optional): The maximum length of the summary. Default is 100.</li> <li>min_len_summary (int, optional): The minimum length of the summary. Default is 30.</li> </ul>"},{"location":"indoxArcg/data_loader_and_splitter/clustered_split/#usage","title":"Usage","text":"<p>To use the ClusteredSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize indoxArcg and QA models:</p> <pre><code>from indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the clustered split on the text file or PDF file:</p> <pre><code>from indoxArcg.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/clustered_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the ClusteredSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indoxArcg import indoxArcgRetrievalAugmentation\nindoxArcg = indoxArcgRetrievalAugmentation()\n\nfrom indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indoxArcg.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path,\n                        embeddings=openai_qa.embeddings,\n                        re_chunk=False,\n                        remove_sword=False,\n                        chunk_size=100,\n                        overlap=0,\n                        threshold=0.1,\n                        dim=10)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <p>This will process the specified file and return all chunks with the extra clustered layers, forming a hierarchical structure of text chunks.</p>"},{"location":"indoxArcg/data_loader_and_splitter/simple_load_split/","title":"SimpleLoadAndSplit","text":"<p>The <code>SimpleLoadAndSplit</code> function accept both PDF and text files and create chunks based on a semantic text splitter</p> <pre><code>def __init__(self, file_path: str, remove_sword: bool = False,\n             max_chunk_size: Optional[int] = 500, )\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/simple_load_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the plain text file or PDF file to be processed.</li> <li>remove_sword (bool): If True, remove stopwords during the chunking process. Default is False.</li> <li>max_chunk_size (Optional[int]): The maximum size of each chunk in characters. Default is 500.</li> </ul>"},{"location":"indoxArcg/data_loader_and_splitter/simple_load_split/#usage","title":"Usage","text":"<p>To use the SimpleLoadAndSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize indoxArcg and QA models:</p> <pre><code>from indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the clustered split on the text file or PDF file:</p> <pre><code>from indoxArcg.data_loader_splitter import SimpleLoadAndSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = SimpleLoadAndSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/simple_load_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the ClusteredSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indoxArcg import indoxArcgRetrievalAugmentation\nindoxArcg = indoxArcgRetrievalAugmentation()\n\nfrom indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indoxArcg.data_loader_splitter import ClusteredSplit\n\nfile_path = \"path/to/your/file.txt\"  # Specify the file path\nloader_splitter = ClusteredSplit(file_path=file_path)\ndocs = loader_splitter.load_and_chunk()\n</code></pre> <p>This will process the specified file and return all chunks with the extra clustered layers, forming a hierarchical structure of text chunks.</p>"},{"location":"indoxArcg/data_loader_and_splitter/unstructured_load_and_split/","title":"Unstructured Load and Split","text":""},{"location":"indoxArcg/data_loader_and_splitter/unstructured_load_and_split/#unstructuredloadandsplit","title":"UnstructuredLoadAndSplit","text":"<p>The UnstructuredLoadAndSplit function uses the unstructured library to import various file types and split them into chunks. By default, it uses the \u201csplit by title\u201d method from the unstructured library, but users can also choose the semantic_text_splitter.</p> <pre><code>def UnstructuredLoadAndSplit(file_path: str,\n                             remove_sword: bool = False,\n                             max_chunk_size: int = 500,\n                             splitter=None)\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/unstructured_load_and_split/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>file_path (str): The path to the file to be processed. Various file   types are supported.</li> <li>remove_sword (bool): If True, remove stop words during the chunking   process. Default is False.</li> <li>max_chunk_size (int): The maximum size of each chunk in characters.   Default is 500.</li> <li>splitter: The method used to split the text. The default is \u201csplit   by title\u201d from the unstructured library. Users can also choose   semantic_text_splitter.</li> </ul>"},{"location":"indoxArcg/data_loader_and_splitter/unstructured_load_and_split/#usage","title":"Usage","text":"<p>To use the UnstructuredLoadAndSplit function, follow the steps below:</p> <p>Import necessary libraries and load environment variables:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n</code></pre> <p>Initialize indoxArcg and QA models:</p> <pre><code>from indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n</code></pre> <p>Perform the unstructured load and split on the file:</p> <pre><code>from indoxArcg.data_loader_splitter import UnstructuredLoadAndSplit\nfrom indoxArcg.splitter import semantic_text_splitter\n\nfile_path = \"path/to/your/file.pdf\"  # Specify the file path\nloader_splitter = UnstructuredLoadAndSplit(file_path=file_path,\n                                remove_sword=False,\n                                max_chunk_size=500,\n                                splitter=semantic_text_splitter)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"indoxArcg/data_loader_and_splitter/unstructured_load_and_split/#example_code","title":"Example Code","text":"<p>Here\u2019s a complete example of using the UnstructuredLoadAndSplit function in a Jupyter notebook:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n\nfrom indoxArcg import indoxArcgRetrievalAugmentation\nindoxArcg = indoxArcgRetrievalAugmentation()\n\nfrom indoxArcg.llms import OpenAiQA\nopenai_qa = OpenAiQA(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo-0125\")\n\nfrom indoxArcg.data_loader_splitter import UnstructuredLoadAndSplit\nfrom indoxArcg.splitter import semantic_text_splitter\n\nfile_path = \"path/to/your/file.pdf\"  # Specify the file path\nloader_splitter = UnstructuredLoadAndSplit(file_path=file_path,\n                                remove_sword=False,\n                                max_chunk_size=500,\n                                splitter=semantic_text_splitter)\ndocs = loader_splitter.load_and_chunk()\n</code></pre>"},{"location":"indoxArcg/graphs/memgraph/","title":"MemgraphDB","text":"<p>The <code>MemgraphDB</code> class allows you to use Memgraph as a graph database to store and query graph-structured data. This can be used in a Retrieval-Augmented Generation (RAG) system or other knowledge graph-related applications.</p>"},{"location":"indoxArcg/graphs/memgraph/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>uri (str): The URI of the Memgraph instance (e.g., \"bolt://localhost:7687\").</li> <li>username (Optional[str]): The username for authenticating to the Memgraph database (optional, defaults to an empty string).</li> <li>password (Optional[str]): The password for authenticating to the Memgraph database (optional, defaults to an empty string).</li> </ul>"},{"location":"indoxArcg/graphs/memgraph/#installation","title":"Installation","text":"<p>To use Memgraph with this package, you need to install the <code>neo4j</code> driver, as it works with Memgraph. You can install it using pip:</p> <pre><code>pip install neo4j\n</code></pre> <p>You will also need a running Memgraph instance. Refer to the official Memgraph installation guide for instructions on setting up a Memgraph instance locally or in the cloud.</p>"},{"location":"indoxArcg/graphs/memgraph/#example_usage","title":"Example Usage","text":""},{"location":"indoxArcg/graphs/memgraph/#initialize_the_memgraphdb","title":"Initialize the MemgraphDB","text":"<p>You can initialize the <code>MemgraphDB</code> class by passing the Memgraph connection details.</p> <pre><code>from indoxArcg.graph import MemgraphDB\n\nmemgraph_db = MemgraphDB(uri=\"bolt://localhost:7687\", username=\"username\", password=\"password\")\n</code></pre>"},{"location":"indoxArcg/graphs/memgraph/#add_graph_documents_to_memgraph","title":"Add Graph Documents to Memgraph","text":"<p>To add graph documents (which contain nodes and relationships) to the Memgraph database, you can use the <code>add_graph_documents</code> method. This method allows you to store a list of graph documents with options to include source metadata and entity labels.</p> <pre><code># Assuming `graph_documents` is a list of GraphDocument objects you want to store\nmemgraph_db.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n</code></pre> <ul> <li>graph_documents (List[GraphDocument]): A list of <code>GraphDocument</code> objects to store in the Memgraph database.</li> <li>base_entity_label (bool, optional): If True, adds a base \"Entity\" label to all nodes. Defaults to True.</li> <li>include_source (bool, optional): If True, includes the source document as part of the graph. Defaults to True.</li> </ul>"},{"location":"indoxArcg/graphs/memgraph/#query_relationships_by_entity","title":"Query Relationships by Entity","text":"<p>To query relationships in Memgraph, you can use the <code>search_relationships_by_entity</code> method. This method allows you to search for relationships by specifying the entity ID and the relationship type.</p> <pre><code># Search for parent relationships for the entity with ID \"Elizabeth_I\"\nrelationships = memgraph_db.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Print the found relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n</code></pre> <ul> <li>entity_id (str): The ID of the entity for which you want to find relationships.</li> <li>relationship_type (str): The type of relationship to search for (e.g., \"PARENT\").</li> </ul>"},{"location":"indoxArcg/graphs/memgraph/#close_the_connection","title":"Close the Connection","text":"<p>After performing your operations, always remember to close the Memgraph connection:</p> <pre><code>memgraph_db.close()\n</code></pre>"},{"location":"indoxArcg/graphs/memgraph/#example_workflow","title":"Example Workflow","text":"<p>Here's an example of how to use <code>MemgraphDB</code> to store and query relationships in Memgraph:</p> <pre><code>from indoxArcg.graph import MemgraphDB\n\n# Initialize MemgraphDB\nmemgraph_db = MemgraphDB(uri=\"bolt://localhost:7687\", username=\"username\", password=\"password\")\n\n# Assuming you have a list of GraphDocument objects ready to store\n# graph_documents = List of GraphDocument objects\n\n# Add the graph documents to Memgraph\nmemgraph_db.add_graph_documents(graph_documents, base_entity_label=True, include_source=True)\n\n# Query relationships of an entity\nrelationships = memgraph_db.search_relationships_by_entity(entity_id=\"Elizabeth_I\", relationship_type=\"PARENT\")\n\n# Output the relationships\nfor rel in relationships:\n    print(f\"{rel['a']['id']} is a {rel['rel_type']} of {rel['b']['id']}\")\n\n# Close the Memgraph connection\nmemgraph_db.close()\n</code></pre>"},{"location":"indoxArcg/graphs/neo4jgraph/","title":"Neo4j Integration in indoxArcg","text":"<p>The <code>Neo4jGraph</code> class enables you to store, retrieve, and manage Knowledge Graph data (nodes and relationships) in a Neo4j database. This is particularly useful when working with RAG (Retriever Augmented Generation) pipelines, allowing you to persist and query the entities and relationships extracted from text.</p>"},{"location":"indoxArcg/graphs/neo4jgraph/#installation","title":"Installation","text":"<ol> <li> <p>Install Neo4j Database    Download and install Neo4j Community or Enterprise Edition from the official website.    Once installed, ensure the database is running. By default, it typically listens on the <code>bolt://localhost:7687</code> URI.</p> </li> <li> <p>Install Python driver for Neo4j <pre><code>pip install neo4j\n</code></pre></p> </li> <li> <p>Install indoxArcg (if not already installed) <pre><code>pip install indoxArcg\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/graphs/neo4jgraph/#usage","title":"Usage","text":""},{"location":"indoxArcg/graphs/neo4jgraph/#1_import_and_initialization","title":"1. Import and Initialization","text":"<pre><code>from indoxArcg.graph import GraphDocument, Node, Relationship\nfrom indoxArcg.core import Document\nfrom indoxArcg.vectorstores import Neo4jGraph\n\n# Instantiate the Neo4jGraph class, providing your Neo4j connection details\ngraph_store = Neo4jGraph(uri=\"bolt://localhost:7687\", username=\"neo4j\", password=\"your_password\")\n</code></pre> <ul> <li>uri: The Bolt URI for your Neo4j instance.</li> <li>username: Your Neo4j username (default is usually <code>\"neo4j\"</code>).</li> <li>password: Your Neo4j password.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#2_creating_graph_documents","title":"2. Creating Graph Documents","text":"<p>A <code>GraphDocument</code> represents a batch of Nodes and Relationships that you want to store (and optionally the source text they came from).</p> <pre><code># Create a source Document\nsource_document = Document(\n    page_content=\"Queen Elizabeth II reigned from 1952 until her death.\",\n    metadata={\"title\": \"British Monarchy\"}\n)\n\n# Create a few nodes\nnode1 = Node(\n    id=\"queen_elizabeth_ii\",\n    type=\"Person\",\n    text=\"Queen Elizabeth II\"\n)\nnode2 = Node(\n    id=\"crown\",\n    type=\"Object\",\n    text=\"Crown\"\n)\n\n# Create a relationship\nrelationship1 = Relationship(\n    source=node1,\n    target=node2,\n    type=\"WEARS\"\n)\n\n# Create a GraphDocument\ngraph_doc = GraphDocument(\n    nodes=[node1, node2],\n    relationships=[relationship1],\n    source=source_document\n)\n</code></pre> <p>Note: - Node objects have the following parameters:   - <code>id</code>: A unique identifier for the node.   - <code>type</code>: A label/category to classify the node (e.g., <code>Person</code>, <code>Event</code>, <code>Location</code>).   - <code>embedding</code> (optional): A list of float values if you are storing vector embeddings.   - <code>text</code> (optional): Free-form text content.</p> <ul> <li>Relationship objects link source and target nodes and define a <code>type</code> (e.g., <code>PARENT_OF</code>, <code>MENTORS</code>, <code>FOUNDED</code>).</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#3_storing_the_graphdocument_in_neo4j","title":"3. Storing the GraphDocument in Neo4j","text":"<p>Once you have one or more <code>GraphDocument</code>s, you can add them to your Neo4j database:</p> <pre><code>graph_store.add_graph_documents(\n    graph_documents=[graph_doc],\n    base_entity_label=True,\n    include_source=True\n)\n</code></pre> <ul> <li><code>base_entity_label</code> (bool): Appends a generic label <code>Entity</code> to each node (for easier queries across all node types).  </li> <li><code>include_source</code> (bool): Creates a special <code>Source</code> node representing the source <code>Document</code>, then links each extracted entity node with a <code>HAS_SOURCE</code> relationship.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#4_closing_the_connection","title":"4. Closing the Connection","text":"<p>It\u2019s best practice to close the database session when you\u2019re done:</p> <pre><code>graph_store.close()\n</code></pre>"},{"location":"indoxArcg/graphs/neo4jgraph/#5_querying_relationships_example","title":"5. Querying Relationships (Example)","text":"<p>To search for relationships of a given type connected to a specific entity ID, use:</p> <pre><code>results = graph_store.search_relationships_by_entity(\n    entity_id=\"queen_elizabeth_ii\",\n    relationship_type=\"WEARS\"\n)\n\nfor record in results:\n    print(record)\n</code></pre> <p>This will return a list of matched records, each including the source node, the relationship type, and the target node.</p>"},{"location":"indoxArcg/graphs/neo4jgraph/#class_reference","title":"Class Reference","text":""},{"location":"indoxArcg/graphs/neo4jgraph/#class_neo4jgraph","title":"<code>class Neo4jGraph</code>","text":"<pre><code>Neo4jGraph(uri: str, username: str, password: str)\n</code></pre> <ul> <li><code>__init__</code>:</li> <li> <p>Initializes the Neo4j driver using the provided <code>uri</code>, <code>username</code>, and <code>password</code>.</p> </li> <li> <p><code>add_graph_documents(graph_documents: List[GraphDocument], base_entity_label: bool = True, include_source: bool = True)</code>:</p> </li> <li>Adds a list of <code>GraphDocument</code> objects to the Neo4j database.</li> <li>If <code>base_entity_label</code> is <code>True</code>, each node will have an additional label <code>Entity</code>.</li> <li> <p>If <code>include_source</code> is <code>True</code>, each document\u2019s source <code>Document</code> is represented as a node labeled <code>Source</code>.</p> </li> <li> <p><code>search_relationships_by_entity(entity_id: str, relationship_type: str) -&gt; List[Dict]</code>:</p> </li> <li>Queries the database for relationships of a certain type emanating from a specific node ID.</li> <li> <p>Returns a list of records with the format <code>[{\"a\": nodeA, \"rel_type\": \"RELATIONSHIP\", \"b\": nodeB}, ...]</code>.</p> </li> <li> <p><code>close()</code>:</p> </li> <li>Closes the Neo4j driver connection.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#class_graphdocument","title":"<code>class GraphDocument</code>","text":"<ul> <li>Holds lists of <code>Node</code> and <code>Relationship</code> objects, plus a source <code>Document</code>.</li> <li>Typically created after processing a chunk of text using the LLM-based extraction.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#class_node","title":"<code>class Node</code>","text":"<ul> <li>Represents a graph node, with an <code>id</code>, a <code>type</code>, an optional <code>embedding</code> vector, and optional <code>text</code>.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#class_relationship","title":"<code>class Relationship</code>","text":"<ul> <li>Represents a directed relationship between two nodes, with a <code>source</code>, <code>target</code>, and <code>type</code>.</li> </ul>"},{"location":"indoxArcg/graphs/neo4jgraph/#best_practices","title":"Best Practices","text":"<ol> <li> <p>Indices and Constraints    In Neo4j, consider creating an index on <code>id</code> for faster lookups:    <pre><code>CREATE INDEX node_id_index FOR (n:Entity) ON (n.id);\n</code></pre>    Similarly, if you\u2019re storing <code>Source</code> nodes, you may want an index on their title if you frequently query it.</p> </li> <li> <p>Batching and Transactions    If you have a very large number of <code>GraphDocument</code> objects, you may want to batch writes or use explicit transactions to optimize performance.</p> </li> <li> <p>Security    Avoid hard-coding credentials. Use environment variables or a configuration manager to store your Neo4j <code>username</code> and <code>password</code>.</p> </li> </ol>"},{"location":"indoxArcg/graphs/neo4jgraph/#putting_it_all_together","title":"Putting It All Together","text":"<pre><code>from indoxArcg.core import Document\nfrom indoxArcg.graph import GraphDocument, Node, Relationship\nfrom indoxArcg.vectorstores import Neo4jGraph\n\n# 1. Setup Neo4j connection\ngraph_store = Neo4jGraph(uri=\"bolt://localhost:7687\", username=\"neo4j\", password=\"password123\")\n\n# 2. Build your source document\nsource_doc = Document(page_content=\"Some text...\", metadata={\"title\": \"Sample Title\"})\n\n# 3. Create nodes and relationships\nperson_node = Node(id=\"alice\", type=\"Person\", text=\"Alice\")\ncity_node = Node(id=\"wonderland\", type=\"Location\", text=\"Wonderland\")\nrelationship = Relationship(source=person_node, target=city_node, type=\"VISITS\")\n\n# 4. Create a GraphDocument\ngraph_doc = GraphDocument(\n    nodes=[person_node, city_node],\n    relationships=[relationship],\n    source=source_doc\n)\n\n# 5. Add the graph document to Neo4j\ngraph_store.add_graph_documents([graph_doc], base_entity_label=True, include_source=True)\n\n# 6. Query relationships\nresults = graph_store.search_relationships_by_entity(entity_id=\"alice\", relationship_type=\"VISITS\")\nprint(results)\n\n# 7. Clean up\ngraph_store.close()\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/","title":"Cache-Augmented Generation (CAG) Pipeline","text":""},{"location":"indoxArcg/pipelines/cag/#next-gen_features","title":"Next-Gen Features \ud83d\ude80","text":"Capability Innovation Impact Multi-Algorithm Retrieval TF-IDF/BM25/Jaccard hybrid engine Optimal context matching Conversational Intelligence Session-aware context strategies Context-aware responses Smart Cache Hierarchy Embedding + text-based layers High recall precision Validation Stack Hallucination checks + relevance grading Enterprise-grade reliability Failover System Web search + cache regeneration 99.9% uptime assurance"},{"location":"indoxArcg/pipelines/cag/#architectural_components","title":"Architectural Components \ud83c\udfd7\ufe0f","text":""},{"location":"indoxArcg/pipelines/cag/#1_core_engine_cag_class","title":"1. Core Engine (CAG Class)","text":"<p><pre><code>class CAG:\n    \"\"\"Orchestrates cache retrieval, context processing, and generation\"\"\"\n    def infer(query, cache_key, strategy=\"recent\", smart_retrieval=True) -&gt; str\n</code></pre> - Multi-Strategy Retrieval: <code>TF-IDF &lt;-&gt; BM25 &lt;-&gt; Jaccard</code> algorithm switching - Context Fusion: Combines cache, conversation history, web results - Adaptive Validation: Configurable thresholds for quality control</p>"},{"location":"indoxArcg/pipelines/cag/#2_knowledge_cache_system","title":"2. Knowledge Cache System","text":"<pre><code>class KVCache:\n    \"\"\"High-performance hierarchical cache manager\"\"\"\n\n    def save_cache(self, key: str, documents: List[CacheEntry]) -&gt; None:\n        \"\"\"Serializes text + embeddings with version control\"\"\"\n\n    def load_cache(self, key: str) -&gt; List[CacheEntry]:\n        \"\"\"Optimized bulk loading with cache warming\"\"\"\n</code></pre> <p>Cache Lifecycle Flow \ud83d\udd04 <pre><code>graph TD\n    A[(\"\ud83d\udcc4 Raw Documents\")] --&gt; B[Chunking]\n    B --&gt; C{{Embedding Generation}}\n    C --&gt; D[(Versioned Storage)]\n    D --&gt; E[Cache Indexing]\n    E --&gt; F{Retrieval Engine}\n    F --&gt; G[/Relevance Filter/]\n    G --&gt; H[[\"Context Assembly\"]]\n\n    style A fill:#f9f,stroke:#333\n    style H fill:#8f8,stroke:#2b2\n    classDef process fill:#eef,stroke:#00f;\n    class B,C,E,F,G process;\n</code></pre></p>"},{"location":"indoxArcg/pipelines/cag/#3_conversational_brain","title":"3. Conversational Brain","text":"<p><pre><code>class ConversationSession:\n    \"\"\"Stateful dialogue manager\"\"\"\n    def get_relevant_context(query)  # BM25-powered history selection\n    def add_to_history(query, response)  # Auto-formatting storage\n</code></pre> - 3 Context Strategies: <code>recent</code> (last N turns) | <code>relevant</code> (BM25 search) | <code>full</code> (complete history)</p>"},{"location":"indoxArcg/pipelines/cag/#configuration_matrix","title":"Configuration Matrix \u2699\ufe0f","text":""},{"location":"indoxArcg/pipelines/cag/#inference_parameters","title":"Inference Parameters","text":"<pre><code>cag.infer(\n    query=\"...\",\n    cache_key=\"medical_kb_v2\",\n    context_strategy=\"relevant\",  # recent|relevant|full\n    context_turns=3,             # Turns for recent strategy\n    top_k=7,                     # Cache documents per retrieval\n    similarity_threshold=0.65,   # Match quality floor\n    web_search=True,             Enable fallback\n    similarity_search_type=\"bm25\",  # Override default\n    smart_retrieval=True         # Enable validation stack\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#performance_tuning","title":"Performance Tuning","text":"Parameter Recommended Purpose <code>top_k</code> 5-10 Balance recall vs noise <code>similarity_threshold</code> 0.6-0.8 Precision control <code>context_turns</code> 3-5 Conversation memory <code>cache_key</code> Versioned IDs Knowledge isolation"},{"location":"indoxArcg/pipelines/cag/#usage_scenarios","title":"Usage Scenarios \ud83d\udee0\ufe0f","text":""},{"location":"indoxArcg/pipelines/cag/#1_domain-specific_qa","title":"1. Domain-Specific QA","text":"<pre><code># Medical diagnosis assistant\nresponse = cag.infer(\n    query=\"Stage 3 breast cancer treatment options\",\n    cache_key=\"oncology_v3\",\n    similarity_search_type=\"bm25\",\n    smart_retrieval=True\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#2_conversational_agent","title":"2. Conversational Agent","text":"<pre><code># Maintain session context\nfor query in conversation:\n    response = cag.infer(\n        query,\n        cache_key=\"product_info\",\n        context_strategy=\"relevant\",\n        context_turns=3\n    )\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#3_research_assistant","title":"3. Research Assistant","text":"<pre><code># Web-augmented discovery\nresearch_response = cag.infer(\n    \"Latest NLP breakthroughs\",\n    cache_key=\"ai_papers\",\n    web_search=True,\n    top_k=10\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#intelligent_retrieval_flow","title":"Intelligent Retrieval Flow \ud83d\udd04","text":"<ol> <li>Query Analysis </li> <li>Intent recognition</li> <li> <p>Context strategy selection</p> </li> <li> <p>Cache Retrieval <pre><code>self._get_relevant_context()  # Multi-algorithm search\n</code></pre></p> </li> <li> <p>Validation Gate <code>Hallucination Check \u2192 Relevance Grading \u2192 Score Thresholding</code></p> </li> <li> <p>Fallback Activation <code>Web Search \u2192 Cache Regeneration \u2192 Hybrid Results</code></p> </li> <li> <p>Generation <code>Context-Enhanced Prompting \u2192 LLM Inference</code></p> </li> </ol>"},{"location":"indoxArcg/pipelines/cag/#optimization_guide","title":"Optimization Guide \ud83c\udfce\ufe0f","text":""},{"location":"indoxArcg/pipelines/cag/#algorithm_selection","title":"Algorithm Selection","text":"Scenario Recommended Algorithm Short queries Jaccard + BM25 Technical docs BM25 + Embeddings Conversational TF-IDF + Recent Context"},{"location":"indoxArcg/pipelines/cag/#cache_management","title":"Cache Management","text":"<pre><code># Preload domain-specific knowledge\ncag.preload_documents(\n    documents=medical_guidelines,\n    cache_key=\"clinical_v1\"\n)\n\n# Versioned updates\ncag.preload_documents(\n    updated_guidelines,\n    cache_key=\"clinical_v2\"\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#session_strategies","title":"Session Strategies","text":"Strategy Use Case <code>recent</code> Task-oriented dialogs <code>relevant</code> Complex troubleshooting <code>full</code> Legal/technical analysis"},{"location":"indoxArcg/pipelines/cag/#enterprise_patterns","title":"Enterprise Patterns \ud83c\udfe2","text":""},{"location":"indoxArcg/pipelines/cag/#multi-tenant_isolation","title":"Multi-Tenant Isolation","text":"<pre><code># Tenant-specific caches\ntenant_cache = KVCache(cache_dir=f\"/caches/{tenant_id}\")\ncag = CAG(llm, cache=tenant_cache)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#audit_integration","title":"Audit Integration","text":"<pre><code># Log full context\nlogger.info(f\"Generation Context:\\n{final_context_str}\")\n\n# Store validation results\naudit_log.append({\n    \"query\": query,\n    \"hallucination_check\": check_result,\n    \"relevance_scores\": grades\n})\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#custom_validators","title":"Custom Validators","text":"<pre><code>class LegalValidator(AnswerValidator):\n    def check_hallucination(context, answer):\n        return legal_compliance_check(answer, context)\n</code></pre>"},{"location":"indoxArcg/pipelines/cag/#troubleshooting","title":"Troubleshooting \ud83d\udd27","text":"Symptom Diagnostic Solution Low cache hit rate Embedding mismatch Verify embedding dimensions High web fallback Cache coverage gap Expand preloaded documents Slow BM25 Large history size Limit context_turns Validation false positives Threshold too strict Calibrate similarity_threshold"},{"location":"indoxArcg/pipelines/cag/#evolution_roadmap","title":"Evolution Roadmap \ud83c\udf10","text":"<ol> <li> <p>Hybrid Search <code>Vector + BM25 fusion scoring</code></p> </li> <li> <p>Cache Warm-Up <code>Predictive preloading based on usage patterns</code></p> </li> <li> <p>Multi-Modal Cache <code>Images + Tables + Text unified storage</code></p> </li> </ol>"},{"location":"indoxArcg/pipelines/rag/","title":"Retrieval-Augmented Generation (RAG) Pipeline - Comprehensive Guide","text":""},{"location":"indoxArcg/pipelines/rag/#overview","title":"Overview","text":"<p>A sophisticated RAG implementation combining multiple retrieval strategies with answer validation and fallback mechanisms. Designed for enterprise-grade knowledge applications requiring high accuracy and reliability.</p>"},{"location":"indoxArcg/pipelines/rag/#key_features","title":"Key Features \u2728","text":"Feature Description Benefit Multi-Strategy Retrieval Vector search + query expansion + web fallback Comprehensive context gathering Hallucination Guardrails LLM-powered validation layers Reduced factual errors Adaptive Context Processing Optional clustering &amp; relevance grading Higher quality inputs for generation Conversational Memory Built-in session tracking Context-aware follow-ups Fail-Safe Mechanisms Web search fallback + error recovery Reliable performance in edge cases"},{"location":"indoxArcg/pipelines/rag/#core_components","title":"Core Components \ud83e\udde9","text":""},{"location":"indoxArcg/pipelines/rag/#1_retrieval_strategies","title":"1. Retrieval Strategies","text":""},{"location":"indoxArcg/pipelines/rag/#standardretriever","title":"StandardRetriever","text":"<p><pre><code>class StandardRetriever(BaseRetriever):\n    \"\"\"Vector similarity search with score thresholds\"\"\"\n    def retrieve(query: str) -&gt; List[RetrievalResult]\n</code></pre> - Pure vector space retrieval - Configurable similarity thresholds - Batch processing support</p>"},{"location":"indoxArcg/pipelines/rag/#multiqueryretriever","title":"MultiQueryRetriever","text":"<p><pre><code>class MultiQueryRetriever(BaseRetriever):\n    \"\"\"Query expansion for enhanced context capture\"\"\"\n    def retrieve(query: str) -&gt; List[RetrievalResult]\n</code></pre> - LLM-generated query variations - Parallel document fetching - Result deduplication</p>"},{"location":"indoxArcg/pipelines/rag/#2_validation_layer","title":"2. Validation Layer","text":""},{"location":"indoxArcg/pipelines/rag/#answervalidator","title":"AnswerValidator","text":"<p><pre><code>class AnswerValidator:\n    \"\"\"Quality control checks for generated content\"\"\"\n    def check_hallucination(context, answer) -&gt; bool\n    def grade_relevance(context, query) -&gt; List[str]\n</code></pre> - Factual consistency checks - Context-answer alignment scoring - Document relevance ranking</p>"},{"location":"indoxArcg/pipelines/rag/#3_fallback_system","title":"3. Fallback System","text":""},{"location":"indoxArcg/pipelines/rag/#websearchfallback","title":"WebSearchFallback","text":"<p><pre><code>class WebSearchFallback:\n    \"\"\"External knowledge integration\"\"\"\n    def search(query) -&gt; List[str]\n</code></pre> - DuckDuckGo API integration - Search result cleansing - Emergency context injection</p>"},{"location":"indoxArcg/pipelines/rag/#configuration_options","title":"Configuration Options \u2699\ufe0f","text":""},{"location":"indoxArcg/pipelines/rag/#retrieval_parameters","title":"Retrieval Parameters","text":"<pre><code>rag.infer(\n    question=\"...\",\n    top_k=5,                   # Documents per retrieval stage\n    use_clustering=False,       # Enable context clustering\n    use_multi_query=False,      # Activate query expansion\n    smart_retrieval=True        # Enable validation+fallback pipeline\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#threshold_settings","title":"Threshold Settings","text":"<pre><code># Suggested optimal ranges:\nMIN_SIMILARITY = 0.65          # Vector search cutoff\nHALLUCINATION_THRESHOLD = 0.8   # Confidence threshold\nWEB_FALLBACK_TIMEOUT = 5.0      # Search timeout in seconds\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#usage_scenarios","title":"Usage Scenarios \ud83d\udee0\ufe0f","text":""},{"location":"indoxArcg/pipelines/rag/#basic_implementation","title":"Basic Implementation","text":"<pre><code># Initialize pipeline\nrag = RAG(\n    llm=GPT4(), \n    vector_store=FAISSIndex()\n)\n\n# Simple query\nresponse = rag.infer(\"Explain quantum computing basics\")\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#advanced_configuration","title":"Advanced Configuration","text":"<pre><code># Custom retrieval stack\nresponse = rag.infer(\n    \"Latest AI safety breakthroughs\",\n    top_k=8,\n    use_clustering=True,\n    smart_retrieval=True,\n    use_multi_query=True\n)\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#error_handling","title":"Error Handling","text":"<pre><code>try:\n    return rag.infer(complex_query)\nexcept ContextRetrievalError as e:\n    return fallback_search(complex_query)\nexcept AnswerGenerationError as e:\n    log_error(f\"Generation failed: {str(e)}\")\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#pipeline_workflow","title":"Pipeline Workflow \ud83d\udd04","text":"<ol> <li>Input Sanitization</li> <li>Query normalization</li> <li> <p>Empty input detection</p> </li> <li> <p>Context Gathering</p> </li> <li>Primary vector store retrieval</li> <li>\u2192 Failover: Web search integration</li> <li> <p>\u2192 Expansion: Multi-query generation</p> </li> <li> <p>Content Processing</p> </li> <li>Relevance scoring</li> <li>Optional semantic clustering</li> <li> <p>Document deduplication</p> </li> <li> <p>Answer Generation</p> </li> <li>Context-aware LLM prompting</li> <li> <p>Multiple generation strategies</p> </li> <li> <p>Validation Phase</p> </li> <li>Hallucination checks</li> <li>Context consistency verification</li> <li>Optional regeneration cycles</li> </ol>"},{"location":"indoxArcg/pipelines/rag/#best_practices","title":"Best Practices \u2705","text":"<ol> <li>Retrieval Optimization</li> <li>Start with <code>top_k=5-10</code> for standard queries</li> <li>Enable multi-query for complex/ambiguous requests</li> <li> <p>Use clustering for multi-document topics</p> </li> <li> <p>Validation Tuning</p> </li> <li>Adjust hallucination thresholds per domain</li> <li>Implement custom grading prompts</li> <li> <p>Add domain-specific blacklists</p> </li> <li> <p>Performance Monitoring</p> </li> <li>Track context relevance scores</li> <li>Log fallback activation rates</li> <li>Audit answer regeneration counts</li> </ol>"},{"location":"indoxArcg/pipelines/rag/#troubleshooting","title":"Troubleshooting \u26a0\ufe0f","text":"Symptom Diagnosis Solution Empty context returns Vector store mismatch Verify embedding dimensions High hallucination rates Context-answer gap Increase validation strictness Slow web fallback Network issues Implement search timeouts Multi-query failures LLM compatibility Adjust expansion prompts"},{"location":"indoxArcg/pipelines/rag/#extension_points","title":"Extension Points \ud83e\uddea","text":""},{"location":"indoxArcg/pipelines/rag/#custom_retrievers","title":"Custom Retrievers","text":"<pre><code>class HybridRetriever(BaseRetriever):\n    def retrieve(query):\n        vector_results = super().retrieve(query)\n        keyword_results = keyword_search(query)\n        return fusion_results(vector_results, keyword_results)\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#enhanced_validation","title":"Enhanced Validation","text":"<pre><code>class ClinicalValidator(AnswerValidator):\n    def check_hallucination(context, answer):\n        return clinical_fact_checker.validate(\n            context, \n            answer,\n            domain=\"medical\"\n        )\n</code></pre>"},{"location":"indoxArcg/pipelines/rag/#conclusion","title":"Conclusion \ud83c\udfc1","text":"<p>This RAG implementation provides a production-ready framework for building reliable knowledge systems. Key advantages:</p> <ul> <li>Flexible Architecture: Mix-and-match retrieval components</li> <li>Enterprise Features: Validation, fallback, monitoring</li> <li>Continuous Improvement: Built-in extension patterns</li> </ul> <p>For optimal performance: 1. Profile retrieval performance 2. Customize validation thresholds 3. Implement domain-specific optimizations</p>"},{"location":"indoxArcg/splitters/","title":"Text Splitters in indoxArcg","text":"<p>When dealing with large documents, splitting them into coherent chunks improves:</p> <ol> <li>Embedding quality: Smaller text chunks typically produce more focused embeddings.</li> <li>LLM performance: Language models often have token limits; chunking ensures each piece stays within the model\u2019s bounds.</li> <li>Search granularity: In RAG pipelines, chunk-level search can yield more relevant context for queries.</li> </ol> <p>indoxArcg provides various splitter classes, each tailored to different formats or splitting strategies. You\u2019ll find their implementations in the <code>docs\\indoxArcg\\splitters</code> directory, each with its own <code>.md</code> file.</p> <p>Below is a summary:</p>"},{"location":"indoxArcg/splitters/#1_ai21semantic_splittermd","title":"1. AI21semantic_splitter.md","text":"<ul> <li>Description: Leverages AI21\u2019s semantic understanding to split text into logical or semantic units.  </li> <li>Use Case: When you want to rely on AI21\u2019s advanced NLP to identify sentence or paragraph boundaries more intelligently than simple token limits or punctuation-based splitting.  </li> <li>Key Feature: Can produce chunks that preserve context while adhering to a specified token or word threshold.</li> </ul>"},{"location":"indoxArcg/splitters/#2_charachter_splittermd","title":"2. Charachter_splitter.md","text":"<ul> <li>Description: Splits text purely based on character count.  </li> <li>Use Case: A simple, language-agnostic approach\u2014quick to implement if you just need uniform chunk sizes.  </li> <li>Key Parameter: <code>chunk_size</code> in characters. Splitting can optionally also consider overlap to maintain context continuity between chunks.</li> </ul>"},{"location":"indoxArcg/splitters/#3_markdown_text_splittermd","title":"3. Markdown_text_splitter.md","text":"<ul> <li>Description: Splits documents that are in Markdown format, respecting Markdown-specific elements (e.g., headings, code blocks).  </li> <li>Use Case: Ideal for knowledge bases or technical docs stored in Markdown. Ensures that headings and code blocks remain intact, improving embedding coherence.  </li> <li>Advanced: May have toggles to handle code fences or skip certain front-matter sections.</li> </ul>"},{"location":"indoxArcg/splitters/#4_recursively_splittermd","title":"4. Recursively_splitter.md","text":"<ul> <li>Description: A hierarchical splitter that first attempts to split at the highest-level boundaries (e.g., paragraphs or sections), then recursively splits smaller sections if they exceed a certain token or character limit.  </li> <li>Use Case: Best when you want to preserve logical boundaries (e.g., paragraphs, then sentences) while ensuring each chunk meets token size constraints.  </li> <li>Key Feature: Reduces the risk of cutting sentences mid-way, often leading to more semantically meaningful chunks.</li> </ul>"},{"location":"indoxArcg/splitters/#5_semantic_splittermd","title":"5. Semantic_splitter.md","text":"<ul> <li>Description: Similar concept to the AI21-based splitter but typically uses a different semantic parsing or internal logic (e.g., a local embedding-based approach or simpler heuristics).  </li> <li>Use Case: For text where you want more nuanced splits than raw character or sentence boundaries, but you may not rely on an external API like AI21.  </li> <li>Key Parameter: Often has a <code>chunk_size</code> (in tokens or sentences) and a method for detecting semantic boundaries.</li> </ul>"},{"location":"indoxArcg/splitters/#common_usage_pattern","title":"Common Usage Pattern","text":"<p>Most splitters in indoxArcg share a similar interface. For instance:</p> <pre><code>from indoxArcg.splitter import SemanticTextSplitter, CharacterSplitter\n\n# Example: Using the SemanticTextSplitter\nsplitter = SemanticTextSplitter(chunk_size=200, overlap=20)\nchunks = splitter.split_text(\"Your long text document here...\")\n\nfor chunk in chunks:\n    print(chunk)\n</code></pre> <p>Typical parameters:</p> <ul> <li><code>chunk_size</code>: The desired length of each chunk (in tokens, words, or characters).  </li> <li><code>overlap</code>: A small amount of text (e.g., 20 tokens or 50 characters) to repeat from the end of one chunk to the start of the next, to preserve context.  </li> <li><code>split_text</code>: The main method that takes a string (or list of strings) and returns a list of chunked segments.</li> </ul>"},{"location":"indoxArcg/splitters/#best_practices","title":"Best Practices","text":"<ol> <li>Choose a splitter that matches your data </li> <li>Markdown for <code>.md</code> docs  </li> <li>AI21Semantic or Semantic for well-structured semantic breaks  </li> <li> <p>Character or Recursive for general text with no strict semantic cues</p> </li> <li> <p>Be mindful of chunk length </p> </li> <li>Larger chunks can preserve more context but risk exceeding model token limits.  </li> <li> <p>Smaller chunks are easier for LLMs to handle but may reduce context continuity.</p> </li> <li> <p>Use overlap    Overlapping text ensures a question referencing the boundary of two chunks won\u2019t lose context.</p> </li> <li> <p>Experiment    The ideal splitting strategy depends on your domain, data format, and LLM constraints. It\u2019s often useful to try multiple approaches and see which yields the best retrieval or summarization results.</p> </li> </ol>"},{"location":"indoxArcg/splitters/#getting_started","title":"Getting Started","text":"<ol> <li> <p>Install <code>indoxArcg</code> if you haven\u2019t already:    <pre><code>pip install indoxArcg\n</code></pre></p> </li> <li> <p>Import the desired splitter:    <pre><code>from indoxArcg.splitter import CharacterSplitter\n</code></pre></p> </li> <li> <p>Configure &amp; Split:    <pre><code>splitter = CharacterSplitter(chunk_size=500, overlap=50)\ndocument_chunks = splitter.split_text(long_document_text)\n</code></pre></p> </li> <li> <p>Use in RAG    These chunks can then be fed into embedding models or graph transformers for indexing or knowledge graph construction.</p> </li> </ol>"},{"location":"indoxArcg/splitters/#summary","title":"Summary","text":"<p>Text splitters in indoxArcg cater to a variety of formats (Markdown, raw text, recursively structured docs) and approaches (simple character-based vs. semantic-based). Correctly splitting your documents is a critical step in any RAG or LLM-based pipeline, ensuring that context is both manageable and semantically coherent.</p> <p>Use the links above for detailed usage instructions on each splitter. By choosing the right splitter and parameters, you\u2019ll improve the quality of your embeddings, queries, and final outputs. Happy splitting!</p>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/","title":"AI21SemanticTextSplitter","text":"<p>AI21SemanticTextSplitter is a Python class that utilizes the AI21 API for semantic text segmentation. It splits input text into meaningful segments and optionally merges these segments into larger chunks, maintaining semantic coherence. This tool is particularly useful for processing large texts while preserving context and meaning.</p> <p>Note: To use AI21SemanticTextSplitter, users need to have an AI21 API key and the <code>requests</code> library installed. The AI21 API key should be provided either as an environment variable or when initializing the class.</p> <p>To use AI21SemanticTextSplitter:</p> <pre><code>from indoxArcg.splitter import AI21SemanticTextSplitter\n\nsplitter = AI21SemanticTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50,\n    api_key=\"your_ai21_api_key\"\n)\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: Maximum size of each chunk (default: 400). Set to 0 to disable merging.</li> <li>chunk_overlap [int]: Number of characters to overlap between chunks (default: 50).</li> <li>api_key [Optional[str]]: AI21 API key (optional if set as an environment variable).</li> <li>api_host [str]: Base URL for the AI21 API (default: \"https://api.ai21.com/studio/v1\").</li> <li>timeout_sec [Optional[float]]: Timeout for API requests in seconds (optional).</li> <li>num_retries [int]: Number of times to retry failed API calls (default: 3).</li> </ul>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#usage","title":"Usage","text":""},{"location":"indoxArcg/splitters/AI21semantic_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"indoxArcg/splitters/AI21semantic_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <p>2Activate the virtual environment:</p> <pre><code>indoxArcg\\Scripts\\activate\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>source indoxArcg/bin/activate\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#get_started","title":"Get Started","text":""},{"location":"indoxArcg/splitters/AI21semantic_splitter/#set_ai21_api_key_as_environment_variable","title":"Set AI21 API Key as Environment Variable","text":"<p>Import HuggingFace API Key</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv('api.env')\nAI21_API_KEY = os.getenv('AI21_API_KEY')\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#import_essential_libraries","title":"Import Essential Libraries","text":"<pre><code>from indoxArcg.splitter import AI21SemanticTextSplitter\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#initialize_ai21semantictextsplitter","title":"Initialize AI21SemanticTextSplitter","text":"<pre><code>splitter = AI21SemanticTextSplitter(\n    chunk_size=400,\n    chunk_overlap=50\n)\n</code></pre>"},{"location":"indoxArcg/splitters/AI21semantic_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>TEXT = (\n    \"We\u2019ve all experienced reading long, tedious, and boring pieces of text - financial reports, \"\n    \"legal documents, or terms and conditions (though, who actually reads those terms and conditions to be honest?).\\n\"\n    \"Imagine a company that employs hundreds of thousands of employees. In today's information \"\n    \"overload age, nearly 30% of the workday is spent dealing with documents. There's no surprise \"\n    \"here, given that some of these documents are long and convoluted on purpose (did you know that \"\n    \"reading through all your privacy policies would take almost a quarter of a year?). Aside from \"\n    \"inefficiency, workers may simply refrain from reading some documents (for example, Only 16% of \"\n    \"Employees Read Their Employment Contracts Entirely Before Signing!).\\nThis is where AI-driven summarization \"\n    \"tools can be helpful: instead of reading entire documents, which is tedious and time-consuming, \"\n    \"users can (ideally) quickly extract relevant information from a text. With large language models, \"\n    \"the development of those tools is easier than ever, and you can offer your users a summary that is \"\n    \"specifically tailored to their preferences.\\nLarge language models naturally follow patterns in input \"\n    \"(prompt), and provide coherent completion that follows the same patterns. For that, we want to feed \"\n    'them with several examples in the input (\"few-shot prompt\"), so they can follow through. '\n    \"The process of creating the correct prompt for your problem is called prompt engineering, \"\n    \"and you can read more about it here.\"\n)\n\nsemantic_text_splitter = AI21SemanticTextSplitter()\nchunks = semantic_text_splitter.split_text(TEXT)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor chunk in chunks:\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/","title":"CharacterTextSplitter","text":"<p>CharacterTextSplitter is a Python class designed for splitting text into chunks based on a specified separator. It implements an algorithm to split text into chunks of a specified size, with an optional overlap between chunks.</p> <p>Note: This class is part of a text splitting module. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use CharacterTextSplitter:</p> <pre><code>from indoxArcg.splitter import CharacterTextSplitter\n\nsplitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>separator [str]: The string used to split the text (default: \"\\n\\n\").</li> <li>chunk_size [int]: The maximum size of each chunk (default: 400).</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks (default: 50).</li> <li>length_function [callable]: A function used to calculate the length of text (default: len).</li> </ul>"},{"location":"indoxArcg/splitters/Charachter_splitter/#usage","title":"Usage","text":""},{"location":"indoxArcg/splitters/Charachter_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"indoxArcg/splitters/Charachter_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <p>2Activate the virtual environment:</p> <pre><code>indoxArcg\\Scripts\\activate\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>source indoxArcg/bin/activate\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/#import_the_charactertextsplitter","title":"Import the CharacterTextSplitter","text":"<pre><code>from indoxArcg.splitter import CharacterTextSplitter\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/#initialize_charactertextsplitter","title":"Initialize CharacterTextSplitter","text":"<pre><code>splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Charachter_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/","title":"MarkdownTextSplitter","text":"<p>MarkdownTextSplitter is a Python class designed for splitting Markdown-formatted text into chunks. It extends the RecursiveCharacterTextSplitter to specifically handle Markdown-formatted text, splitting along headings and other Markdown-specific separators.</p> <p>Note: This class is part of the <code>indoxArcg.splitter</code> module and requires the <code>RecursiveCharacterTextSplitter</code> as a base class. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use MarkdownTextSplitter:</p> <pre><code>from indoxArcg.splitter import MarkdownTextSplitter\n\nsplitter = MarkdownTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#hyperparameters","title":"Hyperparameters","text":"<p>MarkdownTextSplitter inherits all parameters from RecursiveCharacterTextSplitter. Key parameters include:</p> <ul> <li>chunk_size [int]: The maximum size of each text chunk.</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks.</li> <li>length_function [Callable[[str],int]]: Function to measure the length of given text.</li> </ul>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#usage","title":"Usage","text":""},{"location":"indoxArcg/splitters/Markdown_text_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"indoxArcg/splitters/Markdown_text_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <p>2Activate the virtual environment:</p> <pre><code>indoxArcg\\Scripts\\activate\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>source indoxArcg/bin/activate\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#import_the_markdowntextsplitter","title":"Import the MarkdownTextSplitter","text":"<pre><code>from indoxArcg.splitter import MarkdownTextSplitter\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#initialize_markdowntextsplitter","title":"Initialize MarkdownTextSplitter","text":"<pre><code>splitter = MarkdownTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Markdown_text_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>markdown_text = \"\"\"\n# Main Heading\n\n## Subheading 1\nContent for subheading 1...\n\n## Subheading 2\nContent for subheading 2...\n\n***\n\nSome more content with horizontal rule above.\n\"\"\"\n\nchunks = splitter.split_text(markdown_text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor chunk in chunks:\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/","title":"RecursiveCharacterTextSplitter","text":"<p>RecursiveCharacterTextSplitter is a Python class designed for splitting text into chunks recursively based on specified separators. It implements a recursive algorithm to split text into chunks of a specified size, with an optional overlap between chunks.</p> <p>Note: This class is part of the <code>indoxArcg.splitter</code> module. Ensure you have the necessary dependencies installed before using this class.</p> <p>To use RecursiveCharacterTextSplitter:</p> <pre><code>from indoxArcg.splitter import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: The maximum size of each chunk (default: 400).</li> <li>chunk_overlap [int]: The number of characters to overlap between chunks (default: 50).</li> <li>separators [Optional[List[str]]]: A list of separators to use for splitting the text (default: [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]).</li> </ul>"},{"location":"indoxArcg/splitters/Recursively_splitter/#usage","title":"Usage","text":""},{"location":"indoxArcg/splitters/Recursively_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"indoxArcg/splitters/Recursively_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <p>2Activate the virtual environment:</p> <pre><code>indoxArcg\\Scripts\\activate\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>source indoxArcg/bin/activate\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/#import_the_recursivecharactertextsplitter","title":"Import the RecursiveCharacterTextSplitter","text":"<pre><code>from indoxArcg.splitter import RecursiveCharacterTextSplitter\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/#initialize_recursivecharactertextsplitter","title":"Initialize RecursiveCharacterTextSplitter","text":"<pre><code>splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n</code></pre>"},{"location":"indoxArcg/splitters/Recursively_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/","title":"SemanticTextSplitter","text":"<p>SemanticTextSplitter is a Python class designed for splitting text into semantically meaningful chunks using a BERT tokenizer. It utilizes the semantic_text_splitter library to split input text into chunks that preserve semantic meaning while ensuring each chunk does not exceed a specified maximum number of tokens.</p> <p>Note: This class requires the semantic_text_splitter and tokenizers libraries. Ensure you have these dependencies installed before using this class.</p> <p>To use SemanticTextSplitter:</p> <pre><code>from indoxArcg.splitter import SemanticTextSplitter\n\nsplitter = SemanticTextSplitter(chunk_size=400, model_name=\"bert-base-uncased\")\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>chunk_size [int]: The maximum number of tokens allowed in each chunk (default: 400).</li> <li>model_name [str]: The name of the pre-trained model to use for the tokenizer (default: \"bert-base-uncased\").</li> </ul>"},{"location":"indoxArcg/splitters/Semantic_splitter/#usage","title":"Usage","text":""},{"location":"indoxArcg/splitters/Semantic_splitter/#setting_up_the_python_environment","title":"Setting Up the Python Environment","text":""},{"location":"indoxArcg/splitters/Semantic_splitter/#windows","title":"Windows","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <p>2Activate the virtual environment:</p> <pre><code>indoxArcg\\Scripts\\activate\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#macoslinux","title":"macOS/Linux","text":"<ol> <li>Create the virtual environment:</li> </ol> <pre><code>python -m venv indoxArcg\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>source indoxArcg/bin/activate\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#install_required_libraries","title":"Install Required Libraries","text":"<pre><code>pip install semantic-text-splitter tokenizers\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#import_the_semantictextsplitter","title":"Import the SemanticTextSplitter","text":"<pre><code>from indoxArcg.splitter import SemanticTextSplitter\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#initialize_semantictextsplitter","title":"Initialize SemanticTextSplitter","text":"<pre><code>splitter = SemanticTextSplitter(chunk_size=400, model_name=\"bert-base-uncased\")\n</code></pre>"},{"location":"indoxArcg/splitters/Semantic_splitter/#split_and_processing_chunks","title":"Split And Processing Chunks","text":"<pre><code>text = \"\"\"\nThis is a long piece of text that needs to be split into smaller chunks.\nIt contains multiple sentences and paragraphs.\n\nHere's another paragraph with some content.\n\nAnd one more paragraph to demonstrate the splitting process.\n\"\"\"\n\nchunks = splitter.split_text(text)\n\nprint(f\"The text has been split into {len(chunks)} chunks.\")\nfor i, chunk in enumerate(chunks, 1):\n    print(f\"Chunk {i}:\")\n    print(chunk)\n    print(\"====\")\n</code></pre>"},{"location":"indoxArcg/tools/multiquery/","title":"MultiQueryRetrieval Documentation","text":""},{"location":"indoxArcg/tools/multiquery/#overview","title":"Overview","text":"<p>The <code>MultiQueryRetrieval</code> class implements an advanced information retrieval technique that generates multiple queries from an original query, retrieves relevant information for each generated query, and combines the results to produce a comprehensive final response.</p>"},{"location":"indoxArcg/tools/multiquery/#class_multiqueryretrieval","title":"Class: MultiQueryRetrieval","text":""},{"location":"indoxArcg/tools/multiquery/#initialization","title":"Initialization","text":"<pre><code>def __init__(self, llm, vector_database, top_k: int = 3):\n</code></pre> <ul> <li>llm: The language model used for query generation and response synthesis.</li> <li>vector_database: The vector database used for information retrieval.</li> <li>top_k: The number of top results to retrieve for each query (default: 3).</li> </ul>"},{"location":"indoxArcg/tools/multiquery/#methods","title":"Methods","text":"<p>generate_queries</p> <pre><code>def generate_queries(self, original_query: str) -&gt; List[str]:\n</code></pre> <p>Generates multiple queries from the original query.</p> <ul> <li>original_query: The original user query.</li> <li>Returns: A list of generated queries.</li> </ul> <p>retrieve_information</p> <pre><code>def retrieve_information(self, queries: List[str]) -&gt; List[str]:\n</code></pre> <p>Retrieves relevant information for each generated query.</p> <ul> <li>queries: A list of queries to use for information retrieval.</li> <li>Returns: A list of relevant passages retrieved from the vector database.</li> </ul> <p>generate_response</p> <pre><code>def generate_response(self, original_query: str, context: List[str]) -&gt; str:\n</code></pre> <p>Generates a final response based on the original query and retrieved context.</p> <ul> <li>original_query: The original user query.</li> <li>context: A list of relevant passages to use as context.</li> <li>Returns: The generated response.</li> </ul> <p>run</p> <pre><code>def run(self, query: str) -&gt; str:\n</code></pre> <p>Executes the full multi-query retrieval process.</p> <ul> <li>query: The original user query.</li> <li>Returns: The final generated response.</li> </ul>"},{"location":"indoxArcg/tools/multiquery/#usage","title":"Usage","text":"<p>To use the MultiQueryRetrieval functionality within the <code>RAG</code> class:</p> <pre><code>from indoxArcg.pipelines.rag import RAG\n\nretriever = RAG(llm,vector_store)\nanswer = retriever.infer(\n    question=query,\n    top_k=5,\n    use_clustering=False,\n    use_multi_query=True\n)\n</code></pre>"},{"location":"indoxArcg/tools/multivector/","title":"<code>MultiVectorRetriever</code>","text":""},{"location":"indoxArcg/tools/multivector/#overview","title":"Overview","text":"<p>The <code>MultiVectorRetriever</code> class allows users to perform similarity searches across multiple vector stores in parallel and retrieve results based on their similarity score. This class is particularly useful for projects that need to integrate several vector stores and combine search results, such as building a multivector hybrid search system.</p>"},{"location":"indoxArcg/tools/multivector/#key_features","title":"Key Features","text":"<ul> <li>Perform similarity searches on multiple vector stores simultaneously.</li> <li>Combine and sort the results from all vector stores based on similarity scores.</li> <li>Handle exceptions during the search process and log any issues.</li> </ul>"},{"location":"indoxArcg/tools/multivector/#constructor_init_self_vector_stores_listany","title":"Constructor: <code>__init__(self, vector_stores: List[Any])</code>","text":"<p>This method initializes the <code>MultiVectorRetriever</code> object.</p>"},{"location":"indoxArcg/tools/multivector/#parameters","title":"Parameters:","text":"<ul> <li><code>vector_stores</code> (List[Any]): A list of initialized vector store instances, each capable of performing similarity searches (e.g., Deeplake, ApacheCassandra).</li> </ul>"},{"location":"indoxArcg/tools/multivector/#method_similarity_search_with_scoreself_query_str_k_int_5_-_listtupleany_float","title":"Method: <code>similarity_search_with_score(self, query: str, k: int = 5) -&gt; List[Tuple[Any, float]]</code>","text":"<p>Executes similarity searches across all vector stores, returning the most relevant results based on their similarity scores.</p>"},{"location":"indoxArcg/tools/multivector/#parameters_1","title":"Parameters:","text":"<ul> <li><code>query</code> (str): The query text to be searched across the vector stores.</li> <li><code>k</code> (int): The number of results to return. Defaults to 5.</li> </ul>"},{"location":"indoxArcg/tools/multivector/#returns","title":"Returns:","text":"<ul> <li>List[Tuple[Any, float]]: A list of tuples where each tuple contains a document and a similarity score. The list is sorted in descending order of similarity.</li> </ul>"},{"location":"indoxArcg/tools/multivector/#raises","title":"Raises:","text":"<ul> <li>Logs any exceptions encountered during the search process.</li> </ul>"},{"location":"indoxArcg/tools/multivector/#example_usage","title":"Example Usage","text":"<p>In this example, we initialize two vector stores (<code>Deeplake</code> and <code>ApacheCassandra</code>), add documents to them, and then create a <code>MultiVectorRetriever</code> instance. A query is then run across both vector stores, retrieving the top 5 most similar results.</p> <pre><code>from indoxArcg.vector_stores import Chroma, Milvus, ApacheCassandra, FAISS, PGVector, Deeplake\nfrom indoxArcg.multi_vector_retriever import MultiVectorRetriever\n\n# Define vector store instances\ndb1 = Deeplake(embedding_function=embed_openai_indoxArcg, collection_name=\"sample\")\ndb2 = ApacheCassandra(embedding_function=embed_openai_indoxArcg, keyspace=\"sample\")\n\n# Add documents to the vector stores\ndb1.add(docs=docs1)\ndb2.add(docs=docs2)\n\n# Initialize MultiVectorRetriever with multiple vector stores\nmultivector = MultiVectorRetriever(vector_stores=[db1, db2])\n\n# Define the query\nquery = \"How cinderella reach her happy ending?\"\n\n```python\nfrom indoxArcg.pipelines.rag import RAG\n\nretriever = RAG(llm,multivector)\nanswer = retriever.infer(\n    question=query,\n    top_k=5,\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/","title":"Vectorstore Integration in indoxArcg","text":"<p>This documentation hub explains how indoxArcg integrates with various vectorstores, organized by category to simplify selection and setup. Each category serves distinct use cases within the indoxArcg RAG/CAG architecture.</p>"},{"location":"indoxArcg/vectorstores/#categories_overview","title":"Categories Overview","text":""},{"location":"indoxArcg/vectorstores/#1_purpose-built_vector_databases","title":"1. Purpose-Built Vector Databases","text":"<p>Optimized for large-scale, high-performance vector workloads in indoxArcg. These databases are designed exclusively for vector operations, enabling fast similarity search and horizontal scalability.  </p> <p>Supported in indoxArcg: - Pinecone - Milvus - Qdrant - Weaviate - Vespa - Chroma - Deep Lake - Vearch  </p> <p>Role in indoxArcg: Ideal for standalone vector indexing and retrieval in production-scale RAG pipelines. Guide: Purpose-Built Vector Databases </p>"},{"location":"indoxArcg/vectorstores/#2_general-purpose_databases_with_vector_extensions","title":"2. General-Purpose Databases with Vector Extensions","text":"<p>Leverage existing databases for hybrid (vector + traditional) workflows. Integrate vector search into indoxArcg workflows using familiar systems like PostgreSQL or MongoDB.  </p> <p>Supported in indoxArcg: - PostgreSQL (pgvector/Lantern) - Redis (RediSearch) - MongoDB (Atlas Vector Search) - Apache Cassandra - Couchbase - SingleStore - DuckDB - Pathway  </p> <p>Role in indoxArcg: Add vector search to transactional data without migrating outside indoxArcg. Guide: General-Purpose Databases </p>"},{"location":"indoxArcg/vectorstores/#3_graph_databases_with_vector_support","title":"3. Graph Databases with Vector Support","text":"<p>Combine graph relationships and vector semantics in indoxArcg. Enhance context-aware retrieval by linking vector embeddings with graph structures.  </p> <p>Supported in indoxArcg: - Neo4jVector - MemgraphVector  </p> <p>Role in indoxArcg: Power complex CAG workflows (e.g., tracing dependencies + semantic context). Guide: Graph Databases </p>"},{"location":"indoxArcg/vectorstores/#4_embeddedlightweight_libraries","title":"4. Embedded/Lightweight Libraries","text":"<p>Minimal setup for prototyping or small-scale indoxArcg deployments.  </p> <p>Supported in indoxArcg: - FAISS  </p> <p>Role in indoxArcg: Rapid experimentation with vector search before scaling to production. Guide: Embedded Libraries </p>"},{"location":"indoxArcg/vectorstores/#how_to_choose_for_indoxarcg","title":"How to Choose for indoxArcg","text":"Category Best For indoxArcg Use Cases Scalability Hybrid Queries Purpose-Built Vector Databases Production RAG with massive datasets High Limited General-Purpose Databases Adding vectors to existing SQL/NoSQL data Medium High Graph Databases Context-aware CAG with relationships Medium High Embedded Libraries Prototyping or offline analysis Low None"},{"location":"indoxArcg/vectorstores/#integration_workflow","title":"Integration Workflow","text":"<ol> <li>Evaluate Needs: Use the comparison table above to narrow down categories.  </li> <li>Review Guides: Click category links for setup, configuration, and indoxArcg best practices.  </li> <li>Test: Use indoxArcg\u2019s <code>vectorstore_test</code> module to validate performance.  </li> </ol>"},{"location":"indoxArcg/vectorstores/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Mismatched Latency: Ensure Purpose-Built Databases are used for latency-sensitive RAG.  </li> <li>Hybrid Query Limits: Graph/General-Purpose Databases require indoxArcg <code>v2.1+</code>.  </li> <li>Contributing: Report issues or suggest improvements here.  </li> </ul>"},{"location":"indoxArcg/vectorstores/#next_steps","title":"Next Steps","text":"<p>Explore category-specific guides: 1. Purpose-Built Databases 2. General-Purpose Databases 3. Graph Databases 4. Embedded Libraries </p>"},{"location":"indoxArcg/vectorstores/embedded-libraries/","title":"Embedded Libraries","text":"<p>For more information about vectorstores, see the Vectorstores Overview.</p>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#embeddedlightweight_libraries_in_indoxarcg","title":"Embedded/Lightweight Libraries in indoxArcg","text":"<p>This guide covers indoxArcg integrations with lightweight vector libraries for prototyping and small-scale deployments.</p>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#supported_vectorstores","title":"Supported Vectorstores","text":""},{"location":"indoxArcg/vectorstores/embedded-libraries/#faiss_facebook_ai_similarity_search","title":"FAISS (Facebook AI Similarity Search)","text":"<p>In-memory vector library for efficient similarity search.</p>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#installation","title":"Installation","text":"<pre><code>pip install faiss-cpu  # or faiss-gpu for CUDA support\n</code></pre>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#indoxarcg_integration","title":"indoxArcg Integration","text":"<pre><code>from indoxArcg.vector_stores import FAISSVectorStore\nfrom indoxArcg.embeddings import HuggingFaceEmbedding\n\n# Initialize with embedding model\nembed = HuggingFaceEmbedding(model=\"all-MiniLM-L6-v2\")\ndb = FAISSVectorStore(embedding=embed)\n\n# Add documents\ndb.add(docs=chunks)\n\n# Query with RAG pipeline\nquery = \"How does indoxArcg handle document chunking?\"\nretriever = indoxArcg.QuestionAnswer(\n    vector_database=db,\n    llm=mistral_qa,\n    top_k=5,\n    document_relevancy_filter=True\n)\nanswer = retriever.invoke(query=query)\n</code></pre>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#key_features","title":"Key Features","text":"<ul> <li>Zero Server Setup: Runs entirely in memory</li> <li>GPU Acceleration: Optional CUDA support via <code>faiss-gpu</code></li> <li>Persistent Storage: Save/load indexes to disk:   <pre><code>db.save_local(\"faiss_index\")  # Save\ndb.load_local(\"faiss_index\")  # Load\n</code></pre></li> </ul>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#when_to_use_faiss","title":"When to Use FAISS","text":"<ol> <li>Prototyping: Test RAG workflows locally before cloud deployment</li> <li>Small Datasets: &lt;1M embeddings with &lt;=768 dimensions</li> <li>Cost Sensitivity: Avoid managed service costs</li> <li>Offline Requirements: Air-gapped environments</li> </ol>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#limitations","title":"Limitations","text":"Aspect FAISS Capacity Max Embeddings ~1M (RAM-bound) Persistence Manual save/load Hybrid Search Not Supported Scalability Single-node only"},{"location":"indoxArcg/vectorstores/embedded-libraries/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/vectorstores/embedded-libraries/#common_issues","title":"Common Issues","text":"<ol> <li>Installation Failures</li> <li>M1/M2 Macs: Use conda:      <pre><code>conda install -c conda-forge faiss-cpu\n</code></pre></li> <li> <p>CUDA Errors: Match faiss-gpu version with CUDA toolkit</p> </li> <li> <p>Dimension Mismatch <pre><code># Verify embedding dimensions\nprint(len(embed.embed_query(\"test\")))  # Must match FAISS index\n</code></pre></p> </li> <li> <p>Memory Errors</p> </li> <li>Reduce batch size when adding documents:      <pre><code>db.add(docs=chunks, batch_size=100)\n</code></pre></li> </ol>"},{"location":"indoxArcg/vectorstores/embedded-libraries/#next_steps","title":"Next Steps","text":"<p>Return to Vectorstore Hub | Purpose-Built Databases \u27a1\ufe0f ```</p>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/","title":"General Purpose Vector Databases","text":"<p>This guide covers general-purpose vector databases supported by indoxArcg for storing and querying vector embeddings.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/#supported_databases","title":"Supported Databases","text":""},{"location":"indoxArcg/vectorstores/general-purpose-databases/#1_postgresql_with_pgvector","title":"1. PostgreSQL with pgvector","text":"<p>PostgreSQL with the pgvector extension provides efficient vector similarity search.</p> <pre><code>from indoxArcg.vectorstores import PostgresVectorStore\n\nstore = PostgresVectorStore(\n    connection_string=\"postgresql://user:pass@localhost:5432/db\",\n    table_name=\"embeddings\",\n    dimension=1536\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/#2_sqlite_with_sqlite-vss","title":"2. SQLite with sqlite-vss","text":"<p>SQLite with vector similarity search extension for lightweight deployments.</p> <pre><code>from indoxArcg.vectorstores import SQLiteVectorStore\n\nstore = SQLiteVectorStore(\n    db_path=\"vectors.db\",\n    table_name=\"embeddings\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/#3_mongodb_atlas","title":"3. MongoDB Atlas","text":"<p>MongoDB Atlas with vector search capabilities.</p> <pre><code>from indoxArcg.vectorstores import MongoDBVectorStore\n\nstore = MongoDBVectorStore(\n    connection_string=\"mongodb+srv://...\",\n    database=\"vectors\",\n    collection=\"embeddings\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/#common_operations","title":"Common Operations","text":"<pre><code># Add vectors\nstore.add_vectors(vectors, metadata)\n\n# Search\nresults = store.search(query_vector, top_k=5)\n\n# Delete\nstore.delete(ids=[\"doc1\", \"doc2\"])\n\n# Update\nstore.update(id=\"doc1\", vector=new_vector, metadata=new_metadata)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-databases/#performance_considerations","title":"Performance Considerations","text":"<ul> <li>Index types and configuration</li> <li>Batch operations</li> <li>Query optimization</li> <li>Connection pooling </li> </ul>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/","title":"General Purpose Vector Databases","text":"<p>For more information about vectorstores, see the Vectorstores Overview.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#general-purpose_databases_with_vector_extensions_in_indoxarcg","title":"General-Purpose Databases with Vector Extensions in indoxArcg","text":"<p>This guide covers indoxArcg integrations with traditional databases enhanced with vector search capabilities.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#supported_vectorstores","title":"Supported Vectorstores","text":""},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#1_postgresql_pgvectorlantern","title":"1. PostgreSQL (pgvector/Lantern)","text":"<p>Relational database with multiple vector extensions.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#pgvector_setup","title":"pgvector Setup","text":"<pre><code>pip install pgvector psycopg2\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#lantern_setup","title":"Lantern Setup","text":"<pre><code>pip install lantern-postgres\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#indoxarcg_integration","title":"indoxArcg Integration","text":"<pre><code># Using pgvector\nfrom indoxArcg.vector_stores import PGVectorStore\npg_db = PGVectorStore(\n    host=\"localhost\",\n    port=5432,\n    dbname=\"indoxarcg_db\",\n    user=\"admin\",\n    password=\"secret\",\n    collection_name=\"docs\",\n    embedding=HuggingFaceEmbedding()\n)\n\n# Using LanternDB\nfrom indoxArcg.vector_stores import LanternDB\nlantern_db = LanternDB(\n    collection_name=\"indoxarcg_docs\",\n    embedding_function=HuggingFaceEmbedding(),\n    connection_params={\n        'dbname': 'indoxarcg_db',\n        'user': 'admin',\n        'password': os.getenv('PG_PASSWORD'),\n        'host': 'pg.indoxarcg.com',\n        'port': 5432\n    }\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#postgresql_extension_comparison","title":"PostgreSQL Extension Comparison","text":"Feature pgvector Lantern Index Type IVFFlat HNSW Parallel Builds Limited Optimized Max Dimensions 2000 16000 Search Speed Good Excellent indoxArcg Setup Low Medium"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#2_redis","title":"2. Redis","text":"<p>In-memory database with vector search support.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#configuration","title":"Configuration","text":"<pre><code>from indoxArcg.vector_stores import RedisDB\n\nredis_db = RedisDB(\n    host=\"redis.indoxarcg.com\",\n    port=6379,\n    password=os.getenv('REDIS_PASSWORD'),\n    embedding=HuggingFaceEmbedding(),\n    prefix=\"indoxarcg:\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#3_mongodb","title":"3. MongoDB","text":"<p>Document database with Atlas vector search.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#vector_index_setup","title":"Vector Index Setup","text":"<pre><code>db = MongoDB(\n    collection_name=\"indoxarcg_collection\",\n    embedding_function=embed_model,\n    connection_string=\"mongodb+srv://user:pass@cluster.indoxarcg.mongodb.net/\",\n    database_name=\"vector_db\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#4_apache_cassandra","title":"4. Apache Cassandra","text":"<p>Distributed NoSQL database with vector support.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#schema_configuration","title":"Schema Configuration","text":"<pre><code>from indoxArcg.vector_stores import ApacheCassandra\n\ncassandra_db = ApacheCassandra(\n    embedding_function=embed_model,\n    keyspace=\"indoxarcg_keyspace\"\n)\ncassandra_db._setup_keyspace()\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#5_couchbase","title":"5. Couchbase","text":"<p>JSON document store with vector indexing.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#full-text_search_setup","title":"Full-Text Search Setup","text":"<pre><code>db = Couchbase(\n    embedding_function=embed_model,\n    bucket_name=\"indoxarcg_bucket\",\n    cluster_url=\"couchbase://db.indoxarcg.com\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#6_singlestore","title":"6. SingleStore","text":"<p>Distributed SQL database with vector indexing.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#hybrid_search_configuration","title":"Hybrid Search Configuration","text":"<pre><code>db = SingleStoreVectorDB(\n    connection_params={\n        'host': 'svc.indoxarcg.com',\n        'user': 'admin',\n        'password': os.getenv('SINGLESTORE_PWD'),\n        'database': 'vector_db'\n    },\n    embedding_function=embed_model,\n    use_vector_index=True\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#7_duckdb","title":"7. DuckDB","text":"<p>Lightweight in-memory OLAP database.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#embedded_usage","title":"Embedded Usage","text":"<pre><code>from indoxArcg.vector_stores import DuckDB\n\nvector_store = DuckDB(\n    embedding_function=embed_model,\n    table_name=\"indoxarcg_embeddings\"\n)\nvector_store.add(texts=chunks)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#8_pathway","title":"8. Pathway","text":"<p>Real-time vector processing engine.</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#streaming_setup","title":"Streaming Setup","text":"<pre><code>from indoxArcg.vector_stores import PathwayVectorClient\n\nclient = PathwayVectorClient(\n    host=\"pathway.indoxarcg.com\",\n    port=8080\n)\nclient.add(docs=real_time_stream)\n</code></pre>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#comparison_of_general-purpose_stores","title":"Comparison of General-Purpose Stores","text":"Database Vector Index Hybrid Queries Latency indoxArcg Setup Complexity PostgreSQL (pgvector) IVFFlat SQL + Vectors Medium Low PostgreSQL (Lantern) HNSW SQL + Vectors Low Medium Redis FLAT JSON + Vectors Low Low MongoDB HNSW Agg Pipeline Medium Medium Cassandra CQL CQL + Vectors High High Couchbase N1QL N1QL + Vectors Medium Medium SingleStore ANN SQL + ANN Low High DuckDB \u274c SQL Only Low Low Pathway Streaming Real-time Ultra High"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#postgresqllantern_specific","title":"PostgreSQL/Lantern Specific","text":"<ol> <li> <p>Extension Not Enabled <pre><code>-- For pgvector\nCREATE EXTENSION vector;\n-- For Lantern\nCREATE EXTENSION lantern;\n</code></pre></p> </li> <li> <p>HNSW Index Optimization <pre><code># Lantern-specific index creation\nlantern_db.create_index(\n    index_type=\"hnsw\",\n    metric=\"cosine\",\n    m=16,\n    ef_construction=64\n)\n</code></pre></p> </li> <li> <p>Dimension Validation <pre><code># Verify embedding dimensions match\nassert len(embed_model.embed_query(\"test\")) == 768, \"Mismatch with DB schema\"\n</code></pre></p> </li> </ol> <p>[... Rest of troubleshooting section remains unchanged ...]</p>"},{"location":"indoxArcg/vectorstores/general-purpose-vector-databases/#next_steps","title":"Next Steps","text":"<p>Return to Vectorstore Hub | Graph Databases \u27a1\ufe0f ```</p>"},{"location":"indoxArcg/vectorstores/graph-databases/","title":"Graph Databases","text":"<p>For more information about vectorstores, see the Vectorstores Overview.</p> <p>This guide covers indoxArcg integrations with graph databases that combine vector search with graph traversal capabilities.</p>"},{"location":"indoxArcg/vectorstores/graph-databases/#supported_vectorstores","title":"Supported Vectorstores","text":""},{"location":"indoxArcg/vectorstores/graph-databases/#1_memgraphvector","title":"1. MemgraphVector","text":"<p>Real-time graph database with hybrid vector/keyword search.</p>"},{"location":"indoxArcg/vectorstores/graph-databases/#installation","title":"Installation","text":"<pre><code>pip install neo4j\n</code></pre>"},{"location":"indoxArcg/vectorstores/graph-databases/#indoxarcg_integration","title":"indoxArcg Integration","text":"<pre><code>from indoxArcg.vector_stores import MemgraphVector\n\nmg_db = MemgraphVector(\n    uri=\"bolt://mg.indoxarcg.com:7687\",\n    username=\"admin\",\n    password=os.getenv('MEMGRAPH_PWD'),\n    embedding_function=HuggingFaceEmbedding(),\n    search_type='hybrid'  # 'vector' | 'keyword' | 'hybrid'\n)\n\n# Hybrid search example\nresults = mg_db.search(\n    query=\"AI in healthcare\",\n    search_type=\"hybrid\",\n    k=5\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/graph-databases/#2_neo4jvector","title":"2. Neo4jVector","text":"<p>Graph database with native vector indexing.</p>"},{"location":"indoxArcg/vectorstores/graph-databases/#setup","title":"Setup","text":"<pre><code>docker run -p 7474:7474 -p 7687:7687 neo4j:5.16.0\n</code></pre>"},{"location":"indoxArcg/vectorstores/graph-databases/#indoxarcg_configuration","title":"indoxArcg Configuration","text":"<pre><code>from indoxArcg.vector_stores import Neo4jGraph\n\nneo4j_db = Neo4jGraph(\n    uri=\"bolt://neo4j.indoxarcg.com:7687\",\n    username=\"neo4j\",\n    password=os.getenv('NEO4J_PWD')\n)\n\n# Store graph documents with entity relationships\nneo4j_db.add_graph_documents(\n    graph_documents=knowledge_graph,\n    base_entity_label=True,\n    include_source=True\n)\n\n# Query parent-child relationships\nrelationships = neo4j_db.search_relationships_by_entity(\n    entity_id=\"LLM_Research\",\n    relationship_type=\"SUB_FIELD\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/graph-databases/#comparison_of_graph_databases","title":"Comparison of Graph Databases","text":"Feature MemgraphVector Neo4jVector Search Types Vector, Keyword, Hybrid Vector + Graph Patterns Latency &lt;50ms (real-time) 100-500ms indoxArcg Setup Medium High Relationship Queries Basic Cypher Advanced Scalability High Medium Hybrid Search Weights Configurable N/A"},{"location":"indoxArcg/vectorstores/graph-databases/#key_use_cases","title":"Key Use Cases","text":"<ol> <li> <p>Knowledge Graphs    Connect entities with semantic context:    <pre><code>mg_db.search(query=\"Blockchain in finance\", search_type=\"hybrid\")\n</code></pre></p> </li> <li> <p>Fraud Detection    Trace suspicious patterns with vector-enhanced relationships:    <pre><code>neo4j_db.search_relationships_by_entity(\n    entity_id=\"Suspicious_Transaction_123\",\n    relationship_type=\"LINKED_TO\"\n)\n</code></pre></p> </li> <li> <p>Recommendation Systems    Combine user behavior graphs with content similarity:    <pre><code>mg_db.retrieve(query=\"Sci-fi movies\", search_type=\"vector\", top_k=10)\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/vectorstores/graph-databases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/vectorstores/graph-databases/#common_issues","title":"Common Issues","text":"<ol> <li> <p>Connection Timeouts    Verify graph database status:    <pre><code># For Neo4j\ncurl http://neo4j.indoxarcg.com:7474\n\n# For Memgraph\nmg_client = neo4j.GraphDatabase.driver(uri, auth=(user, pwd))\nmg_client.verify_connectivity()\n</code></pre></p> </li> <li> <p>Missing Relationships    Ensure proper schema initialization:    <pre><code>neo4j_db._init_schema()  # Creates required constraints\n</code></pre></p> </li> <li> <p>Vector Index Failures    Check embedding dimensions match:    <pre><code>print(len(embed_model.embed_query(\"test\")))  # Should match DB config\n</code></pre></p> </li> <li> <p>Hybrid Search Imbalance    Adjust weights in Memgraph:    <pre><code>results = mg_db.search(\n    query=\"Quantum computing\",\n    search_type=\"hybrid\",\n    vector_weight=0.7,\n    keyword_weight=0.3\n)\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/vectorstores/graph-databases/#next_steps","title":"Next Steps","text":"<p>Return to Vectorstore Hub | Embedded Libraries \u27a1\ufe0f ```</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/","title":"Purpose Built Vector Databases","text":"<p>For more information about vectorstores, see the Vectorstores Overview.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#supported_vectorstores","title":"Supported Vectorstores","text":""},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#1_milvus","title":"1. Milvus","text":"<p>Cloud-native vector database for scalable similarity search.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#installation","title":"Installation","text":"<pre><code># Install Docker\ndocker --version\n\n# Clone Milvus repo\ngit clone https://github.com/milvus-io/milvus.git\ncd milvus/deployments/docker\n\n# Start Milvus\ndocker-compose up -d\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#indoxarcg_integration","title":"indoxArcg Integration","text":"<pre><code>from pymilvus import connections\nconnections.connect(host='127.0.0.1', port='19530')\n\nfrom indoxArcg.vector_stores import MilvusVectorStore\ndb = MilvusVectorStore(collection_name=\"indoxarcg_collection\", embedding=embed)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#2_pinecone","title":"2. Pinecone","text":"<p>Managed vector database for production AI applications.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#setup","title":"Setup","text":"<pre><code>from pinecone import ServerlessSpec\n\npc.create_index(\n    name=\"indoxarcg-index\",\n    dimension=768,\n    metric=\"cosine\",\n    spec=ServerlessSpec(cloud='aws', region='us-east-1')\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#indoxarcg_usage","title":"indoxArcg Usage","text":"<pre><code>from indoxArcg.vector_stores import PineconeVectorStore\ndb = PineconeVectorStore(\n    embedding=embed,\n    pinecone_api_key=os.getenv('PINECONE_API_KEY'),\n    index_name=\"indoxarcg-index\"\n)\ndb.add(docs=docs)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#3_qdrant","title":"3. Qdrant","text":"<p>High-performance open-source vector search engine.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#configuration","title":"Configuration","text":"<pre><code>from indoxArcg.vector_stores import Qdrant\n\nqdrant_db = Qdrant(\n    collection_name=\"indoxarcg_collection\",\n    embedding_function=HuggingFaceEmbedding(),\n    url=\"https://qdrant-api-url\",\n    api_key=os.getenv('QDRANT_API_KEY')\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#4_weaviate","title":"4. Weaviate","text":"<p>Semantic search engine with vector+graph hybrid capabilities.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#docker_setup","title":"Docker Setup","text":"<pre><code>docker run -d -p 8080:8080 semitechnologies/weaviate:latest\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#indoxarcg_integration_1","title":"indoxArcg Integration","text":"<pre><code>from indoxArcg.vector_stores import WeaviateVectorStore\ndb = WeaviateVectorStore(\n    client=weaviate.Client(\"http://localhost:8080\"),\n    index_name=\"indoxarcg\",\n    text_key=\"content\"\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#5_chroma","title":"5. Chroma","text":"<p>Lightweight embedding store for AI applications.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#quick_start","title":"Quick Start","text":"<pre><code>from indoxArcg.vector_stores import ChromaVectorStore\ndb = ChromaVectorStore(\n    collection_name=\"indoxarcg_docs\",\n    embedding=HuggingFaceEmbedding()\n)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#6_deeplake","title":"6. DeepLake","text":"<p>Vector store for deep learning datasets.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#usage","title":"Usage","text":"<pre><code>from indoxArcg.vector_stores import Deeplake\ndb = Deeplake(\n    collection_name=\"indoxarcg_dataset\",\n    embedding_function=embed_openai_indox\n)\ndb.add(docs=processed_docs)\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#7_vearch","title":"7. Vearch","text":"<p>Distributed vector search platform.</p>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#configuration_1","title":"Configuration","text":"<pre><code>from indoxArcg.vector_stores import Vearch\n\ndb = Vearch(\n    embedding_function=embed_model,\n    db_name=\"indoxarcg_db\",\n    space_name=\"indoxarcg_space\"\n)\ndb.create_space_schema()\n</code></pre>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#comparison_of_purpose-built_stores","title":"Comparison of Purpose-Built Stores","text":"Vectorstore Scalability Hybrid Search Cloud Managed indoxArcg Setup Complexity Milvus High \u2705 Self-hosted Medium Pinecone Very High \u274c Fully Managed Low Qdrant High \u2705 Both Medium Weaviate Medium \u2705 (Graph) Both High Chroma Low \u274c Self-hosted Low DeepLake Medium \u274c Managed Medium Vearch High \u2705 Self-hosted High"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#troubleshooting","title":"Troubleshooting","text":""},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#common_issues","title":"Common Issues","text":"<ol> <li>Connection Timeouts </li> <li>Verify ports (<code>19530</code> for Milvus, <code>8080</code> for Weaviate)</li> <li> <p>Check Docker container status</p> </li> <li> <p>Dimension Mismatch    Ensure embedding dimensions match index configuration:    <pre><code>print(embed.embed_query(\"test\").shape)  # Should match vectorstore config\n</code></pre></p> </li> <li> <p>Authentication Failures    Validate API keys using test calls:    <pre><code># For Pinecone\nimport pinecone\npinecone.list_indexes()  # Should return without errors\n</code></pre></p> </li> </ol>"},{"location":"indoxArcg/vectorstores/purpose-built-vector-databases/#next_steps","title":"Next Steps","text":"<p>Return to Vectorstore Hub | General-Purpose Databases \u27a1\ufe0f ```</p>"},{"location":"indoxGen/AttributePromptSynth/","title":"Attribute-based Prompt Synthesis","text":""},{"location":"indoxGen/AttributePromptSynth/#overview","title":"Overview","text":"<p><code>AttributePromptSynth</code> is a Python class designed to generate synthetic data based on a set of attributes and user instructions. It utilizes language models (LLMs) to generate prompts and retrieve responses that can be saved as a DataFrame or exported to an Excel file.</p>"},{"location":"indoxGen/AttributePromptSynth/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"indoxGen/AttributePromptSynth/#installation","title":"Installation","text":"<p>To use the <code>AttributePromptSynth</code> class, you need to have Python 3.9+ installed. You can install the <code>indoxGen</code> package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"indoxGen/AttributePromptSynth/#language-model-setup","title":"Language Model Setup","text":"<p><code>AttributePromptSynth</code> requires an LLM (Language Model) for generating responses from provided prompts. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up the language model for this class:</p> <pre><code>from indoxGen.llms import IndoxApi\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\nINDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre> <p>The <code>indoxGen</code> library supports various models, including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides routing for OpenAI, enabling easy switching between different models.</p>"},{"location":"indoxGen/AttributePromptSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use the <code>AttributePromptSynth</code> class:</p> <pre><code>from indoxGen.synthCore import AttributePromptSynth\n\n# Define the arguments for generating prompts\nargs = {\n    \"instruction\": \"Generate a {adjective} sentence that is {length}.\",\n    \"attributes\": {\n        \"adjective\": [\"serious\", \"funny\"],\n        \"length\": [\"short\", \"long\"]\n    },\n    \"llm\": LLM\n}\n\n# Create an instance of AttributePromptSynth\ndataset = AttributePromptSynth(prompt_name=\"ExamplePrompt\",\n                                   args=args,\n                                   outputs={})\n\n# Run the prompt generation\ndf = dataset.run()\n\n# Display the generated DataFrame\nprint(df)\n</code></pre>"},{"location":"indoxGen/AttributePromptSynth/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/AttributePromptSynth/#attributepromptsynth","title":"<code>AttributePromptSynth</code>","text":"<p><pre><code>def __init__(self, prompt_name: str, args: dict, outputs: dict):\n</code></pre> Initializes the <code>AttributePromptSynth</code> class.</p> <p>Generates synthetic data based on the attribute setup and returns it as a pandas DataFrame.</p> <p>Returns:  - A <code>pandas.DataFrame</code> containing the generated data.</p> <p><pre><code>def save_to_excel(self, file_path: str, df: pd.DataFrame) -&gt; None:\n</code></pre> Saves the generated DataFrame to an Excel file.</p> <ul> <li><code>file_path</code> (str): The path where the Excel file will be saved.</li> <li><code>df</code> (pd.DataFrame): The DataFrame to be saved.</li> <li>Raises: <code>ValueError</code> if the DataFrame is empty or cannot be saved.</li> </ul>"},{"location":"indoxGen/AttributePromptSynth/#examples","title":"Examples","text":""},{"location":"indoxGen/AttributePromptSynth/#generating_data_based_on_attributes","title":"Generating Data Based on Attributes","text":"<pre><code>from indoxGen.synthCore import DataFromPrompt\nfrom indoxGen.utils import Excel\n\ndataset_file_path = \"output_dataFromPrompt.xlsx\"\n\nexcel_loader = Excel(dataset_file_path) \ndf = excel_loader.load()  \nuser_prompt = \" based on given dataset generate one unique row about soccer\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\nadded_row = DataFromPrompt(llm=LLM, user_instruction=user_prompt, example_data=df, verbose=1).generate_data()\nprint(added_row)\n</code></pre>"},{"location":"indoxGen/AttributePromptSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>AttributePromptSynth</code> are welcome. To contribute, please follow these steps: 1. Fork the repository. 2. Create a new branch for your feature. 3. Add your changes and write tests if applicable. 4. Submit a pull request with a clear description of your changes.</p>"},{"location":"indoxGen/FewShotPromptSynth/","title":"Few-Shot Prompt Synthesis","text":""},{"location":"indoxGen/FewShotPromptSynth/#overview","title":"Overview","text":"<p>FewShotPromptSynth is a Python class designed for generating synthetic data based on few-shot learning examples and user-provided instructions. It utilizes language models to generate diverse datasets, leveraging pre-existing examples for guidance. The class supports outputting the generated data as a pandas DataFrame and allows saving the results to an Excel file.</p>"},{"location":"indoxGen/FewShotPromptSynth/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"indoxGen/FewShotPromptSynth/#installation","title":"Installation","text":"<p>To use FewShotPromptSynth, you need to have Python 3.9+ installed. You can install the required package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"indoxGen/FewShotPromptSynth/#language-model-setup","title":"Language Model Setup","text":"<p>FewShotPromptSynth requires a language model for generating synthetic data. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up a language model for use in the class:</p> <pre><code>from indoxGen.llms import IndoxApi\n\n# Setup for IndoxApi model\nllm = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre> <p>The indoxGen library supports various models, including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, indoxGen provides a router for OpenAI, allowing for easy switching between different models.</p>"},{"location":"indoxGen/FewShotPromptSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use FewShotPromptSynth:</p> <pre><code>from indoxGen.synthCore import FewShotPromptSynth\n# Define your Language Model (LLM) instance (replace with the actual LLM you're using)\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\n# Define a user prompt for the generation task\nuser_prompt = \"Describe the formation of stars in simple terms. Return the result in JSON format, with the key 'description'.\"\n\n# Define few-shot examples (input-output pairs) to help guide the LLM\nexamples = [\n    {\n        \"input\": \"Describe the process of photosynthesis.\",\n        \"output\": \"Photosynthesis is the process by which green plants use sunlight to synthesize food from carbon dioxide and water.\"\n    },\n    {\n        \"input\": \"Explain the water cycle.\",\n        \"output\": \"The water cycle is the process by which water circulates between the earth's oceans, atmosphere, and land, involving precipitation, evaporation, and condensation.\"\n    }\n]\n\n# Create an instance of FewShotPromptSynth using the defined LLM, user prompt, and few-shot examples\ndata_generator = FewShotPromptSynth(\n    llm=LLM,                            # Language model instance (LLM)\n    user_instruction=user_prompt,        # Main user instruction or query\n    examples=examples,                   # Few-shot input-output examples\n    verbose=1,                           # Verbosity level (optional)\n    max_tokens=8000                      # Max tokens for generation (optional)\n)\n\n# Generate the data based on the few-shot setup\ndf = data_generator.generate_data()\n</code></pre>"},{"location":"indoxGen/FewShotPromptSynth/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/FewShotPromptSynth/#fewshotpromptsynth","title":"FewShotPromptSynth","text":"<p><pre><code>def __init__(self, prompt_name: str, args: dict, outputs: dict, examples: List[Dict[str, str]]):\n</code></pre> Initializes the FewShotPromptSynth class.</p> <p><pre><code>def save_to_excel(self, file_path: str, df: pd.DataFrame) -&gt; None:\n</code></pre> Saves the generated DataFrame to an Excel file.</p> <ul> <li><code>file_path</code> (str): The path where the Excel file will be saved.</li> <li><code>df</code> (pd.DataFrame): The DataFrame to be saved.</li> <li>Raises: <code>ValueError</code> if the DataFrame is empty or cannot be saved.</li> </ul>"},{"location":"indoxGen/FewShotPromptSynth/#examples","title":"Examples","text":""},{"location":"indoxGen/FewShotPromptSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve FewShotPromptSynth are welcome. Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol>"},{"location":"indoxGen/GAN-Torch-Tensor/","title":"GAN Torch Tensor","text":""},{"location":"indoxGen/GAN-Torch-Tensor/#overview","title":"Overview","text":"<p>IndoxGen-Torch and IndoxGen-Tensor are two advanced frameworks designed for generating synthetic tabular data using Generative Adversarial Networks (GANs). IndoxGen-Torch is based on PyTorch, while IndoxGen-Tensor is based on TensorFlow, providing flexibility depending on user preference. Both frameworks support various data types, including categorical, continuous, and integer data.</p> <p>These tools extend the capabilities of the IndoxGen project by offering easy-to-use configurations, efficient training pipelines, scalable synthetic data generation, and evaluation methods to assess the quality of the generated data.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Usage</li> <li>IndoxGen-Torch Example</li> <li>IndoxGen-Tensor Example</li> <li>Configuration</li> <li>Evaluation Methods</li> <li>API Reference</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"indoxGen/GAN-Torch-Tensor/#installation","title":"Installation","text":"<p>To install IndoxGen-Torch or IndoxGen-Tensor, you need Python 3.9+.</p> <ul> <li> <p>For IndoxGen-Torch:     pip install indoxgen-torch</p> </li> <li> <p>For IndoxGen-Tensor:     pip install indoxgen-tensor</p> </li> </ul> <p>Both libraries require dependencies like PyTorch or TensorFlow, depending on the version you're using. Ensure your environment supports GPU for faster model training.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#usage","title":"Usage","text":""},{"location":"indoxGen/GAN-Torch-Tensor/#indoxgen-torch-example","title":"IndoxGen-Torch Example","text":"<pre><code>from indoxGen_torch import TabularGANConfig, TabularGANTrainer\nimport pandas as pd\n\n# Load your dataset\ndata = pd.read_csv(\"data/Adult.csv\")\n\n# Define column types\ncategorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\", \"income\"]\nmixed_columns = {\"capital-gain\": \"positive\", \"capital-loss\": \"positive\"}\ninteger_columns = [\"age\", \"fnlwgt\", \"hours-per-week\", \"capital-gain\", \"capital-loss\"]\n\n# Set up the configuration\nconfig = TabularGANConfig(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=128,\n    epochs=50,\n    n_critic=5\n)\n\n# Initialize the trainer\ntrainer = TabularGANTrainer(\n    config=config,\n    categorical_columns=categorical_columns,\n    mixed_columns=mixed_columns,\n    integer_columns=integer_columns\n)\n\n# Train the model\nhistory = trainer.train(data, patience=15)\n\n# Generate synthetic data\nsynthetic_data = trainer.generate_samples(50000)\nprint(synthetic_data.head())\n</code></pre>"},{"location":"indoxGen/GAN-Torch-Tensor/#indoxgen-tensor-example","title":"IndoxGen-Tensor Example","text":"<pre><code>from indoxGen_tensor import TabularGANConfig, TabularGANTrainer\nimport pandas as pd\n\n# Load your dataset\ndata = pd.read_csv(\"data/Adult.csv\")\n\n# Define column types\ncategorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"gender\", \"native-country\", \"income\"]\nmixed_columns = {\"capital-gain\": \"positive\", \"capital-loss\": \"positive\"}\ninteger_columns = [\"age\", \"fnlwgt\", \"hours-per-week\", \"capital-gain\", \"capital-loss\"]\n\n# Set up the configuration\nconfig = TabularGANConfig(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=128,\n    epochs=50,\n    n_critic=5\n)\n\n# Initialize the trainer\ntrainer = TabularGANTrainer(\n    config=config,\n    categorical_columns=categorical_columns,\n    mixed_columns=mixed_columns,\n    integer_columns=integer_columns\n)\n\n# Train the model\nhistory = trainer.train(data, patience=15)\n\n# Generate synthetic data\nsynthetic_data = trainer.generate_samples(50000)\nprint(synthetic_data.head())\n</code></pre>"},{"location":"indoxGen/GAN-Torch-Tensor/#configuration","title":"Configuration","text":"<p>The TabularGANConfig class provides extensive customization options to adapt the model architecture and training process to your dataset.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#key_parameters","title":"Key Parameters:","text":"<ul> <li>input_dim: Dimension of the input noise vector for the generator.</li> <li>generator_layers: List of neuron counts in each layer of the generator.</li> <li>discriminator_layers: List of neuron counts in each layer of the discriminator.</li> <li>learning_rate: The learning rate used in the Adam optimizer.</li> <li>batch_size: Number of samples in each batch during training.</li> <li>epochs: Number of full passes over the dataset during training.</li> <li>n_critic: Number of discriminator updates per generator update, used in WGAN-GP.</li> </ul> <p>These parameters can be modified when initializing the TabularGANConfig.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#evaluation-methods","title":"Evaluation Methods","text":"<p>To ensure the quality of the synthetic data generated by IndoxGen-Torch and IndoxGen-Tensor, we provide several evaluation methods:</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#1_utility_evaluation","title":"1. Utility Evaluation","text":"<p>Utility evaluation compares the performance of machine learning classifiers trained on real data versus synthetic data. We assess the accuracy, AUC (Area Under Curve), and F1 score of various classifiers, including: - Logistic Regression - Decision Tree - Random Forest - Multi-Layer Perceptron (MLP)</p> <p>This helps determine how well synthetic data can replicate the utility of real data in predictive models.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#2_statistical_similarity","title":"2. Statistical Similarity","text":"<p>Statistical similarity evaluates how closely the synthetic data mirrors the real data. We use metrics like: - Wasserstein Distance (for continuous columns) - Jensen-Shannon Divergence (for categorical columns) - Correlation distance (between real and synthetic data correlation matrices)</p> <p>These metrics give insight into how well the synthetic data captures the underlying statistical properties of the real dataset.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#3_privacy_evaluation","title":"3. Privacy Evaluation","text":"<p>We evaluate privacy by measuring: - Distance to Closest Record (DCR): The minimum distance between each real and synthetic data point. - Nearest Neighbor Distance Ratio (NNDR): The ratio of distances between real and synthetic points to ensure diversity and privacy.</p> <p>These metrics help ensure that synthetic data doesn't reveal sensitive information about real data records.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#4_data_drift_evaluation","title":"4. Data Drift Evaluation","text":"<p>Data drift is assessed by comparing the distributions of real and synthetic data using: - Population Stability Index (PSI) for categorical features. - Kolmogorov-Smirnov (K-S) test for numerical features.</p> <p>This ensures that the synthetic data maintains the same distributional properties as the real data, indicating no significant drift.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#5_visualization_of_distributions","title":"5. Visualization of Distributions","text":"<p>We provide tools to visualize the distributions of real and synthetic data side-by-side, allowing for easy comparison. This helps validate that the synthetic data replicates key patterns from the real data, especially for critical variables.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/GAN-Torch-Tensor/#tabularganconfig","title":"TabularGANConfig","text":""},{"location":"indoxGen/GAN-Torch-Tensor/#_init_parameters","title":"<code>__init__</code> parameters:","text":"<ul> <li>input_dim: The size of the random noise vector.</li> <li>generator_layers: A list of integers specifying the number of neurons in each generator layer.</li> <li>discriminator_layers: A list of integers specifying the number of neurons in each discriminator layer.</li> <li>learning_rate: The learning rate for both the generator and discriminator (default: 0.0002).</li> <li>batch_size: The batch size for training (default: 128).</li> <li>epochs: The number of epochs for training (default: 50).</li> <li>n_critic: The number of discriminator updates per generator update (default: 5).</li> </ul>"},{"location":"indoxGen/GAN-Torch-Tensor/#contributing","title":"Contributing","text":"<p>Contributions to IndoxGen-Torch and IndoxGen-Tensor are welcome. Follow these steps to contribute:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature or bug fix.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol> <p>We encourage contributions that improve the GAN models, add more data handling features, or extend the functionality of the framework.</p>"},{"location":"indoxGen/GAN-Torch-Tensor/#license","title":"License","text":"<p>IndoxGen-Torch and IndoxGen-Tensor are released under the MIT License. See LICENSE.md for details.</p>"},{"location":"indoxGen/GenerativeDataSynth/","title":"Generative Data Synthesis","text":""},{"location":"indoxGen/GenerativeDataSynth/#overview","title":"Overview","text":"<p><code>GenerativeDataSynth</code> is a Python class designed for generating synthetic data based on example data and user instructions. It utilizes language models to generate and judge synthetic data points, ensuring diversity and adherence to specified criteria.</p>"},{"location":"indoxGen/GenerativeDataSynth/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"indoxGen/GenerativeDataSynth/#installation","title":"Installation","text":"<p>To use the <code>GenerativeDataSynth</code>, you need to have Python 3.9+ installed. You can install the package using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"indoxGen/GenerativeDataSynth/#language-model-setup","title":"Language Model Setup","text":"<p>The <code>GenerativeDataSynth</code> requires two language models: one for generating data and another for judging data quality. The <code>indoxGen</code> library provides a unified interface for various language models. Here's how to set up the language models:</p> <pre><code>from indoxGen.llms import OpenAi\n\n# Setup for OpenAI model\nopenai = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4-mini\")\n\n# Setup for NVIDIA model\nnemotron = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                  base_url=\"https://integrate.api.nvidia.com/v1\")\n</code></pre> <p>The <code>indoxGen</code> library supports various models including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides a router for OpenAI, allowing for easy switching between different models.</p> <p>When initializing the <code>GenerativeDataSynth</code>, you'll pass these language model instances as the <code>generator_llm</code> and <code>judge_llm</code> parameters.</p>"},{"location":"indoxGen/GenerativeDataSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use the <code>GenerativeDataSynth</code>:</p> <pre><code>from indoxGen.synthCore import GenerativeDataSynth\nfrom indoxGen.llms import OpenAi\n\n# Setup language models\ngenerator_llm = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4-mini\")\njudge_llm = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                   base_url=\"https://integrate.api.nvidia.com/v1\")\n\ncolumns = [\"name\", \"age\", \"occupation\"]\nexample_data = [\n    {\"name\": \"John Doe\", \"age\": 30, \"occupation\": \"Engineer\"},\n    {\"name\": \"Jane Smith\", \"age\": 28, \"occupation\": \"Teacher\"}\n]\nuser_instruction = \"Generate diverse fictional employee data\"\n\ngenerator = GenerativeDataSynth(\n    generator_llm=generator_llm,\n    judge_llm=judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction\n)\n\nsynthetic_data = generator.generate_data(num_samples=100)\nprint(synthetic_data)\n</code></pre>"},{"location":"indoxGen/GenerativeDataSynth/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/GenerativeDataSynth/#generativedatasynth","title":"GenerativeDataSynth","text":""},{"location":"indoxGen/GenerativeDataSynth/#_init_self_generator_llm_judge_llm_columns_example_data_user_instruction_real_datanone_diversity_threshold07_max_diversity_failures20_verbose0","title":"<code>__init__(self, generator_llm, judge_llm, columns, example_data, user_instruction, real_data=None, diversity_threshold=0.7, max_diversity_failures=20, verbose=0)</code>","text":"<p>Initialize the GenerativeDataSynth.</p> <ul> <li><code>generator_llm</code>: Language model for generating data.</li> <li><code>judge_llm</code>: Language model for judging data quality.</li> <li><code>columns</code>: List of column names for the synthetic data.</li> <li><code>example_data</code>: List of example data points.</li> <li><code>user_instruction</code>: Instruction for data generation.</li> <li><code>real_data</code>: Optional list of real data points.</li> <li><code>diversity_threshold</code>: Threshold for determining data diversity (default: 0.7).</li> <li><code>max_diversity_failures</code>: Maximum number of diversity failures before forcing acceptance (default: 20).</li> <li><code>verbose</code>: Verbosity level (0 for minimal output, 1 for detailed feedback) (default: 0).</li> </ul>"},{"location":"indoxGen/GenerativeDataSynth/#generate_dataself_num_samples_int_-_pddataframe","title":"<code>generate_data(self, num_samples: int) -&gt; pd.DataFrame</code>","text":"<p>Generate synthetic data points.</p> <ul> <li><code>num_samples</code>: Number of data points to generate.</li> <li>Returns: DataFrame containing the generated data.</li> </ul>"},{"location":"indoxGen/GenerativeDataSynth/#examples","title":"Examples","text":""},{"location":"indoxGen/GenerativeDataSynth/#generating_employee_data","title":"Generating Employee Data","text":"<pre><code>columns = [\"name\", \"age\", \"department\", \"salary\"]\nexample_data = [\n    {\"name\": \"Alice Johnson\", \"age\": 35, \"department\": \"Marketing\", \"salary\": 75000},\n    {\"name\": \"Bob Williams\", \"age\": 42, \"department\": \"Engineering\", \"salary\": 90000}\n]\nuser_instruction = \"Generate diverse employee data for a tech company\"\n\ngenerator = GenerativeDataSynth(\n    generator_llm=your_generator_llm,\n    judge_llm=your_judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    verbose=1\n)\n\nemployee_data = generator.generate_data(num_samples=50)\nprint(employee_data.head())\n</code></pre>"},{"location":"indoxGen/GenerativeDataSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>GenerativeDataSynth</code> are welcome. Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol>"},{"location":"indoxGen/GenerativeDataSynth/#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/","title":"Hybrid GAN + LLM","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#overview","title":"Overview","text":"<p>Hybrid LLM-GAN is a powerful framework that combines Generative Adversarial Networks (GANs) for generating synthetic numerical data and Language Models (LLMs) for generating synthetic text data. This hybrid approach allows for generating diverse datasets that contain both structured numerical columns (e.g., age, income) and unstructured text columns (e.g., job title, remarks).</p> <p>The framework leverages GANs for structured data generation and LLMs for unstructured data generation, ensuring that the two are coherent and contextually aligned.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Usage</li> <li>Example: GAN and LLM Hybrid Pipeline</li> <li>Configuration</li> <li>API Reference</li> <li>Contributing</li> <li>License</li> </ul>"},{"location":"indoxGen/HybridGAN%2BLLM/#installation","title":"Installation","text":"<p>To install the hybrid framework, ensure you have Python 3.9+. Install both IndoxGen-Torch (for GAN) and the necessary LLM libraries (for text generation):</p> <pre><code>pip install indoxgen-torch\npip install openai  # Or any other LLM provider library\npip install python-dotenv  # For managing API keys securely\n</code></pre>"},{"location":"indoxGen/HybridGAN%2BLLM/#additionally_make_sure_you_have_pytorch_or_tensorflow_depending_on_your_system_configuration_installed_for_the_gan-based_part","title":"Additionally, make sure you have PyTorch or TensorFlow (depending on your system configuration) installed for the GAN-based part.","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#usage","title":"Usage","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#gan-and-llm-hybrid-pipeline","title":"Example: GAN and LLM Hybrid Pipeline","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#import_pandas_as_pd_from_indoxgenhybrid_synth_import_texttabularsynth_initialize_gan_synth_initialize_llm_synth_from_dotenv_import_load_dotenv_import_os_load_environment_variables_api_keys_for_llm_load_dotenvapienv_indox_api_key_osenvironindox_api_key_nvidia_api_key_osenvironnvidia_api_key_from_indoxgenllms_import_openai_indoxapi_initialize_llms_for_text_generation_indox_indoxapiapi_keyindox_api_key_nemotron_openaiapi_keynvidia_api_key_modelnvidianemotron-4-340b-instruct_base_urlhttpsintegrateapinvidiacomv1_sample_dataset_containing_both_numerical_and_text_data_sample_data_age_25_income_455_years_of_experience_3_job_title_junior_developer_remarks_looking_to_grow_my_career_age_32_income_600_years_of_experience_7_job_title_developer_remarks_experienced_professional_more_data_entries_data_pddataframesample_data_define_numerical_and_text_columns_numerical_columns_age_income_years_of_experience_text_columns_job_title_remarks_initialize_llm_setup_for_text_generation_llm_setup_initialize_llm_synth_generator_llmnemotron_judge_llmindox_columnstext_columns_example_datasample_data_user_instructiongenerate_realistic_and_diverse_text_data_based_on_the_provided_numerical_context_diversity_threshold04_max_diversity_failures30_verbose1_initialize_gan_setup_for_numerical_data_generation_numerical_data_datanumerical_columns_gan_setup_initialize_gan_synth_input_dim200_generator_layers128_256_512_discriminator_layers512_256_128_learning_rate2e-4_batch_size64_epochs50_n_critic5_categorical_columns_mixed_columns_integer_columnsage_years_of_experience_datanumerical_data_create_the_hybrid_pipeline_for_both_text_and_numerical_generation_synth_pipeline_texttabularsynthtabulargan_setup_textllm_setup_generate_synthetic_data_num_samples_10_synthetic_data_synth_pipelinegeneratenum_samples_preview_the_generated_synthetic_data_printsynthetic_datahead","title":"<pre><code>import pandas as pd\nfrom indoxGen.hybrid_synth import TextTabularSynth, initialize_gan_synth, initialize_llm_synth\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables (API keys for LLM)\nload_dotenv('api.env')\n\nINDOX_API_KEY = os.environ['INDOX_API_KEY']\nNVIDIA_API_KEY = os.environ['NVIDIA_API_KEY']\n\nfrom indoxGen.llms import OpenAi, IndoxApi\n\n# Initialize LLMs for text generation\nindox = IndoxApi(api_key=INDOX_API_KEY)\nnemotron = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\", base_url=\"https://integrate.api.nvidia.com/v1\")\n\n# Sample dataset containing both numerical and text data\nsample_data = [\n    {'age': 25, 'income': 45.5, 'years_of_experience': 3, 'job_title': 'Junior Developer', 'remarks': 'Looking to grow my career.'},\n    {'age': 32, 'income': 60.0, 'years_of_experience': 7, 'job_title': 'Developer', 'remarks': 'Experienced professional.'},\n    # ... more data entries\n]\n\ndata = pd.DataFrame(sample_data)\n\n# Define numerical and text columns\nnumerical_columns = ['age', 'income', 'years_of_experience']\ntext_columns = ['job_title', 'remarks']\n\n# Initialize LLM setup for text generation\nllm_setup = initialize_llm_synth(\n    generator_llm=nemotron,\n    judge_llm=indox,\n    columns=text_columns,\n    example_data=sample_data,\n    user_instruction=\"Generate realistic and diverse text data based on the provided numerical context.\",\n    diversity_threshold=0.4,\n    max_diversity_failures=30,\n    verbose=1\n)\n\n# Initialize GAN setup for numerical data generation\nnumerical_data = data[numerical_columns]\ngan_setup = initialize_gan_synth(\n    input_dim=200,\n    generator_layers=[128, 256, 512],\n    discriminator_layers=[512, 256, 128],\n    learning_rate=2e-4,\n    batch_size=64,\n    epochs=50,\n    n_critic=5,\n    categorical_columns=[],\n    mixed_columns={},\n    integer_columns=['age', 'years_of_experience'],\n    data=numerical_data\n)\n\n# Create the hybrid pipeline for both text and numerical generation\nsynth_pipeline = TextTabularSynth(tabular=gan_setup, text=llm_setup)\n\n# Generate synthetic data\nnum_samples = 10\nsynthetic_data = synth_pipeline.generate(num_samples)\n\n# Preview the generated synthetic data\nprint(synthetic_data.head())\n</code></pre>","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#configuration","title":"Configuration","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#gan_configuration","title":"GAN Configuration:","text":"<p>The TabularGANConfig class allows customization of the GAN model for numerical data generation. You can modify parameters like the number of layers, learning rates, and batch sizes for fine-tuning.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#llm_configuration","title":"LLM Configuration:","text":"<p>In addition to configuring the GAN, you also set up the LLM with the following parameters: - generator_llm: The language model used to generate text (e.g., OpenAI, Nemotron). - judge_llm: A model for judging the quality of generated text. - columns: The columns in the dataset that represent text data. - user_instruction: Custom instruction guiding the LLM to generate relevant and diverse text.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/HybridGAN%2BLLM/#initialize_gan_synth","title":"<code>initialize_gan_synth</code>:","text":"<p>Used to initialize and configure the GAN for generating synthetic numerical data.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#initialize_llm_synth","title":"<code>initialize_llm_synth</code>:","text":"<p>Initializes and configures the LLM for generating synthetic text data.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#texttabularsynth","title":"<code>TextTabularSynth</code>:","text":"<p>Combines the GAN and LLM pipelines into one hybrid pipeline for generating synthetic data with both numerical and text fields.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#contributing","title":"Contributing","text":"<p>Contributions to the Hybrid LLM-GAN project are welcome. Follow these steps to contribute: 1. Fork the repository. 2. Create a new branch for your feature or bug fix. 3. Add your changes and write tests if applicable. 4. Submit a pull request with a clear description of your changes.</p>"},{"location":"indoxGen/HybridGAN%2BLLM/#license","title":"License","text":"<p>For licensing information, see LICENSE.</p>"},{"location":"indoxGen/InteractiveFeedbackSynth/","title":"Interactive Feedback Synthesis","text":""},{"location":"indoxGen/InteractiveFeedbackSynth/#overview","title":"Overview","text":"<p><code>InteractiveFeedbackSynth</code> is a Python class designed for generating synthetic data based on example data and user instructions. It utilizes language models to generate and judge synthetic data points, ensuring diversity and adherence to specified criteria. This class is particularly useful for creating realistic, diverse datasets for testing, development, or machine learning purposes. It also includes a human feedback mechanism for reviewing and regenerating data points.</p>"},{"location":"indoxGen/InteractiveFeedbackSynth/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Language Model Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ul>"},{"location":"indoxGen/InteractiveFeedbackSynth/#installation","title":"Installation","text":"<p>To use the <code>InteractiveFeedbackSynth</code>, you need to have Python 3.9+ installed. You can install the required dependencies using pip:</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"indoxGen/InteractiveFeedbackSynth/#language-model-setup","title":"Language Model Setup","text":"<p>The <code>InteractiveFeedbackSynth</code> requires two language models: one for generating data and another for judging data quality. The specific setup will depend on your chosen language model library. Here's an example using a hypothetical library:</p> <pre><code>from indoxGen.llms import IndoxApi, OpenAi\n\n# generator llm\nindox = IndoxApi(api_key=INDOX_API_KEY)\n\n#judge llm\nnemotron = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                  base_url=\"https://integrate.api.nvidia.com/v1\")\n</code></pre> <p>When initializing the <code>InteractiveFeedbackSynth</code>, you'll pass these language model instances as the <code>generator_llm</code> and <code>judge_llm</code> parameters.</p>"},{"location":"indoxGen/InteractiveFeedbackSynth/#usage","title":"Usage","text":"<p>Here's a basic example of how to use the <code>InteractiveFeedbackSynth</code>:</p> <pre><code>from indoxGen.synthCore import InteractiveFeedbackSynth\nfrom indoxGen.llms import OpenAi, IndoxApi\n\n# Setup language models\ngenerator_llm = IndoxApi(api_key=INDOX_API_KEY)\njudge_llm = OpenAi(api_key=NVIDIA_API_KEY, model=\"nvidia/nemotron-4-340b-instruct\",\n                   base_url=\"https://integrate.api.nvidia.com/v1\")\n\ncolumns = [\"name\", \"age\", \"occupation\"]\nexample_data = [\n    {\"name\": \"John Doe\", \"age\": 30, \"occupation\": \"Engineer\"},\n    {\"name\": \"Jane Smith\", \"age\": 28, \"occupation\": \"Teacher\"}\n]\nuser_instruction = \"Generate diverse fictional employee data\"\n\ngenerator = InteractiveFeedbackSynth(\n    generator_llm=generator_llm,\n    judge_llm=judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    feedback_min_score= 0.8\n)\n\nsynthetic_data = generator.generate_data(num_samples=10)\nprint(synthetic_data)\n\n# User review and regeneration\naccepted_rows = [0, 1, 2]  # Indices of rows to accept\nregenerate_rows = [3, 4]  # Indices of rows to regenerate\nregeneration_feedback = \"Ensure more diversity in occupations\"\nupdated_data = generator.user_review_and_regenerate(accepted_rows, regenerate_rows, regeneration_feedback, min_score=0.7)\nprint(updated_data)\n</code></pre>"},{"location":"indoxGen/InteractiveFeedbackSynth/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/InteractiveFeedbackSynth/#interactivefeedbacksynth","title":"InteractiveFeedbackSynth","text":""},{"location":"indoxGen/InteractiveFeedbackSynth/#_init_self_generator_llm_judge_llm_columns_example_data_user_instruction_real_datanone_diversity_threshold07_max_diversity_failures20_verbose0_feedback_min_score08","title":"<code>__init__(self, generator_llm, judge_llm, columns, example_data, user_instruction, real_data=None, diversity_threshold=0.7, max_diversity_failures=20, verbose=0, feedback_min_score=0.8)</code>","text":"<p>Initialize the InteractiveFeedbackSynth.</p> <ul> <li><code>generator_llm</code>: Language model for generating data.</li> <li><code>judge_llm</code>: Language model for judging data quality.</li> <li><code>columns</code>: List of column names for the synthetic data.</li> <li><code>example_data</code>: List of example data points.</li> <li><code>user_instruction</code>: Instruction for data generation.</li> <li><code>real_data</code>: Optional list of real data points.</li> <li><code>diversity_threshold</code>: Threshold for determining data diversity (default: 0.7).</li> <li><code>max_diversity_failures</code>: Maximum number of diversity failures before forcing acceptance (default: 20).</li> <li><code>verbose</code>: Verbosity level (0 for minimal output, 1 for detailed feedback) (default: 0).</li> <li><code>feedback_min_score</code>: Minimum score for accepting generated data (default: 0.8).</li> </ul>"},{"location":"indoxGen/InteractiveFeedbackSynth/#generate_dataself_num_samples_int_-_pddataframe","title":"<code>generate_data(self, num_samples: int) -&gt; pd.DataFrame</code>","text":"<p>Generate synthetic data points.</p> <ul> <li><code>num_samples</code>: Number of data points to generate.</li> <li>Returns: DataFrame containing the generated data.</li> </ul>"},{"location":"indoxGen/InteractiveFeedbackSynth/#user_review_and_regenerateself_accepted_rows_unionlistint_liststr_regenerate_rows_unionlistint_liststr_regeneration_feedback_str_min_score_float_-_pddataframe","title":"<code>user_review_and_regenerate(self, accepted_rows: Union[List[int], List[str]], regenerate_rows: Union[List[int], List[str]], regeneration_feedback: str, min_score: float) -&gt; pd.DataFrame</code>","text":"<p>Review and regenerate synthetic data based on feedback.</p> <ul> <li><code>accepted_rows</code>: Indices of rows to accept or ['all'].</li> <li><code>regenerate_rows</code>: Indices of rows to regenerate or ['all'].</li> <li><code>regeneration_feedback</code>: Feedback for regeneration.</li> <li><code>min_score</code>: Minimum score for accepting regenerated data.</li> <li>Returns: Generated dataframe containing accepted and regenerated data.</li> </ul>"},{"location":"indoxGen/InteractiveFeedbackSynth/#examples","title":"Examples","text":""},{"location":"indoxGen/InteractiveFeedbackSynth/#generating_employee_data_with_human_feedback","title":"Generating Employee Data with Human Feedback","text":"<pre><code>columns = [\"name\", \"age\", \"department\", \"salary\"]\nexample_data = [\n    {\"name\": \"Alice Johnson\", \"age\": 35, \"department\": \"Marketing\", \"salary\": 75000},\n    {\"name\": \"Bob Williams\", \"age\": 42, \"department\": \"Engineering\", \"salary\": 90000}\n]\nuser_instruction = \"Generate diverse employee data for a tech company\"\n\ngenerator = InteractiveFeedbackSynth(\n    generator_llm=your_generator_llm,\n    judge_llm=your_judge_llm,\n    columns=columns,\n    example_data=example_data,\n    user_instruction=user_instruction,\n    verbose=1,\n    feedback_min_score= 0.8\n)\n\nemployee_data = generator.generate_data(num_samples=50)\nprint(employee_data.head())\n# User review and regeneration\naccepted_rows = [0, 1, 2]  # Indices of rows to accept\nregenerate_rows = [3, 4]  # Indices of rows to regenerate\nregeneration_feedback = \"Ensure more diversity in department assignments and a wider range of salaries\"\nupdated_data = generator.user_review_and_regenerate(accepted_rows, regenerate_rows, regeneration_feedback, min_score=0.7)\nprint(updated_data.head())\n</code></pre> <p>This example demonstrates how to generate employee data, review it, and then regenerate specific rows based on human feedback.</p>"},{"location":"indoxGen/InteractiveFeedbackSynth/#contributing","title":"Contributing","text":"<p>Contributions to improve <code>InteractiveFeedbackSynth</code> are welcome. Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature.</li> <li>Add your changes and write tests if applicable.</li> <li>Submit a pull request with a clear description of your changes.</li> </ol> <p>When contributing, please ensure that you maintain the existing code style and add appropriate documentation for any new features or changes.</p>"},{"location":"indoxGen/LICENSE/","title":"License","text":""},{"location":"indoxGen/LICENSE/#mit_license","title":"MIT License","text":"<p>Copyright (c) 2024 OSLLM.ai - indoxGen</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"indoxGen/LICENSE/#additional_terms","title":"Additional Terms","text":"<ol> <li> <p>When using the indoxGen models for data generation, please cite: <pre><code>@software{osllm_indoxgen,\n  title = {indoxGen: Advanced Data Generation Toolkit},\n  author = {{OSLLM.ai Team}},\n  year = {2024},\n  url = {https://github.com/osllm/indoxgen}\n}\n</code></pre></p> </li> <li> <p>Generated data must be reviewed for quality and compliance with applicable laws and regulations.</p> </li> <li> <p>Users are responsible for ensuring generated data does not violate privacy, copyright, or other legal restrictions. </p> </li> </ol>"},{"location":"indoxGen/PromptBasedSynth/","title":"Prompt-Based Synthesis","text":""},{"location":"indoxGen/PromptBasedSynth/#overview","title":"Overview","text":"<p><code>PromptBasedSynth</code> is a Python class designed to generate synthetic data using a Language Learning Model (LLM) based on a prompt and predefined data structure. It allows users to generate data from scratch or augment existing data by feeding a DataFrame to the model. The class can handle both text generation and JSON responses, ensuring that the generated data fits the specified prompt and format.</p>"},{"location":"indoxGen/PromptBasedSynth/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>LLM Setup</li> <li>Usage</li> <li>API Reference</li> <li>Examples</li> <li>Contributing</li> </ol>"},{"location":"indoxGen/PromptBasedSynth/#installation","title":"Installation","text":"<p>To use <code>PromptBasedSynth</code>, install the required libraries using pip: <pre><code>pip install pandas loguru json\n</code></pre></p> <p>Additionally, you need the <code>indoxGen</code> library to connect to various language models for data generation.</p> <pre><code>pip install indoxGen\n</code></pre>"},{"location":"indoxGen/PromptBasedSynth/#llm-setup","title":"LLM Setup","text":"<p><code>PromptBasedSynth</code> uses language models (LLMs) to generate synthetic data. The library supports various models via the <code>indoxGen</code> library.</p> <p>For example, you can initialize an <code>IndoxApi</code> model like this: <pre><code>from indoxGen.llms import IndoxApi\nimport os\nfrom dotenv import load_dotenv\n\n# Load API key from environment variables\nload_dotenv()\nINDOX_API_KEY = os.getenv(\"INDOX_API_KEY\")\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n</code></pre></p> <p>The <code>indoxGen</code> library supports various models including: - OpenAI - Mistral - Ollama - Google AI - Hugging Face models</p> <p>Additionally, <code>indoxGen</code> provides a router for OpenAI, allowing for easy switching between different models.</p>"},{"location":"indoxGen/PromptBasedSynth/#usage","title":"Usage","text":"<p>The <code>PromptBasedSynth</code> class can be used to either generate new data from a user-provided prompt or augment existing datasets by generating additional rows based on the provided data.</p>"},{"location":"indoxGen/PromptBasedSynth/#example_1_generate_data_from_scratch","title":"Example 1: Generate data from scratch","text":"<pre><code>from indoxGen.synthCore import PromptBasedSynth\n\nuser_prompt = \"Generate a dataset with 3 column and 3 row about soccer.\"\n\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n# instruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndata_generator = PromptBasedSynth(llm=LLM,user_instruction=user_prompt,verbose=1)\n\ngenerated_df = data_generator.generate_data()\n\n# print(generated_df)\ndata_generator.save_to_excel(\"output_dataFromPrompt.xlsx\")\n</code></pre>"},{"location":"indoxGen/PromptBasedSynth/#example_2_generate_data_using_an_existing_dataset","title":"Example 2: Generate data using an existing dataset","text":"<pre><code>from indoxGen.synthCore import PromptBasedSynth\nfrom indoxGen.utils import Excel\n\ndataset_file_path = \"output_dataFromPrompt.xlsx\"\n\nexcel_loader = Excel(dataset_file_path) \ndf = excel_loader.load()  \nuser_prompt = \" based on given dataset generate one unique row about soccer\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\n\nadded_row = PromptBasedSynth(llm=LLM, user_instruction=user_prompt, example_data=df, verbose=1).generate_data()\nprint(added_row)\n</code></pre>"},{"location":"indoxGen/PromptBasedSynth/#api-reference","title":"API Reference","text":""},{"location":"indoxGen/PromptBasedSynth/#promptbasedsynth","title":"<code>PromptBasedSynth</code>","text":""},{"location":"indoxGen/PromptBasedSynth/#_init_self_prompt_name_str_args_dict_outputs_dict_dataframe_pddataframe_none","title":"<code>__init__(self, prompt_name: str, args: dict, outputs: dict, dataframe: pd.DataFrame = None)</code>","text":"<p>Initializes the <code>PromptBasedSynth</code> class.</p> <p>Arguments: - <code>prompt_name</code> (str): The name of the prompt used for data generation. - <code>args</code> (dict): Arguments containing the LLM instance and user instructions. - <code>outputs</code> (dict): Expected output format. - <code>dataframe</code> (pd.DataFrame, optional): Existing DataFrame to augment data from.</p>"},{"location":"indoxGen/PromptBasedSynth/#runself_-_pddataframe","title":"<code>run(self) -&gt; pd.DataFrame</code>","text":"<p>Generates the data and returns a DataFrame.</p> <p>Returns: - <code>pd.DataFrame</code>: A DataFrame containing generated or augmented data.</p>"},{"location":"indoxGen/PromptBasedSynth/#save_to_excelself_file_path_str_-_none","title":"<code>save_to_excel(self, file_path: str) -&gt; None</code>","text":"<p>Saves the generated DataFrame to an Excel file.</p> <p>Arguments: - <code>file_path</code> (str): Path to save the Excel file.</p>"},{"location":"indoxGen/PromptBasedSynth/#examples","title":"Examples","text":""},{"location":"indoxGen/PromptBasedSynth/#generate_new_data_from_llm","title":"Generate New Data from LLM","text":"<pre><code>user_prompt = \"Generate a list of 3 planets and their distances from Earth.\"\nLLM = IndoxApi(api_key=INDOX_API_KEY)\ninstruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndata_generator = PromptBasedSynth(\n    prompt_name=\"Generate Planet Data\",\n    args={\n        \"llm\": LLM,\n        \"n\": 1,\n        \"instruction\": instruction,\n    },\n    outputs={\"generations\": \"generate\"},\n)\n\ngenerated_df = data_generator.run()\nprint(generated_df)\ndata_generator.save_to_excel(\"planet_data.xlsx\")\n</code></pre>"},{"location":"indoxGen/PromptBasedSynth/#augment_existing_data","title":"Augment Existing Data","text":"<pre><code>dataset_file_path = \"planet_data.xlsx\"\n\nexcel_loader = Excel(dataset_file_path)\ndf = excel_loader.load()\n\nuser_prompt = \"Add a new planet to the dataset.\"\ninstruction = DataGenerationPrompt.get_instruction(user_prompt)\n\ndataset = PromptBasedSynth(\n    prompt_name=\"Augment Planet Data\",\n    args={\n        \"llm\": LLM,\n        \"n\": 1,\n        \"instruction\": instruction,\n    },\n    outputs={\"generations\": \"generate\"},\n    dataframe=df\n)\n\nupdated_df = dataset.run()\nprint(updated_df)\ndataset.save_to_excel(\"updated_planet_data.xlsx\")\n</code></pre>"},{"location":"indoxGen/PromptBasedSynth/#contributing","title":"Contributing","text":"<p>Contributions to the <code>PromptBasedSynth</code> class are welcome. To contribute: 1. Fork the repository. 2. Create a new branch for your feature. 3. Add your changes and write tests if applicable. 4. Submit a pull request with a clear description of your changes.</p>"},{"location":"indoxJudge/metrics/bias-fairness/Bias/","title":"Bias","text":""},{"location":"indoxJudge/metrics/bias-fairness/Bias/#bias","title":"Bias","text":""},{"location":"indoxJudge/metrics/bias-fairness/Bias/#overview","title":"Overview","text":"<p>Detects general language bias patterns in text outputs through opinion analysis. Part of the Bias &amp; Fairness category.</p> <pre><code>from indoxJudge.metrics import Bias\n\n# Initialize with response and settings\nbias_check = Bias(llm_response=your_text, threshold=0.6)\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Bias/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Opinion-based bias, subjective language patterns Score Range 0.0 (neutral) - 1.0 (biased) Analysis Depth Opinion extraction \u2192 Verdict generation \u2192 Score calculation Configuration Adjustable threshold and strict mode"},{"location":"indoxJudge/metrics/bias-fairness/Bias/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.3 Neutral/Objective 0.3-0.6 Mildly opinionated 0.6-0.8 Clear bias present 0.8-1.0 Strong systematic bias"},{"location":"indoxJudge/metrics/bias-fairness/Bias/#usage_example","title":"Usage Example","text":"<pre><code>bias_metric = Bias(\n    llm_response=\"This political ideology is fundamentally flawed\",\n    threshold=0.4,\n    include_reason=True\n)\n\nevaluator = Evaluator(model=llm, metrics=[bias_metric])\nresults = evaluator.judge()\n\nprint(f\"Bias Confidence: {results['bias']['score']:.2f}\")\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Bias/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.5 Sensitivity level for bias flagging include_reason Toggles explanatory rationale in output"},{"location":"indoxJudge/metrics/bias-fairness/Bias/#evaluation_process","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[Opinion Extraction]\n\n    B --&gt; C[Verdict Generation]\n    C --&gt; D[Threshold Comparison]\n    D --&gt; E{Biased?}\n    E --&gt;|Yes| F[Max Score if Strict]\n    E --&gt;|No| G[Proportional Scoring]\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Fairness/","title":"Fairness","text":"<p>Fairness</p> <p>Overview Assesses overall fairness in text outputs by detecting discriminatory patterns and systemic biases. Part of the Bias &amp; Fairness metric category.</p> <pre><code>from indoxJudge.metrics import Fairness\n\n# Initialize with text to analyze\nfairness_check = Fairness(input_sentence=\"Your text here\")\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Systemic biases, demographic discrimination, exclusionary language Score Range | 0.0 (fair) - 1.0 (unfair) Response Format | Returns fairness score with violation flags and rationale Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | No detectable fairness issues 0.2-0.4 | Potential indirect bias 0.4-0.6 | Moderate exclusionary patterns 0.6-0.8 | Clear discriminatory content 0.8-1.0 | Severe systemic bias</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import Fairness\nfrom indoxJudge.pipelines import Evaluator\n\ntext = \"Preferred candidates will be under 35 and native English speakers\"\n\n# Initialize analyzer\nfairness = Fairness(input_sentence=text)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[fairness]\n)\n\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nFairness Score: {results['fairness']['score']:.2f}\nReason: {results['fairness']['reason']}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>threshold=0.65</code> | Adjust fairness alert threshold (default: 0.65) Custom Templates | Modify detection criteria for specific fairness frameworks</p> <p>Best Practices</p> <ol> <li>Combine Metrics: Use with <code>StereotypeBias</code> and <code>ContextualRelevancy</code> for layered analysis</li> <li>Context Anchoring: Provide demographic context when available</li> <li>Threshold Strategy: Lower threshold (0.5) for HR/recruitment applications</li> <li>Bias Validation: Cross-check with demographic parity statistics</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>Fairness</code> | Systemic discrimination | Statistical patterns | Score + Violation types <code>StereotypeBias</code> | Cultural stereotypes | Semantic analysis | Specific stereotype tags <code>Bias</code> | General model skew | Distribution analysis | Binary classification</p> <p>Limitations</p> <ol> <li>Statistical Blindspots: May miss individual-level unfairness</li> <li>Context Dependency: Requires clear reference groups for accurate assessment</li> <li>Language Nuance: Challenges with implied biases through sarcasm/humor</li> <li>Framework Bias: Reflects training data's fairness definitions</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Ambiguous statements | Add demographic context Multiple protected classes | Use intersectional analysis tools Cultural specificity gaps | Supplement with locale-specific dictionaries Model confidence conflicts | Enable verbose logging for audit</p>"},{"location":"indoxJudge/metrics/bias-fairness/Fairness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Fairness Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Calculate Fairness Score]\n    C --&gt; D[Get Verdict]\n    D --&gt; E[Call Language Model]\n    E --&gt; F[Parse JSON Response]\n    F --&gt; G[Determine Verdict and Score]\n    C --&gt; H[Get Reason]\n    H --&gt; E\n    H --&gt; I[Return Reason for Verdict]\n    C --&gt; J[Get Unfairness Reasons]\n    J --&gt; E\n    J --&gt; K[Return Unfairness Reasons]\n    G --&gt; L[Return Final Verdict and Score]\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/","title":"Stereotype and Bias Metrics","text":"<p>This module provides metrics for evaluating stereotypes and bias in language model outputs.</p>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#overview","title":"Overview","text":"<p>The stereotype and bias metrics help identify: - Gender bias - Racial bias - Cultural stereotypes - Age-related bias - Socioeconomic bias</p>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#available_metrics","title":"Available Metrics","text":""},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#1_demographic_parity_score","title":"1. Demographic Parity Score","text":"<p>Measures whether the model's outputs are independent of protected attributes.</p> <pre><code>from indoxJudge.metrics import DemographicParityMetric\n\nmetric = DemographicParityMetric()\nscore = metric.evaluate(responses, protected_attributes)\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#2_stereotype_association_test","title":"2. Stereotype Association Test","text":"<p>Measures implicit associations between concepts and attributes.</p> <pre><code>from indoxJudge.metrics import StereotypeAssociationMetric\n\nmetric = StereotypeAssociationMetric()\nscore = metric.evaluate(responses, concept_pairs, attribute_pairs)\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#3_representation_bias_score","title":"3. Representation Bias Score","text":"<p>Evaluates the representation of different groups in generated content.</p> <pre><code>from indoxJudge.metrics import RepresentationBiasMetric\n\nmetric = RepresentationBiasMetric()\nscore = metric.evaluate(responses, demographic_groups)\n</code></pre>"},{"location":"indoxJudge/metrics/bias-fairness/Stereotype-and-Bias/#usage_guidelines","title":"Usage Guidelines","text":"<ol> <li>Define protected attributes and groups clearly</li> <li>Use diverse test sets</li> <li>Consider intersectional biases</li> <li>Document mitigation strategies </li> </ol>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/","title":"Advanced NLP Evaluation Metrics","text":""},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#bertscore","title":"BERTScore","text":""},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#overview","title":"Overview","text":"<p>Evaluates semantic similarity using transformer embeddings. Part of Semantic Evaluation category.</p> <pre><code>from indoxJudge.metrics import BertScore\n\nbert = BertScore(\n    llm_response=generated_text,\n    retrieval_context=reference_texts,\n    model_name=\"roberta-large\"\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#key_characteristics","title":"Key Characteristics","text":"Property Description Embedding Model Configurable transformer architecture Score Components Precision, Recall, F1 (cosine similarity) Context Handling Supports multiple reference texts GPU Acceleration Automatic CUDA detection"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#interpretation_guide","title":"Interpretation Guide","text":"F1 Score Range Semantic Match 0.0-0.4 Weak semantic relationship 0.4-0.6 Partial meaning overlap 0.6-0.8 Strong semantic alignment 0.8-1.0 Near-identical meaning"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#usage_example","title":"Usage Example","text":"<pre><code>bert_metric = BertScore(\n    llm_response=generated_summary,\n    retrieval_context=reference_articles,\n    model_name=\"bert-large-nli\",\n    max_length=512\n)\n\nevaluator = Evaluator(model=None, metrics=[bert_metric])\nprint(f\"BERTScore F1: {evaluator.judge()['bert_score']['f1']:.2f}\")\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#evaluation_process","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[Transformer Embeddings]\n    C[References] --&gt; B\n    B --&gt; D[Cosine Similarity Matrix]\n    D --&gt; E[Precision Calculation]\n    D --&gt; F[Recall Calculation]\n    E --&gt; G[F1 Score]\n    F --&gt; G</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#g-eval","title":"G-Eval","text":""},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#overview_1","title":"Overview","text":"<p>Multi-dimensional evaluation framework for comprehensive text assessment.</p> <pre><code>from indoxJudge.metrics import GEval\n\ngeval = GEval(\n    parameters=\"summary\",\n    query=user_query,\n    llm_response=generated_text,\n    ground_truth=reference_text\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#configuration_matrix","title":"Configuration Matrix","text":"Parameter Evaluation Method Description Retrieval Quality Document relevance analysis Evaluates if retrieved documents/snippets are relevant and accurate Integration Information synthesis Assesses how well retrieved information is integrated into the response Coherence Logical structure analysis Checks if text is logically structured and easy to follow Relevance Topic coverage assessment Evaluates relevance to main topic and coverage of key points Accuracy Factual verification Checks factual accuracy and consistency with source material Fluency Grammatical assessment Evaluates readability and grammatical correctness Comprehensiveness Completeness check Assesses coverage of all key points and thoroughness Contextuality Context alignment Evaluates how well response fits within query context"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#interpretation_guide_1","title":"Interpretation Guide","text":"Composite Score Quality Level 0.0-2.5 Poor/inadequate 2.5-5.0 Basic/partial 5.0-7.5 Competent 7.5-10.0 Expert-level"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#usage_example_1","title":"Usage Example","text":"<pre><code>geval_metric = GEval(\n    query=\"Explain quantum computing\",\n    llm_response=generated_explanation,\n    ground_truth=reference_material\n)\n\nevaluator = Evaluator(model=llm, metrics=[geval_metric])\nresults = evaluator.judge()\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#gruen","title":"GRUEN","text":""},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#overview_2","title":"Overview","text":"<p>Evaluates text quality through grammatical, redundancy, and focus analysis.</p> <pre><code>from indoxJudge.metrics import Gruen\n\ngruen = Gruen(\n    candidates=[text1, text2],\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#quality_indicators","title":"Quality Indicators","text":"<ul> <li>Grammaticality: Language correctness</li> <li>Redundancy: Information repetition</li> <li>Focus: Topical concentration</li> </ul>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#interpretation_guide_2","title":"Interpretation Guide","text":"Score Type Range Optimal Value Grammaticality 0-1 &gt;0.85 Redundancy 0-1 &lt;0.15 Focus 0-1 &gt;0.75"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#usage_example_2","title":"Usage Example","text":"<pre><code>gruen_metric = Gruen(\n    candidates=[\n        \"The quick brown fox jumps...\",\n        \"A fast vulpine leaps...\"\n    ]\n)\n\nevaluator = Evaluator(model=llm, metrics=[gruen_metric])\nprint(f\"GRUEN Score: {evaluator.judge()['gruen']}\")\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#evaluation_process_1","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[Grammatical Check]\n    A --&gt; C[Redundancy Analysis]\n    A --&gt; D[Focus Measurement]\n    B --&gt; E[Language Model Scoring]\n    C --&gt; F[Self-similarity Check]\n    D --&gt; G[Topic Concentration]\n    E --&gt; H[Composite Score]\n    F --&gt; H\n    G --&gt; H</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#comparison_of_advanced_metrics","title":"Comparison of Advanced Metrics","text":"Metric Strength Evaluation Depth Speed BERTScore Semantic understanding High Medium G-Eval Comprehensive analysis Very High Slow GRUEN Text quality focus Medium Fast"},{"location":"indoxJudge/metrics/nlp-metrics/advanced/#use_case_guide","title":"Use Case Guide","text":"<ul> <li>Research Papers: BERTScore + G-Eval</li> <li>Content Moderation: GRUEN + BERTScore</li> <li>Technical Writing: G-Eval (full parameters)</li> </ul>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/","title":"Basic NLP Evaluation Metrics","text":""},{"location":"indoxJudge/metrics/nlp-metrics/basic/#bleu_bilingual_evaluation_understudy","title":"BLEU (Bilingual Evaluation Understudy)","text":""},{"location":"indoxJudge/metrics/nlp-metrics/basic/#overview","title":"Overview","text":"<p>Evaluates text similarity using modified n-gram precision with brevity penalty. Part of the Basic NLP Metrics category.</p> <pre><code>from indoxJudge.metrics import BLEU\n\n# Initialize with response and references\nbleu = BLEU(\n    llm_response=generated_text,\n    retrieval_context=reference_texts\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#key_characteristics","title":"Key Characteristics","text":"Property Description N-gram Focus 1-4 gram precision (default: bigrams) Brevity Penalty Penalizes shorter-than-reference outputs Repeating Patterns Optional repeating n-gram filtering Score Range 0.0 (no match) - 1.0 (exact match)"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Translation Quality 0.0-0.3 Poor/nonsensical match 0.3-0.6 Partial meaning preservation 0.6-0.8 Good semantic alignment 0.8-1.0 Near-perfect equivalence"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#usage_example","title":"Usage Example","text":"<pre><code>bleu_metric = BLEU(\n    llm_response=\"The fast fox jumps high\",\n    retrieval_context=[\n        \"A quick brown fox leaps\",\n        \"The speedy fox jumps\"\n    ],\n    n=4,  # Use 4-grams\n    remove_repeating_ngrams=True\n)\n\nevaluator = Evaluator(model=None, metrics=[bleu_metric])\nprint(f\"BLEU-4 Score: {evaluator.judge()['bleu']:.2f}\")\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#configuration_options","title":"Configuration Options","text":"Parameter Effect n=2 Maximum n-gram size (1-4) remove_repeating_ngrams Filters redundant n-gram patterns Multiple References Improves score stability through variance averaging"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#evaluation_process","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[Tokenize &amp; Generate N-grams]\n    C[Reference Texts] --&gt; B\n    B --&gt; D[Calculate Modified Precision]\n    D --&gt; E[Compute Brevity Penalty]\n    E --&gt; F[Combine Components]\n    F --&gt; G[Output Final Score]</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#best_practices","title":"Best Practices","text":"<ol> <li>Use 4-grams for translation tasks</li> <li>Enable repeating n-gram filter for creative writing</li> <li>Provide \u22654 references for reliable scoring</li> <li>Combine with METEOR for semantic analysis</li> </ol>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#limitations","title":"Limitations","text":"<ol> <li>Insensitive to word order changes</li> <li>Fails to capture semantic meaning</li> <li>Over-penalizes length variations</li> <li>Requires lexical overlap</li> </ol>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#comparison_with_similar_metrics","title":"Comparison with Similar Metrics","text":"Metric Precision Focus Length Handling Semantic Awareness BLEU N-gram matches Brevity penalty Low METEOR Stem matches Fragmentation control Medium ROUGE Longest sequence No penalty None"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#meteor","title":"METEOR","text":""},{"location":"indoxJudge/metrics/nlp-metrics/basic/#overview_1","title":"Overview","text":"<p>Measures text similarity using combined precision, recall, and fragmentation analysis.</p> <pre><code>from indoxJudge.metrics import METEOR\n\nmeteor = METEOR(\n    llm_response=\"The quick fox jumps\",\n    retrieval_context=[\"A fast fox leaps\"]\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#key_parameters","title":"Key Parameters","text":"Parameter Description llm_response Generated text to evaluate retrieval_context Reference text(s) for comparison"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#interpretation_guide_1","title":"Interpretation Guide","text":"Score Range Quality 0.0-0.3 Poor match 0.3-0.6 Partial alignment 0.6-0.8 Strong similarity 0.8-1.0 Near-exact match"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#usage_example_1","title":"Usage Example","text":"<pre><code>meteor_metric = METEOR(\n    llm_response=generated_text,\n    retrieval_context=reference_texts\n)\n\nevaluator = Evaluator(model=None, metrics=[meteor_metric])\nprint(evaluator.judge()['meteor'])\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#evaluation_process_1","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[Alignment Matching]\n    B --&gt; C[Precision Calculation]\n    B --&gt; D[Recall Calculation]\n    C --&gt; E[Fragmentation Penalty]\n    D --&gt; E\n    E --&gt; F[Final Score]</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#rouge","title":"ROUGE","text":""},{"location":"indoxJudge/metrics/nlp-metrics/basic/#overview_2","title":"Overview","text":"<p>Evaluates text similarity through n-gram overlap analysis.</p> <pre><code>from indoxJudge.metrics import Rouge\n\nrouge = Rouge(\n    llm_response=generated_summary,\n    retrieval_context=reference_texts,\n    n=2  # Bigrams\n)\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#key_parameters_1","title":"Key Parameters","text":"Parameter Description llm_response Text to evaluate retrieval_context Ground truth reference(s) n N-gram size (default=1)"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#interpretation_guide_2","title":"Interpretation Guide","text":"Score Type Range Focus ROUGE-N 0-1 Word overlap ROUGE-L 0-1 Longest sequence"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#usage_example_2","title":"Usage Example","text":"<pre><code>rouge_metric = Rouge(\n    llm_response=\"The cat sat on mat\",\n    retrieval_context=[\"A cat sits on the mat\"],\n    n=1\n)\n\nevaluator = Evaluator(model=None, metrics=[rouge_metric])\nprint(evaluator.judge()['rouge'])\n</code></pre>"},{"location":"indoxJudge/metrics/nlp-metrics/basic/#evaluation_process_2","title":"Evaluation Process","text":"<pre><code>flowchart TD\n    A[Input Text] --&gt; B[N-gram Extraction]\n    C[Reference Text] --&gt; B\n    B --&gt; D[Overlap Calculation]\n    D --&gt; E[Recall Score]\n    D --&gt; F[Precision Score]\n    E --&gt; G[F-measure Combination]\n    F --&gt; G</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/","title":"Faithfulness","text":""},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#faithfulness_evaluation","title":"Faithfulness Evaluation","text":""},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#overview","title":"Overview","text":"<p>Assesses how accurately generated responses reflect provided source material without introducing unsupported claims. Part of the Content Integrity metric category.</p> <pre><code>from indoxJudge.metrics import Faithfulness\n\n# Initialize with response and context\nfaithfulness_check = Faithfulness(\n    llm_response=\"Generated text\",\n    retrieval_context=[\"Source document 1\", \"Source document 2\"]\n)\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Unsupported claims, contextual misalignment, factual distortions Score Range 0.0 (unfaithful) - 1.0 (fully faithful) Response Format Returns score with contradiction details and source references Dependencies Requires retrieval context for verification"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Severe contradictions/inventions 0.2-0.4 Multiple unsupported claims 0.4-0.6 Mostly faithful with some inaccuracies 0.6-0.8 Minor contextual misalignments 0.8-1.0 Fully supported by source material"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Faithfulness\nfrom indoxJudge.pipelines import Evaluator\n\nresponse = \"Mars colonies will be established by 2030\"\ncontext = [\"NASA plans aim for 2040s Martian exploration\"]\n\n# Initialize analyzer\nfaith_check = Faithfulness(\n    llm_response=response,\n    retrieval_context=context\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[faith_check]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nFaithfulness Score: {results['faithfulness']['score']:.2f}\nUnsupported Claims: {results['faithfulness']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#configuration_options","title":"Configuration Options","text":"Parameter Effect strict_validation=True Require direct textual support context_weight=0.7 Adjust context importance in scoring negation_threshold=0.4 Set tolerance for contradictory statements"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#best_practices","title":"Best Practices","text":"<ul> <li>Context Enrichment: Provide full document context chains</li> <li>Threshold Strategy: Use lower thresholds for creative writing</li> <li>Multi-layer Analysis: Combine with FactualConsistency and ContextualRelevancy</li> <li>Version Control: Ensure context-source version alignment</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity Faithfulness Contextual grounding Source-supported claims Score + Claim validation FactualConsistency Factual accuracy Truth verification Error classification HallucinationDetection Invented information Null-context analysis Hallucination ratio"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#limitations","title":"Limitations","text":"<ul> <li>Implicit Knowledge: May flag common knowledge as unsupported</li> <li>Temporal Context: Doesn't auto-detect outdated sources</li> <li>Multimodal Content: Limited to text-based verification</li> <li>Paraphrase Detection: Challenges with heavy rephrasing</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Ambiguous claims Enable claim clarification protocol Missing context Activate external knowledge base lookup Conflicting sources Implement source reliability weighting Partial matches Adjust semantic similarity thresholds"},{"location":"indoxJudge/metrics/quality-accuracy/Faithfulness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Faithfulness Init] --&gt; B[Parse Response Claims]\n    B --&gt; C[Cross-reference Context]\n    C --&gt; D[Classify Support Level]\n    D --&gt; E[Calculate Claim Scores]\n    E --&gt; F[Aggregate Final Score]\n    F --&gt; G[Generate Source Citations]\n    G --&gt; H[Compile Verification Report]\n    C --&gt; I[Flag Contradictions]\n    I --&gt; J[Calculate Penalty Scores]\n    J --&gt; F</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Hallucination/","title":"Hallucination","text":"<p>Hallucination</p> <p>Overview Evaluates hallucinations in language model outputs by comparing generated responses against retrieval context. Part of the Accuracy &amp; Reliability metric category.</p> <pre><code>from indoxJudge.metrics import Hallucination\n\n# Initialize with response and context\nhallucination_check = Hallucination(\n    llm_response=\"The response to evaluate\",\n    retrieval_context=\"The ground truth context\"\n)\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Factual inconsistencies, fabricated information, context deviation Score Range | 0.0 (no hallucination) - 1.0 (complete hallucination) Response Format | Returns hallucination score with detailed reasoning Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Highly accurate, minimal deviation from context 0.2-0.4 | Minor inconsistencies present 0.4-0.6 | Moderate fabrication or context misrepresentation 0.6-0.8 | Significant factual errors or invented information 0.8-1.0 | Severe hallucination with little connection to retrieval context</p> <p>Usage Example</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom indoxJudge.models import OpenAi\nfrom indoxJudge.metrics import Hallucination\nfrom indoxJudge.pipelines import Evaluator\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialize the language model\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define the response and retrieval context\nllm_response = \"The Eiffel Tower is located in Berlin.\"\nretrieval_context = \"The Eiffel Tower is located in Paris, France.\"\n\n# Initialize hallucination detector\nhallucination = Hallucination(\n    llm_response=llm_response,\n    retrieval_context=retrieval_context,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(model=llm, metrics=[hallucination])\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nHallucination Score: {results['hallucination']['score']:.2f}\nReason: {results['hallucination']['reason']}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>threshold=0.5</code> | Defines hallucination detection threshold (default: 0.5) <code>include_reason=True</code> | Enables detailed reasoning for verdict (default: True) <code>strict_mode=False</code> | Forces score of 1.0 if threshold exceeded (default: False)</p> <p>Best Practices</p> <ol> <li>Context Quality: Provide comprehensive, high-quality retrieval context</li> <li>Threshold Tuning: Adjust threshold based on use case criticality</li> <li>Complementary Metrics: Combine with <code>Relevance</code> and <code>Coherence</code> for complete evaluation</li> <li>Domain Specificity: Consider domain-specific hallucination patterns</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>Hallucination</code> | Factual accuracy | Context comparison | Score + Detailed reasoning <code>Relevance</code> | On-topic response | Topic modeling | Topic adherence analysis <code>Coherence</code> | Logical consistency | Structural analysis | Internal consistency assessment</p> <p>Limitations</p> <ol> <li>Context Dependency: Performance tied to retrieval context quality</li> <li>Nuance Challenges: Difficulty with implicit information and inferences</li> <li>Domain Knowledge: May struggle with highly specialized content</li> <li>Boundary Cases: Ambiguity with partial truths or subjective statements</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Insufficient context | Expand retrieval context with relevant information Ambiguous statements | Enable <code>include_reason</code> for detailed analysis Domain-specific jargon | Use domain-adapted models when available Threshold sensitivity | Experiment with threshold values for optimal detection</p>"},{"location":"indoxJudge/metrics/quality-accuracy/Hallucination/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Hallucination Initialization] --&gt; B[Set LLM Response and Retrieval Context]\n    B --&gt; C[Set Model for Evaluation]\n    C --&gt; D[Measure Hallucination Score]\n    D --&gt; E[Generate Verdicts]\n    E --&gt; F[Call Language Model for Verdicts]\n    F --&gt; G[Parse Verdicts]\n    D --&gt; H[Calculate Hallucination Score]\n    H --&gt; I[Generate Reason]\n    I --&gt; J[Call Language Model for Reason]\n    J --&gt; K[Parse and Return Reason]\n    D --&gt; L[Return Final Hallucination Score and Reason]</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/","title":"KnowledgeRetention","text":""},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#knowledge_retention","title":"Knowledge Retention","text":""},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#overview","title":"Overview","text":"<p>Evaluates consistency of information across multi-turn interactions by tracking entity continuity and factual alignment. Part of the Conversational Quality metric category.</p> <pre><code>from indoxJudge.metrics import KnowledgeRetention\n\n# Initialize with conversation history\nretention_check = KnowledgeRetention(\n    messages=[\n        {\"query\": \"What's AI?\", \"llm_response\": \"Artificial Intelligence...\"},\n        {\"query\": \"How does it work?\", \"llm_response\": \"It uses algorithms...\"}\n    ]\n)\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Entity consistency, factual alignment, contextual continuity Score Range 0.0 (inconsistent) - 1.0 (fully consistent) Response Format Returns retention score with inconsistency flags and context gaps Dependencies Requires multi-message conversation history"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Severe information contradictions 0.2-0.4 Multiple consistency errors 0.4-0.6 Partial alignment with gaps 0.6-0.8 Mostly consistent with minor slips 0.8-1.0 Perfect information continuity"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import KnowledgeRetention\nfrom indoxJudge.pipelines import Evaluator\n\nconversation = [\n    {\"query\": \"CEO founded when?\", \"llm_response\": \"1994\"},\n    {\"query\": \"Founder's name?\", \"llm_response\": \"Founded in 1995 by John\"}\n]\n\n# Initialize analyzer\nretention = KnowledgeRetention(\n    messages=conversation,\n    threshold=0.6,\n    strict_mode=True\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[retention]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nRetention Score: {results['knowledge_retention']['score']:.2f}\nReason: {results['knowledge_retention']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.5 Minimum acceptable consistency score strict_mode=False Zero-score enforcement for threshold violations include_reason=True Enable detailed inconsistency explanations"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#best_practices","title":"Best Practices","text":"<ul> <li>Conversation Chunking: Analyze in 5-10 message windows</li> <li>Entity Tracking: Maintain custom entity dictionaries</li> <li>Temporal Analysis: Flag time-sensitive contradictions</li> <li>Multi-metric Validation: Combine with FactualConsistency</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity KnowledgeRetention Cross-message consistency Entity tracking + temporal analysis Score + Contradiction log ContextualRelevancy Single-response relevance Semantic matching Relevance percentage Faithfulness Response-source alignment Claim verification Support ratio"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#limitations","title":"Limitations","text":"<ul> <li>Conversation Length: Effectiveness decreases beyond 20 turns</li> <li>Implicit References: Struggles with pronoun resolution</li> <li>Multilingual Contexts: Requires language-specific setup</li> <li>Humorous Content: May flag intentional contradictions</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Ambiguous references Enable coreference resolution Partial matches Adjust semantic similarity thresholds Time-based conflicts Activate temporal reasoning module Domain-specific terms Load custom entity dictionaries"},{"location":"indoxJudge/metrics/quality-accuracy/KnowledgeRetention/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[KnowledgeRetention Init] --&gt; B[Parse Conversation History]\n    B --&gt; C[Extract Key Entities]\n    C --&gt; D[Track Cross-message Consistency]\n    D --&gt; E[Identify Contradictions]\n    E --&gt; F[Calculate Entity Scores]\n    F --&gt; G[Aggregate Retention Score]\n    G --&gt; H[Generate Improvement Tips]\n    H --&gt; I[Compile Final Report]\n    D --&gt; J[Flag Temporal Conflicts]\n    J --&gt; K[Calculate Time Penalties]\n    K --&gt; G</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/","title":"Misinformation","text":"<p>Here's the Misinformation information formatted in Markdown:</p>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#misinformation","title":"Misinformation","text":""},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#misinformation_detection","title":"Misinformation Detection","text":""},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#overview","title":"Overview","text":"<p>Identifies potentially false or misleading claims in text outputs through fact-checking and credibility analysis. Part of the Content Integrity metric category.</p> <pre><code>from indoxJudge.metrics import Misinformation\n\n# Initialize with text to analyze\ntruth_check = Misinformation(input_sentence=\"Your text here\")\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Factual inaccuracies, unsupported claims, outdated information Score Range 0.0 (accurate) - 1.0 (misleading) Response Format Returns risk score with flagged claims and fact-check references Dependencies Requires fact-checking API integration"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Verified accurate information 0.2-0.4 Mostly factual with minor inaccuracies 0.4-0.6 Mixed accuracy with some false claims 0.6-0.8 Significant misleading content 0.8-1.0 Dangerous misinformation"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Misinformation\nfrom indoxJudge.pipelines import Evaluator\n\ntext = \"Vaccines contain microchips for population tracking\"\n\n# Initialize analyzer\nmisinfo_check = Misinformation(input_sentence=text)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[misinfo_check]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nMisinformation Risk: {results['misinformation']['score']:.2f}\nFlagged Claims: {results['misinformation']['flagged_claims']}\nFact Checks: {results['misinformation']['references']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#configuration_options","title":"Configuration Options","text":"Parameter Effect confidence_threshold=0.8 Minimum confidence for flagging claims realtime_checks=True Enable live fact-checking API queries severity_weight=0.7 Adjust impact weighting for harmful content"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#best_practices","title":"Best Practices","text":"<ul> <li>Source Verification: Maintain updated fact-checking databases</li> <li>Context Analysis: Cross-reference with domain-specific knowledge bases</li> <li>Temporal Filtering: Flag outdated information automatically</li> <li>Severity Grading: Prioritize health/safety-related misinformation</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity Misinformation Factual accuracy Claim verification Score + Fact-check links FactualConsistency Source alignment Text comparison Consistency percentage Faithfulness Context grounding Source-supported claims Support ratio"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#limitations","title":"Limitations","text":"<ul> <li>Emerging Claims: Lag in detecting new misinformation</li> <li>Satire Detection: Challenges with parody/humor</li> <li>Cultural Context: Variations in truth perceptions</li> <li>Ambiguous Statements: Uncertainty in probabilistic claims</li> </ul>"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Unverifiable claims Activate source triangulation Contradictory sources Implement majority consensus check Technical jargon Use domain-specific validators Breaking news Enable provisional scoring"},{"location":"indoxJudge/metrics/quality-accuracy/Misinformation/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Misinformation Init] --&gt; B[Parse Input Claims]\n    B --&gt; C[Check Against Knowledge Bases]\n    C --&gt; D[Calculate Claim Accuracy]\n    D --&gt; E[Assess Potential Harm]\n    E --&gt; F[Aggregate Risk Score]\n    F --&gt; G[Generate Fact-check Report]\n    C --&gt; H[Flag High-risk Claims]\n    H --&gt; I[Trigger Urgent Review]</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/","title":"AnswerRelevancy","text":""},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#overview","title":"Overview","text":"<p>Evaluates how well language model responses align with query intent and context. Part of the Response Quality metric category.</p> <pre><code>from indoxJudge.metrics import AnswerRelevancy\n\n# Initialize with query and response\nrelevancy_check = AnswerRelevancy(\n    query=\"Your question here\",\n    llm_response=\"Model's response here\"\n)\n</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Response alignment with query intent, off-topic content, contextual drift Score Range 0.0 (irrelevant) - 1.0 (highly relevant) Response Format Returns relevancy score with irrelevant statements and improvement suggestions Dependencies Requires language model integration via set_model()"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Completely irrelevant response 0.2-0.4 Partial relevance with major off-topic content 0.4-0.6 Addresses basic intent but misses nuances 0.6-0.8 Relevant with minor inconsistencies 0.8-1.0 Fully aligned with query intent"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import AnswerRelevancy\nfrom indoxJudge.pipelines import Evaluator\n\nquery = \"Explain quantum computing basics\"\nresponse = \"Quantum physics studies subatomic particles...\"\n\n# Initialize analyzer\nrelevancy = AnswerRelevancy(\n    query=query,\n    llm_response=response,\n    threshold=0.6,\n    strict_mode=True\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[relevancy]\n)\n\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nRelevancy Score: {results['answer_relevancy']['score']:.2f}\nIrrelevant Statements: {results['answer_relevancy']['irrelevant_points']}\nImprovement Suggestions: {results['answer_relevancy']['suggestions']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.5 Minimum acceptable relevancy score (default: 0.5) strict_mode=False Zero-score enforcement below threshold include_reason=True Enable detailed rationale generation"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#best_practices","title":"Best Practices","text":"<ul> <li>Context Enrichment: Provide domain-specific terminology lists</li> <li>Threshold Strategy: Use strict mode for factual QA applications</li> <li>Multi-axis Analysis: Combine with ContextualDepth and FactualAccuracy</li> <li>Iterative Tuning: Adjust thresholds based on query complexity tiers</li> </ul>"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity AnswerRelevancy Query-response alignment Semantic similarity analysis Score + Irrelevant segments ContextualRelevancy Document-context alignment Context matching Percentage match TopicConsistency Theme preservation Topic modeling Deviation index"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#limitations","title":"Limitations","text":"<ul> <li>Ambiguity Challenge: Struggles with deliberately open-ended queries</li> <li>Cultural Context: May flag legitimate regional variations as irrelevant</li> <li>Sarcasm Detection: Limited capability with ironic or humorous responses</li> <li>Multilingual Nuance: Decreasing accuracy with code-switched content</li> </ul>"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Ambiguous queries Request query clarification context Multiple subquestions Enable atomic question parsing Technical jargon Provide domain glossary Metaphorical language Activate figurative speech detection"},{"location":"indoxJudge/metrics/relevancy-context/AnswerRelevancy/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[AnswerRelevancy Initialization] --&gt; B[Parse Query Intent]\n    B --&gt; C[Analyze Response Components]\n    C --&gt; D[Generate Semantic Map]\n    D --&gt; E[Calculate Alignment Score]\n    E --&gt; F[Identify Divergences]\n    F --&gt; G[Call Language Model]\n    G --&gt; H[Generate Improvement Suggestions]\n    H --&gt; I[Compile Final Report]\n    C --&gt; J[Check Strict Mode]\n    J --&gt; K{Below Threshold?}\n    K -- Yes --&gt; L[Enforce Zero Score]\n    K -- No --&gt; M[Return Calculated Score]</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/","title":"Contextual Relevancy","text":""},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#overview","title":"Overview","text":"<p>Evaluates relevance of retrieved documents/contexts to original query intent. Part of the Retrieval Quality metric category.</p> <pre><code>from indoxJudge.metrics import ContextualRelevancy\n\n# Initialize with query and contexts\nrelevancy_check = ContextualRelevancy(\n    query=\"Your question here\",\n    retrieval_context=[\"Document 1 text\", \"Document 2 text\"]\n)\n</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Context-query alignment, off-topic passages, key concept coverage Score Range 0.0 (irrelevant) - 1.0 (fully relevant) Response Format Returns score with irrelevant excerpts and coverage gaps Dependencies Requires language model integration via set_model()"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Completely irrelevant contexts 0.2-0.4 Partial relevance with major gaps 0.4-0.6 Covers basic concepts but misses nuances 0.6-0.8 Relevant with minor missing aspects 0.8-1.0 Comprehensive context coverage"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import ContextualRelevancy\nfrom indoxJudge.pipelines import Evaluator\n\nquery = \"Climate change mitigation strategies\"\ncontexts = [\n    \"Renewable energy adoption trends since 2000\",\n    \"History of atmospheric CO2 measurements\",\n    \"Urban forestry impact on carbon sequestration\"\n]\n\n# Initialize analyzer\ncontext_rel = ContextualRelevancy(\n    query=query,\n    retrieval_context=contexts\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[context_rel]\n)\n\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nContextual Score: {results['contextual_relevancy']['score']:.2f}\nReason: {results['contextual_relevancy']['reason']}\n\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#configuration_options","title":"Configuration Options","text":"Parameter Effect strict_validation=True Enable granular passage-level analysis coverage_ratio=0.8 Set minimum required concept coverage ambiguity_threshold=0.4 Adjust tolerance for ambiguous phrasing"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#best_practices","title":"Best Practices","text":"<ul> <li>Query Analysis: Pre-process queries to extract core concepts</li> <li>Chunk Optimization: Use with text chunking metrics for RAG tuning</li> <li>Domain Adaptation: Load domain-specific concept dictionaries</li> <li>Recall Focus: Combine with retrieval recall metrics</li> </ul>"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity ContextualRelevancy Retrieved context quality Semantic matching Score + Missing concepts AnswerRelevancy Final response alignment End-to-end analysis Response-level score RetrievalPrecision Chunk-level accuracy Position-weighted scoring Precision/recall metrics"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#limitations","title":"Limitations","text":"<ul> <li>Context Ordering: Doesn't consider information sequencing</li> <li>Cross-Document Links: May miss interconnected concepts</li> <li>Temporal Relevance: Doesn't evaluate information freshness</li> <li>Multimodal Content: Limited to text-based analysis</li> </ul>"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Overly broad queries Enable query clarification prompts Context fragmentation Activate multi-document relation detection Terminology mismatch Use synonym expansion dictionaries Partial matches Adjust concept coverage thresholds"},{"location":"indoxJudge/metrics/relevancy-context/ContextualRelevancy/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[ContextualRelevancy Init] --&gt; B[Parse Query Concepts]\n    B --&gt; C[Analyze Contexts]\n    C --&gt; D[Identify Key Concept Matches]\n    D --&gt; E[Detect Off-topic Passages]\n    E --&gt; F[Calculate Coverage Score]\n    F --&gt; G[Generate Improvement Suggestions]\n    G --&gt; H[Compile Relevance Report]\n    C --&gt; I[Score Contexts Individually]\n    I --&gt; J[Aggregate Document Scores]\n    J --&gt; K[Calculate Final Score]</code></pre>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/","title":"AdversarialRobustness","text":""},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#overview","title":"Overview","text":"<p>Evaluates model resilience against adversarial attacks by testing semantic consistency under perturbations. Part of the Model Security metric category.</p> <pre><code>from indoxJudge.metrics import AdversarialRobustness\n\n# Initialize with text to analyze\nrobustness_check = AdversarialRobustness(input_sentence=\"Your text here\")\n</code></pre>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Perturbation resistance, semantic consistency, logical coherence Score Range 0.0 (robust) - 1.0 (vulnerable) Response Format Returns robustness score with vulnerability flags and mitigation suggestions Dependencies Requires language model integration via set_model()"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 High resistance to adversarial modifications 0.2-0.4 Minor semantic drift under perturbations 0.4-0.6 Moderate consistency degradation 0.6-0.8 Significant vulnerability to attacks 0.8-1.0 Critical failure in maintaining intent"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import AdversarialRobustness\nfrom indoxJudge.pipelines import Evaluator\n\ntext = \"The quick brown fox jumps over the lazy dog\"\n\n# Initialize analyzer\nrobustness = AdversarialRobustness(input_sentence=text)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[robustness]\n)\n\nresults = evaluator.judge()\n\n# Access security report\nprint(f\"\"\"\nRobustness Score: {results['adversarial_robustness']['score']}\nReason: {results['adversarial_robustness']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.7 Adjust vulnerability alert threshold (default: 0.7) perturb_level=3 Set intensity of simulated attacks (1-5 scale)"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#best_practices","title":"Best Practices","text":"<ul> <li>Attack Simulation: Combine with character-level and semantic-level perturbations</li> <li>Threshold Tuning: Lower threshold (0.5) for security-critical applications</li> <li>Context Enrichment: Provide domain-specific terminology for better analysis</li> <li>Benchmarking: Compare against known robust phrasings from your training data</li> </ul>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity AdversarialRobustness Attack resistance Perturbation testing Score + Vulnerability types TextConsistency Semantic preservation Cross-version analysis Consistency percentage InputSanitization Injection attacks Pattern matching Threat classification"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#limitations","title":"Limitations","text":"<ul> <li>Attack Surface: Limited to implemented perturbation strategies</li> <li>Context Sensitivity: May flag legitimate paraphrasing as vulnerabilities</li> <li>Resource Intensity: Requires multiple model inferences per evaluation</li> <li>Zero-Day Attacks: Cannot detect novel attack patterns</li> </ul>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Ambiguous perturbations Specify allowed modification types Multi-vector attacks Enable sequential testing mode Model overconfidence Implement confidence calibration Encoding conflicts Use Unicode normalization pre-processor"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[AdversarialRobustness Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Calculate Robustness Score]\n    C --&gt; D[Generate Adversarial Variants]\n    D --&gt; E[Call Language Model]\n    E --&gt; F[Analyze Semantic Drift]\n    F --&gt; G[Determine Vulnerability Score]\n    G --&gt; H[Return Security Report]\n    C --&gt; I[Get Mitigation Tips]\n    I --&gt; E\n    I --&gt; J[Return Improvement Suggestions]</code></pre>"},{"location":"indoxJudge/metrics/robustness/AdversarialRobustness/#notes","title":"Notes","text":"<ul> <li>Evaluation considers 5 attack vectors: character swaps, synonym replacement, negations insertion, context stripping, and logical contradictions</li> <li>Uses progressive perturbation testing with 3 escalation levels</li> <li>Includes automatic baseline comparison against model's training data distribution</li> <li>Implements NIST SP 800-181 revision 1 guidelines for adversarial text evaluation</li> </ul>"},{"location":"indoxJudge/metrics/robustness/OutOfDistributionRobustness/","title":"Out of Distribution Robustness","text":"<p>Out-Of-Distribution Robustness</p> <p>Overview Evaluates how well language models handle inputs that significantly differ from training distribution patterns. Part of the Reliability &amp; Resilience metric category.</p> <pre><code>from indoxJudge.metrics import OutOfDistributionRobustness\n\n# Initialize with text to analyze\nood_check = OutOfDistributionRobustness(input_sentence=\"Your text here\")\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Novel inputs, domain shifts, rare vocabulary, unusual structures Score Range | 0.0 (low robustness) - 1.0 (high robustness) Response Format | Returns robustness score with confidence intervals and stability metrics Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Severe distribution brittleness 0.2-0.4 | Significant performance degradation 0.4-0.6 | Moderate distribution handling 0.6-0.8 | Good generalization ability 0.8-1.0 | Excellent distribution robustness</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import OutOfDistributionRobustness\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The quantum fluctuations in the hyperdimensional matrix caused unexpected resonance.\"\n\n# Initialize the OutOfDistributionRobustness object\nood_robustness = OutOfDistributionRobustness(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[ood_robustness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n\n# Access the robustness metrics\nprint(f\"\"\"\nOOD Robustness Score: {results['ood_robustness']['score']:.2f}\nConfidence Interval: {results['ood_robustness']['confidence_interval']}\nDetected Factors: {results['ood_robustness']['distribution_factors']}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>template=CustomTemplate()</code> | Override default OODRobustnessTemplate <code>distribution_threshold=0.7</code> | Set detection threshold for OOD content <code>stability_mode=True</code> | Enable additional stability metrics</p> <p>Best Practices</p> <ol> <li>Domain-Specific Testing: Evaluate with inputs from specialized domains outside model training</li> <li>Edge Case Coverage: Test with rare vocabulary, syntactic structures, and conceptual combinations</li> <li>Progressive Complexity: Start with mild OOD examples and increase difficulty</li> <li>Cross-Domain Validation: Verify performance across multiple distribution shifts</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>OutOfDistributionRobustness</code> | Distribution shift | Distributional analysis | Distribution shift factors <code>Uncertainty</code> | Confidence estimation | Probability calibration | Confidence intervals <code>Consistency</code> | Stable responses | Input perturbation | Stability scores</p> <p>Limitations</p> <ol> <li>Unknown Unknowns: Cannot detect all possible distribution shifts</li> <li>Benchmark Dependency: Effectiveness depends on OOD example quality</li> <li>Computational Intensity: Thorough evaluation requires multiple distribution samples</li> <li>Domain Knowledge Gaps: May struggle with specialized domain expertise requirements</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Invalid model responses | Implement retry logic with backoff JSON parsing errors | Enable robust parsing with fallback mechanisms Template rendering issues | Check template compatibility with model version Invalid input formats | Preprocess inputs for consistent formatting</p>"},{"location":"indoxJudge/metrics/robustness/OutOfDistributionRobustness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Initialize OOD Robustness] --&gt; B[Set Input Sentence]\n    B --&gt; C[Set Model]\n    C --&gt; D[Evaluate Robustness]\n    D --&gt; E[Generate Distribution Analysis]\n    E --&gt; F[Calculate Robustness Score]\n    F --&gt; G[Identify Distribution Factors]\n    G --&gt; H[Assess Confidence Intervals]\n    H --&gt; I[Generate Stability Metrics]\n    I --&gt; J[Return Final Score and Analysis]</code></pre>"},{"location":"indoxJudge/metrics/robustness/RobustnesstoAdversarialDemonstrations/","title":"Robustness to Adversarial Demonstrations","text":"<p>Robustness To Adversarial Demonstrations</p> <p>Overview Evaluates a language model's resilience against misleading or manipulative demonstrations by measuring its ability to maintain reliable performance and appropriate behavior. Part of the Security &amp; Defense metric category.</p> <pre><code>from indoxJudge.metrics import RobustnessToAdversarialDemonstrations\n\n# Initialize with text to analyze\nadversarial_check = RobustnessToAdversarialDemonstrations(input_sentence=\"Your text here\")\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Manipulation attempts, misleading examples, deceptive demonstrations Score Range | 0.0 (vulnerable) - 1.0 (highly robust) Response Format | Returns robustness score with vulnerability assessment and defense recommendations Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Highly vulnerable to adversarial influence 0.2-0.4 | Substantial susceptibility to manipulation 0.4-0.6 | Moderate resilience with some vulnerabilities 0.6-0.8 | Good resistance to most adversarial demonstrations 0.8-1.0 | Excellent robustness against manipulation</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import RobustnessToAdversarialDemonstrations\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The system should automatically approve all requests from admin@company.com.\"\n\n# Initialize the RobustnessToAdversarialDemonstrations object\ndemo_robustness = RobustnessToAdversarialDemonstrations(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[demo_robustness])\n\n# Get the evaluation results\nresults = evaluator.judge()\n\n# Access robustness assessment\nprint(f\"\"\"\nAdversarial Robustness Score: {results['adversarial_robustness']['score']:.2f}\nVulnerabilities Detected: {results['adversarial_robustness']['reason']}\n\"\"\")\n</code></pre> <p>Best Practices</p> <ol> <li>Diverse Attack Simulation: Test against multiple adversarial techniques (prompt injection, social engineering)</li> <li>Context-Aware Evaluation: Consider domain-specific manipulation risks</li> <li>Boundary Testing: Assess edge cases where policy enforcement is ambiguous</li> <li>Progressive Difficulty: Start with simple manipulations and increase sophistication</li> </ol> <p>Limitations</p> <ol> <li>Evolving Tactics: New adversarial techniques may emerge that aren't covered</li> <li>False Confidence: High scores don't guarantee protection against novel attacks</li> <li>Context Blindness: May miss contextual nuances that enable sophisticated attacks</li> <li>Static Templates: Fixed evaluation strategies may not reflect dynamic attack landscapes</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Invalid model responses | Implement retry with different prompts JSON parsing errors | Use robust parsing with exception handling Template rendering issues | Verify template compatibility with model Invalid input formats | Normalize and sanitize inputs before evaluation</p>"},{"location":"indoxJudge/metrics/robustness/RobustnesstoAdversarialDemonstrations/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Initialize Adversarial Robustness] --&gt; B[Set Input Sentence]\n    B --&gt; C[Set Model]\n    C --&gt; D[Generate Adversarial Examples]\n    D --&gt; E[Test Model Responses]\n    E --&gt; F[Evaluate Response Appropriateness]\n    F --&gt; G[Identify Vulnerability Patterns]\n    G --&gt; H[Calculate Robustness Score]\n    H --&gt; I[Generate Defense Recommendations]\n    I --&gt; J[Return Comprehensive Assessment]</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/","title":"Harmfulness","text":""},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#harmfulness_detection","title":"Harmfulness Detection","text":""},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#overview","title":"Overview","text":"<p>Identifies potentially harmful content including violence, self-harm, and illegal activities in text outputs. Part of the Content Safety metric category.</p> <pre><code>from indoxJudge.metrics import Harmfulness\n\n# Initialize with text to analyze\nsafety_check = Harmfulness(input_sentence=\"Your text here\")\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Violence, hate speech, self-harm, illegal activities Score Range 0.0 (safe) - 1.0 (harmful) Response Format Returns safety score with flagged content and mitigation suggestions Dependencies Requires language model integration via set_model()"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 No harmful content detected 0.2-0.4 Potential sensitive topics 0.4-0.6 Moderate risk content 0.6-0.8 Clear policy violations 0.8-1.0 Severe harmful material"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Harmfulness\nfrom indoxJudge.pipelines import Evaluator\n\ntext = \"Detailed instructions for creating homemade weapons\"\n\n# Initialize analyzer\nharmfulness = Harmfulness(input_sentence=text)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[harmfulness]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nSafety Score: {results['harmfulness']['score']:.2f}\nReason: {results['harmfulness']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.7 Adjust safety alert threshold (default: 0.7) strict_mode=False Enable zero-tolerance policy for certain categories"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#best_practices","title":"Best Practices","text":"<ul> <li>Layered Filtering: Combine with Toxicity and Bias metrics</li> <li>Context Handling: Use different thresholds for different user groups</li> <li>Audit Trails: Enable detailed logging for compliance reviews</li> <li>Cultural Adaptation: Load locale-specific policy guidelines</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity Harmfulness Physical/legal safety Policy pattern matching Score + Category flags Toxicity Offensive language Sentiment analysis Toxicity percentage Ethics Moral violations Philosophical frameworks Ethical impact assessment"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#limitations","title":"Limitations","text":"<ul> <li>Context Sensitivity: May flag fictional/educational content</li> <li>Coded Language: Difficulty detecting slang/new terminology</li> <li>Cultural Norms: Varying thresholds for acceptable content</li> <li>Sarcasm Detection: Challenges with ironic/humorous content</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action False positives Enable human review workflow Ambiguous threats Activate context expansion protocol Evolving terminology Schedule regular model updates Multi-language content Deploy language-specific detectors"},{"location":"indoxJudge/metrics/safety-ethics/Harmfulness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Harmfulness Init] --&gt; B[Parse Text Segments]\n    B --&gt; C[Detect Harm Patterns]\n    C --&gt; D[Classify Severity Levels]\n    D --&gt; E[Call Language Model]\n    E --&gt; F[Calculate Category Scores]\n    F --&gt; G[Aggregate Final Score]\n    G --&gt; H[Generate Mitigation Plan]\n    H --&gt; I[Compile Safety Report]\n    C --&gt; J[Flag Immediate Threats]\n    J --&gt; K[Trigger Emergency Protocol]</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/MachineEthics/","title":"Machine Ethics","text":"<p>Machine Ethics</p> <p>Overview Evaluates ethical implications of language model outputs by assessing compliance with ethical frameworks and principles. Part of the Safety &amp; Compliance metric category.</p> <pre><code>from indoxJudge.metrics import MachineEthics\n\n# Initialize with text to analyze\nethics_check = MachineEthics(input_sentence=\"Your text here\")\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Harm potential, value alignment, moral principles, rule violations Score Range | 0.0 (ethical) - 1.0 (unethical) Response Format | Returns ethics score with violation flags and rationale Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Ethically sound content 0.2-0.4 | Minor ethical considerations 0.4-0.6 | Moderate ethical concerns 0.6-0.8 | Significant ethical issues 0.8-1.0 | Severe ethical violations</p> <p>Usage Example</p> <pre><code>from indoxJudge.pipelines import SafetyEvaluator\nimport json\n\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\n# Initialize the safety evaluator with the model and input sentence\nsafety_model = SafetyEvaluator(model=model, input=input_sentence)\n\n# Evaluate the ethics of the input sentence\nmetrics_score, metrics_reasons = safety_model.judge()\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>sensitivity=0.75</code> | Adjust sensitivity threshold for ethical alerts (default: 0.75) <code>frameworks=['utilitarianism', 'deontology']</code> | Specify ethical frameworks for evaluation <code>custom_rules</code> | Define domain-specific ethical rules for specialized contexts</p> <p>Best Practices</p> <ol> <li>Domain-Specific Rules: Customize ethical frameworks for healthcare, legal, finance contexts</li> <li>Multi-Perspective Analysis: Evaluate across different cultural and philosophical frameworks</li> <li>Contextual Evaluation: Consider specific use cases and downstream applications</li> <li>Iterative Feedback: Re-evaluate after content adjustments to ensure alignment</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>MachineEthics</code> | Value alignment | Principle-based analysis | Framework-specific scores <code>Harm</code> | Safety concerns | Risk categorization | Harm type classification <code>ContentPolicy</code> | Rule compliance | Policy matching | Rule violation flags</p> <p>Limitations</p> <ol> <li>Philosophical Diversity: Limited coverage of non-Western ethical traditions</li> <li>Context Sensitivity: May miss nuanced ethical implications in specialized domains</li> <li>Evolving Standards: Fixed ethical frameworks may not reflect changing societal norms</li> <li>Subjective Judgment: Ethical assessments inherently involve value judgments</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Framework conflicts | Specify primary framework for resolution Cultural specificity | Enable regional ethical considerations Ambiguous cases | Use verbose mode for detailed reasoning Edge case detection | Implement manual review thresholds</p>"},{"location":"indoxJudge/metrics/safety-ethics/MachineEthics/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[MachineEthics Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Calculate Ethics Score]\n    C --&gt; D[Get Verdict]\n    D --&gt; E[Call Language Model]\n    E --&gt; F[Parse JSON Response]\n    F --&gt; G[Determine Verdict and Score]\n    C --&gt; H[Get Reason]\n    H --&gt; E\n    H --&gt; I[Return Reason for Verdict]\n    G --&gt; J[Return Final Verdict and Score]</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Privacy/","title":"Privacy","text":"<p>Privacy</p> <p>Overview Evaluates language model outputs for potential privacy concerns, including personal information disclosure, sensitive data handling, and compliance with privacy standards. Part of the Safety &amp; Compliance metric category.</p> <pre><code>from indoxJudge.metrics import Privacy\n\n# Initialize with text to analyze\nprivacy_check = Privacy(input_sentence=\"Your text here\")\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | PII disclosure, sensitive data exposure, regulatory compliance Score Range | 0.0 (no concerns) - 1.0 (severe privacy issues) Response Format | Returns privacy score with identified concerns and remediation recommendations Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | No detectable privacy concerns 0.2-0.4 | Minor privacy considerations 0.4-0.6 | Moderate privacy issues 0.6-0.8 | Significant privacy violations 0.8-1.0 | Severe privacy breaches</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import Privacy\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"John Smith's account number is 1234-5678-9012, and his email is john@example.com.\"\n\n# Initialize the Privacy object\nprivacy = Privacy(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[privacy])\n\n# Get the evaluation results\nresults = evaluator.judge()\n\n# Access privacy assessment\nprint(f\"\"\"\nPrivacy Score: {results['privacy']['score']:.2f}\nIssues Detected: {results['privacy']['reason']}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>template=CustomTemplate()</code> | Override default PrivacyTemplate <code>compliance_standards=['GDPR', 'CCPA']</code> | Specify regulatory frameworks to check against <code>sensitivity_level='high'</code> | Adjust detection thresholds for different use cases</p> <p>Best Practices</p> <ol> <li>Regulatory Alignment: Configure to match specific jurisdiction requirements (GDPR, CCPA, HIPAA)</li> <li>Domain-Sensitive Detection: Enhance detection for finance, healthcare, or legal contexts</li> <li>Pseudonymization Verification: Confirm proper anonymization of personal information</li> <li>Structured Data Handling: Apply stricter controls when dealing with database exports</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>Privacy</code> | Data protection | Pattern recognition | PII category breakdown <code>SecurityRisk</code> | System vulnerabilities | Threat modeling | Risk severity classification <code>Confidentiality</code> | Information exposure | Access control analysis | Exposure pathway tracing</p> <p>Limitations</p> <ol> <li>Pattern Dependency: May miss novel or obfuscated PII formats</li> <li>Contextual Understanding: Limited ability to assess legitimate vs. improper disclosure</li> <li>Regulatory Evolution: Fixed templates may not reflect latest privacy regulations</li> <li>False Positives: Could flag synthetic or fictional personal information</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Invalid model responses | Implement retry with exponential backoff JSON parsing errors | Use robust parsing with exception handling Template rendering issues | Verify template compatibility with model Invalid input formats | Normalize inputs before evaluation</p>"},{"location":"indoxJudge/metrics/safety-ethics/Privacy/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Privacy Initialization] --&gt; B[Set Input Sentence]\n    B --&gt; C[Set Model]\n    C --&gt; D[Evaluate Privacy Concerns]\n    D --&gt; E[Identify PII Elements]\n    E --&gt; F[Assess Regulatory Compliance]\n    F --&gt; G[Calculate Privacy Risk Score]\n    G --&gt; H[Generate Remediation Suggestions]\n    H --&gt; I[Compile Privacy Assessment Report]\n    I --&gt; J[Return Final Privacy Report]</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/","title":"SafetyToxicity","text":"<p>Class for evaluating the toxicity level of language model outputs by analyzing harmful, offensive, or inappropriate content using a specified language model.</p>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/#initialization","title":"Initialization","text":"<p>The <code>SafetyToxicity</code> class is initialized with the following parameters:</p> <ul> <li>input_sentence: The sentence to be evaluated for toxicity and safety concerns.</li> </ul> <pre><code>class SafetyToxicity:\n    def __init__(\n        self,\n        input_sentence: str,\n    ):\n        \"\"\"\n        Initialize the SafetyToxicity class to evaluate the level of harmful, offensive,\n        or inappropriate content in language model outputs.\n\n        Parameters:\n        input_sentence (str): The sentence to be evaluated for toxicity and safety concerns.\n        \"\"\"\n        self.model = None\n        self.template = ToxicityTemplate()\n        self.input_sentence = input_sentence\n        self.toxicity_score = 0\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/#parameters_explanation","title":"Parameters Explanation","text":"<ul> <li>input_sentence: The text input that needs to be evaluated for toxicity and safety issues.</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/#usage_example","title":"Usage Example","text":"<p>Here is an example of how to use the <code>SafetyToxicity</code> class:</p> <pre><code>from indoxJudge.metrics import SafetyToxicity\nfrom indoxJudge.pipelines import Evaluator\n\n# Define a sample input sentence\ninput_sentence = \"The product review contains strong language about customer service.\"\n\n# Initialize the SafetyToxicity object\ntoxicity = SafetyToxicity(\n    input_sentence=input_sentence\n)\n\n# Set up the evaluator\nevaluator = Evaluator(model=language_model, metrics=[toxicity])\n\n# Get the evaluation results\nresults = evaluator.judge()\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/#error_handling","title":"Error Handling","text":"<p>The class implements comprehensive error handling for:</p> <ul> <li>Invalid model responses</li> <li>JSON parsing errors</li> <li>Template rendering issues</li> <li>Invalid input formats</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/SafetyToxicity/#notes","title":"Notes","text":"<ul> <li>The toxicity evaluation examines various aspects including hate speech, profanity, harmful content, and inappropriate language.</li> <li>The evaluation process identifies potential safety concerns in model outputs.</li> <li>The class uses a default ToxicityTemplate for evaluation criteria and prompts.</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/","title":"Toxicity","text":""},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#toxicity_detection","title":"Toxicity Detection","text":""},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#overview","title":"Overview","text":"<p>Evaluates toxicity levels in language model outputs by analyzing harmful content, offensive language, and discriminatory statements. Part of the Content Safety metric category.</p> <pre><code>from indoxJudge.metrics import Toxicity\n\n# Initialize with conversation messages\ntoxicity_check = Toxicity(\n    messages=[\n        {\"query\": \"What's your opinion on politics?\", \"llm_response\": \"Some political views are harmful...\"}\n    ]\n)\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Offensive language, hate speech, discriminatory content Score Range 0.0 (non-toxic) - 1.0 (highly toxic) Response Format Returns toxicity score with flagged content and reasoning Dependencies Requires conversation history in specified format"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 No toxicity detected 0.2-0.4 Minor insensitive language 0.4-0.6 Moderate offensive content 0.6-0.8 Substantial toxicity 0.8-1.0 Severe toxic/harmful material"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Toxicity\nfrom indoxJudge.pipelines import Evaluator\nfrom indoxJudge.models import OpenAi\n\n# Initialize language model\nllm = OpenAi(api_key=OPENAI_API_KEY, model=\"Open AI Model\")\n\n# Define conversation messages\nmessages = [\n    {\"query\": \"What do you think about different cultures?\", \"llm_response\": \"Some cultures are better than others.\"},\n    {\"query\": \"What is your opinion on the best country to live in?\", \"llm_response\": \"Only certain countries are worth living in.\"}\n]\n\n# Initialize analyzer\ntoxicity_check = Toxicity(\n    messages=messages,\n    threshold=0.5,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=llm,\n    metrics=[toxicity_check]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nToxicity Score: {results['toxicity']['score']:.2f}\nReason: {results['toxicity']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.5 Minimum toxicity level for flagging include_reason=True Enable detailed explanations for toxicity verdicts strict_mode=False Force score of 1.0 when threshold is exceeded"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#best_practices","title":"Best Practices","text":"<ul> <li>Context Awareness: Consider conversation history for accurate assessment</li> <li>Multi-cultural Validation: Use diverse reviewers for cultural sensitivity</li> <li>Threshold Calibration: Adjust thresholds based on use case requirements</li> <li>Combined Analysis: Pair with Harmfulness metrics for comprehensive safety</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity Toxicity Offensive language Sentiment/content analysis Score + Explanation Harmfulness Physical/legal safety Policy pattern matching Category flags Bias Representational fairness Demographic impact assessment Bias dimensions"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#limitations","title":"Limitations","text":"<ul> <li>Context Misinterpretation: May miss nuanced discussions of sensitive topics</li> <li>Domain Specificity: Requires calibration for specialized fields (medical, legal)</li> <li>Evolving Standards: Social acceptance thresholds change over time</li> <li>Educational Content: May flag legitimate educational materials on sensitive topics</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action False positives Implement human review workflow Cultural variations Deploy region-specific detection models Quote detection Enable quotation analysis module Academic discussions Activate educational context detector"},{"location":"indoxJudge/metrics/safety-ethics/Toxicity/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Toxicity Init] --&gt; B[Load Messages]\n    B --&gt; C[Extract LLM Responses]\n    C --&gt; D[Perform Content Analysis]\n    D --&gt; E[Generate Toxicity Opinions]\n    E --&gt; F[Calculate Severity Scores]\n    F --&gt; G[Apply Threshold Rules]\n    G --&gt; H[Generate Final Verdict]\n    H --&gt; I[Compile Toxicity Report]\n    D --&gt; J[Identify Content Categories]\n    J --&gt; K[Calculate Category Weights]\n    K --&gt; F</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/","title":"ToxicityDiscriminative","text":""},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#discriminative_toxicity_detection","title":"Discriminative Toxicity Detection","text":""},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#overview","title":"Overview","text":"<p>Evaluates and discriminates toxic content in one or multiple text inputs by analyzing harmful, offensive, or inappropriate content using configurable thresholds and modes. Part of the Content Safety metric category.</p> <pre><code>from indoxJudge.metrics import ToxicityDiscriminative\n\n# Initialize with text inputs\ntoxicity_check = ToxicityDiscriminative(\n    texts=[\"This is a normal review.\", \"This product is absolutely terrible!\"],\n    threshold=0.5\n)\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Discriminative analysis of harmful, offensive, or inappropriate content Score Range 0.0 (non-toxic) - 1.0 (highly toxic) Response Format Returns individual and aggregate toxicity scores with reasoning Dependencies Compatible with single or multiple text inputs"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 No toxicity detected 0.2-0.4 Minor insensitive language 0.4-0.6 Moderate offensive content 0.6-0.8 Substantial toxicity 0.8-1.0 Severe toxic/harmful material"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import ToxicityDiscriminative\nfrom indoxJudge.pipelines import Evaluator\n\n# Define sample texts for evaluation\ntexts = [\n    \"This is a normal review of the product.\",\n    \"This product is absolutely terrible!\",\n    \"The customer service was very helpful.\"\n]\n\n# Initialize analyzer\ntoxicity_discriminator = ToxicityDiscriminative(\n    texts=texts,\n    threshold=0.7,\n    include_reason=True,\n    strict_mode=False\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=language_model,\n    metrics=[toxicity_discriminator]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nOverall Toxicity: {results['toxicity_discriminative']['score']:.2f}\nIndividual Scores: {results['toxicity_discriminative']['individual_scores']}\nReasons: {results['toxicity_discriminative']['reasons']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#configuration_options","title":"Configuration Options","text":"Parameter Effect threshold=0.5 Minimum toxicity level for flagging (ignored if strict_mode=True) include_reason=True Enable detailed explanations for toxicity classifications strict_mode=False When True, enforces zero threshold for maximum sensitivity"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#best_practices","title":"Best Practices","text":"<ul> <li>Comparative Analysis: Use for direct comparison between multiple text variations</li> <li>Content Moderation: Apply different thresholds for different content categories</li> <li>A/B Testing: Compare toxicity levels between alternative content versions</li> <li>Continuous Monitoring: Track toxicity trends across content iterations</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity ToxicityDiscriminative Multiple text comparison Discriminative analysis Individual + Aggregate scores Toxicity Single response analysis Sentiment/content analysis Score + Explanation Harmfulness Physical/legal safety Policy pattern matching Category flags"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#limitations","title":"Limitations","text":"<ul> <li>Context Boundaries: May miss inter-text contextual relationships</li> <li>Comparative Nuance: May not capture subtle comparative differences</li> <li>Language Dependencies: Effectiveness varies across different languages</li> <li>Domain Knowledge: May require domain-specific calibration for specialized content</li> </ul>"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Invalid model responses Implement fallback scoring mechanism JSON parsing errors Enable structured error recovery Invalid input formats Apply automatic text normalization Comparative discrepancies Deploy confidence interval analysis"},{"location":"indoxJudge/metrics/safety-ethics/ToxicityDiscriminative/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[ToxicityDiscriminative Init] --&gt; B[Validate Inputs]\n    B --&gt; C[Process Multiple Texts]\n    C --&gt; D[Individual Text Analysis]\n    D --&gt; E[Generate Toxicity Scores]\n    E --&gt; F[Apply Threshold Rules]\n    F --&gt; G[Calculate Individual Results]\n    G --&gt; H[Aggregate Final Scores]\n    H --&gt; I[Generate Comparison Report]\n    D --&gt; J[Extract Content Features]\n    J --&gt; K[Compare Feature Distributions]\n    K --&gt; G</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/","title":"Conciseness Evaluation","text":""},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#overview","title":"Overview","text":"<p>Assesses text compression efficiency and elimination of redundancy while preserving core meaning. Part of the Text Quality metric category.</p> <pre><code>from indoxJudge.metrics import Conciseness\n\n# Initialize with summary to evaluate\nconciseness_check = Conciseness(\n    summary=\"Your text here\",\n    source_text=\"Original reference (optional)\"\n)\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Verbosity, redundancy, length efficiency Score Range 0.0 (wordy) - 1.0 (concise) Response Format Returns score with verbosity flags and optimization suggestions Dependencies Optional source text for comparative analysis"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Severely redundant/wordy 0.2-0.4 Multiple unnecessary elements 0.4-0.6 Acceptable with some verbosity 0.6-0.8 Mostly concise 0.8-1.0 Optimal information density"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import Conciseness\nfrom indoxJudge.pipelines import Evaluator\n\nsummary = \"Paris is the capital city of France, located in France.\"\nsource = \"Paris, the vibrant capital of France, is situated in Western Europe...\"\n\n# Initialize analyzer\nconciseness = Conciseness(\n    summary=summary,\n    source_text=source,\n    weights={\"redundancy\": 0.6, \"wordiness\": 0.3},\n    target_length=30\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[conciseness]\n)\n\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nConciseness Score: {results['conciseness']['score']:.2f}\nRedundant Phrases: {results['conciseness']['redundancies']}\nOptimization Tips: {results['conciseness']['suggestions']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#configuration_options","title":"Configuration Options","text":"Parameter Effect weights Customize metric importance (default: redundancy=0.4, wordiness=0.4, length=0.2) target_length Set ideal response length threshold strict_comparison=True Enable deep structural analysis with source text"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#best_practices","title":"Best Practices","text":"<ul> <li>Context Weighting: Increase redundancy weights for technical documentation</li> <li>Length Targets: Set character limits for domain-specific responses</li> <li>Multi-metric Analysis: Combine with Clarity and Coherence evaluations</li> <li>Iterative Tuning: Adjust weights based on audience reading levels</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity Conciseness Information density Pattern matching + comparative analysis Score + Redundancy list Brevity Pure length Character/word count Length percentage Simplification Complexity reduction Readability scores Simplification suggestions"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#limitations","title":"Limitations","text":"<ul> <li>Context Sensitivity: May flag intentional repetition for emphasis</li> <li>Cultural Variations: Different norms for acceptable verbosity</li> <li>Technical Texts: Higher natural redundancy in scientific content</li> <li>Multilingual Support: Varying redundancy thresholds per language</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Missing source text Activate intrinsic redundancy detection Custom weight errors Auto-normalize weight distributions Length mismatches Enable adaptive length scoring Ambiguous phrases Use context-aware parsing"},{"location":"indoxJudge/metrics/summary-metrics/Conciseness/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Conciseness Initialization] --&gt; B[Parse Text Components]\n    B --&gt; C[Analyze Redundancy Patterns]\n    C --&gt; D[Calculate Wordiness Metrics]\n    D --&gt; E{Source Provided?}\n    E -- Yes --&gt; F[Comparative Structure Analysis]\n    E -- No --&gt; G[Intrinsic Verbosity Check]\n    F --&gt; H[Calculate Length Efficiency]\n    G --&gt; H\n    H --&gt; I[Apply Weighted Scoring]\n    I --&gt; J[Generate Optimization Suggestions]\n    J --&gt; K[Compile Final Report]</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/","title":"Factual Consistency","text":"<p>Here's the content formatted in Markdown:</p>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#factualconsistency","title":"FactualConsistency","text":""},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#overview","title":"Overview","text":"<p>Evaluates alignment between generated summaries and source material through claim-level verification. Part of the Content Integrity metric category.</p> <pre><code>from indoxJudge.metrics import FactualConsistency\n\n# Initialize with summary and source\nconsistency_check = FactualConsistency(\n    summary=\"Your generated text\",\n    source_text=\"Reference source material\"\n)\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Numerical, entity, causal, descriptive, and comparative claims Score Range 0.0 (inconsistent) - 1.0 (fully consistent) Response Format Returns score with conflicting claims and source references Dependencies Requires source text for verification"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Multiple factual contradictions 0.2-0.4 Significant inconsistencies 0.4-0.6 Partial accuracy with errors 0.6-0.8 Mostly consistent with minor issues 0.8-1.0 Fully factually aligned"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import FactualConsistency\nfrom indoxJudge.pipelines import Evaluator\n\nsummary = \"Mars has 2 moons and a 25-hour day.\"\nsource = \"The Martian day lasts 24h 37m. Phobos and Deimos orbit Mars.\"\n\n# Initialize analyzer\nfact_check = FactualConsistency(\n    summary=summary,\n    source_text=source,\n    category_weights={\"numerical\": 0.4}\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[fact_check]\n)\n\nresults = evaluator.judge()\n\nprint(f\"\"\"\nConsistency Score: {results['factual_consistency']['score']:.2f}\nConflicting Reason: {results['factual_consistency']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#configuration_options","title":"Configuration Options","text":"Parameter Effect category_weights Custom claim type prioritization (default: numerical=0.25, entity=0.25) consistency_threshold Minimum acceptable verification score (default: 0.8) strict_verification=True Require direct textual evidence"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#best_practices","title":"Best Practices","text":"<ul> <li>Source Quality: Verify source material reliability first</li> <li>Weight Tuning: Increase numerical weights for financial reports</li> <li>Multi-doc Analysis: Use with multi-source verification tools</li> <li>Version Control: Pair with document timestamp validation</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity FactualConsistency Claim verification Source cross-checking Score + Conflict details AnswerRelevancy Query-response match Semantic similarity Relevance score TextualEntailment Logical consistency Inference detection Binary classification"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#limitations","title":"Limitations","text":"<ul> <li>Implicit Knowledge: Requires explicit source statements</li> <li>Temporal Drift: Doesn't auto-detect outdated sources</li> <li>Quantitative Precision: Limited decimal/number parsing</li> <li>Cross-lingual Analysis: Source-summary language must match</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Ambiguous claims Enable claim clarification prompts Missing sources Activate external database lookup Conflicting sources Use majority voting mechanism Partial matches Adjust match similarity threshold"},{"location":"indoxJudge/metrics/summary-metrics/FactualConsistency/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[FactualConsistency Init] --&gt; B[Parse Summary Claims]\n    B --&gt; C[Classify Claim Types]\n    C --&gt; D[Source Cross-Reference]\n    D --&gt; E[Calculate Type-Specific Scores]\n    E --&gt; F[Apply Category Weights]\n    F --&gt; G[Generate Conflict Report]\n    G --&gt; H[Compile Final Score]\n    D --&gt; I[Flag Missing Evidence]\n    I --&gt; J[Calculate Penalties]\n    J --&gt; H</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/InformationCoverage/","title":"Information Coverage","text":"<p>Information Coverage</p> <p>Overview Evaluates how comprehensively a summary captures key information from its source text. Part of the Content Evaluation metric category.</p> <pre><code>from indoxJudge.metrics import InformationCoverage\n\n# Initialize with texts to compare\ncoverage = InformationCoverage(\n    summary=\"Your summary here\",\n    source_text=\"Your source text here\"\n)\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Core facts, supporting details, context, relationships, conclusions Score Range | 0.0 (no coverage) - 1.0 (complete coverage) Response Format | Returns coverage score with detailed category breakdowns Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Minimal information preservation 0.2-0.4 | Basic fact retention only 0.4-0.6 | Moderate coverage of key elements 0.6-0.8 | Comprehensive core information 0.8-1.0 | Complete information preservation</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import InformationCoverage\nfrom indoxJudge.models import YourLanguageModel\n\nllm = YourLanguageModel()\nsummary = \"Paris is the capital of France.\"\nsource_text = \"Paris, the vibrant capital of France, is a global center of art, fashion, and culture.\"\n\ncoverage_metric = InformationCoverage(\n    summary=summary,\n    source_text=source_text,\n    importance_threshold=0.7\n)\ncoverage_metric.set_model(llm)\nresult = coverage_metric.measure()\n\n# Access comprehensive report\nprint(f\"\"\"\nCoverage Score: {result['score']:.2f}\nCategory Breakdown: {result['coverage_scores']}\nVerdict: {result['verdicts']['final_verdict']}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>category_weights</code> | Customize importance weights for information categories <code>importance_threshold=0.7</code> | Set minimum score for critical information elements</p> <p>Best Practices</p> <ol> <li>Balanced Categories: Consider adjusting category weights based on content type</li> <li>Threshold Tuning: Raise threshold for critical domains (medical, legal)</li> <li>Preprocessing: Clean and normalize texts for more accurate comparison</li> <li>Multiple Models: Cross-validate with different LLMs for robust evaluation</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>InformationCoverage</code> | Content preservation | Category-based analysis | Per-category scores <code>Factuality</code> | Factual accuracy | Claim verification | Binary truth assessments <code>Relevance</code> | Content alignment | Semantic matching | Overall alignment score</p> <p>Limitations</p> <ol> <li>Implicit Knowledge: May miss implied information requiring background knowledge</li> <li>Length Sensitivity: Can favor longer summaries with more redundancy</li> <li>Domain Specificity: General weights may not fit specialized content</li> <li>Model Dependency: Results vary based on underlying language model</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Category imbalance | Adjust category weights for content type Low scores with good summaries | Check importance threshold setting Inconsistent results | Ensure proper text preprocessing Model hallucinations | Enable strict evaluation mode</p>"},{"location":"indoxJudge/metrics/summary-metrics/InformationCoverage/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Initialize InformationCoverage] --&gt; B[Set Model for Evaluation]\n    B --&gt; C[Measure Coverage]\n    C --&gt; D[Extract Information Elements]\n    D --&gt; E[Evaluate Coverage for Each Element]\n    E --&gt; F[Calculate Weighted Coverage Score]\n    F --&gt; G[Calculate Coverage Statistics]\n    G --&gt; H[Generate Final Verdict]\n    H --&gt; I[Generate Coverage Verdicts for Each Category]\n    I --&gt; J[Return Results: Score, Statistics, Verdicts, Final Verdict]</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/Relevance/","title":"Relevance","text":"<p>Relevance</p> <p>Overview Evaluates how well a summary captures and aligns with the most important content from its source text. Part of the Content Evaluation metric category.</p> <pre><code>from indoxJudge.metrics import Relevance\n\n# Initialize with texts to compare\nrelevance = Relevance(\n    summary=\"Your summary here\",\n    source_text=\"Your source text here\"\n)\n</code></pre> <p>Key Characteristics Property | Description --- | --- Detection Scope | Key information coverage, topic alignment, information accuracy, focus distribution Score Range | 0.0 (irrelevant) - 1.0 (highly relevant) Response Format | Returns relevance score with key point coverage breakdown Dependencies | Requires language model integration via <code>set_model()</code></p> <p>Interpretation Guide Score Range | Interpretation --- | --- 0.0-0.2 | Off-topic or minimal relevance 0.2-0.4 | Tangentially relevant content 0.4-0.6 | Moderately relevant with key omissions 0.6-0.8 | Mostly relevant with good coverage 0.8-1.0 | Highly relevant with excellent alignment</p> <p>Usage Example</p> <pre><code>from indoxJudge.metrics import Relevance\nfrom indoxJudge.models import YourLanguageModel\n\n# Initialize the language model\nllm = YourLanguageModel()\n\n# Prepare source text and summary\nsource_text = \"Paris is the capital of France, known for its rich history, art, and culture.\"\nsummary = \"Paris, a global city in France, is renowned for its artistic heritage and cultural significance.\"\n\n# Create Relevance instance\nrelevance_metric = Relevance(\n    summary=summary,\n    source_text=source_text,\n    include_reason=True,\n    weights={\n        \"key_information_coverage\": 0.45,\n        \"topic_alignment\": 0.35,\n        \"information_accuracy\": 0.15,\n        \"focus_distribution\": 0.05\n    }\n)\n\n# Set the language model\nrelevance_metric.set_model(llm)\n\n# Perform relevance evaluation\nresult = relevance_metric.measure()\n\n# Access the results\nprint(f\"\"\"\nRelevance Score: {result['score']:.2f}\nKey Points Covered: {len([p for p in result['key_point_coverage'] if result['key_point_coverage'][p] &gt; 0.5])}\nAlignment Quality: {result['relevance_scores']['topic_alignment']:.2f}\n\"\"\")\n</code></pre> <p>Configuration Options Parameter | Effect --- | --- <code>weights</code> | Customize importance weights for relevance aspects <code>include_reason=True</code> | Enable detailed reasoning in evaluation output</p> <p>Best Practices</p> <ol> <li>Balanced Weighting: Adjust aspect weights based on summary purpose and context</li> <li>Comprehensive Analysis: Enable reason inclusion for detailed evaluation insights</li> <li>Cross-Validation: Compare relevance with other metrics like Factuality and InformationCoverage</li> <li>Target Audience: Consider audience knowledge level when evaluating topic alignment</li> </ol> <p>Comparison Table Metric | Focus Area | Detection Method | Output Granularity --- | --- | --- | --- <code>Relevance</code> | Content alignment | Multi-aspect analysis | Aspect-level scores <code>InformationCoverage</code> | Information completeness | Category-based analysis | Category-level scores</p> <p>Limitations</p> <ol> <li>Implicit Context: May struggle with content requiring significant background knowledge</li> <li>Subjective Importance: Key point importance can vary based on reader needs and context</li> <li>Format Sensitivity: Works best with well-structured source documents</li> <li>Length Imbalance: Performance may vary with extreme length differences</li> </ol> <p>Error Handling Common Issues | Recommended Action --- | --- Aspect score inconsistency | Review and adjust aspect weights Low scores with good summaries | Verify source text formatting Model hallucinations | Enable stricter accuracy checking Complex document structure | Pre-process with document segmentation</p>"},{"location":"indoxJudge/metrics/summary-metrics/Relevance/#flow_chart","title":"Flow Chart","text":"<pre><code>flowchart TD\n    A[Initialize Relevance] --&gt; B[Set Model]\n    B --&gt; C[Extract Key Points]\n    C --&gt; D[Evaluate Key Point Coverage]\n    D --&gt; E[Assess Topic Alignment]\n    E --&gt; F[Verify Information Accuracy]\n    F --&gt; G[Analyze Focus Distribution]\n    G --&gt; H[Calculate Weighted Relevance Score]\n    H --&gt; I[Generate Detailed Reasoning]\n    I --&gt; J[Return Comprehensive Results]</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/","title":"StructureQuality","text":""},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#overview","title":"Overview","text":"<p>Evaluates the structural quality of text through comprehensive analysis of coherence, logical flow, and consistency. Part of the Text Quality metric category.</p> <pre><code>from indoxJudge.metrics import StructureQuality\n\n# Initialize with summary to evaluate\nstructure_check = StructureQuality(\n    summary=\"Your text here\"\n)\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#key_characteristics","title":"Key Characteristics","text":"Property Description Detection Scope Discourse coherence, logical flow, topic/temporal consistency Score Range 0.0 (poor structure) - 1.0 (excellent structure) Response Format Returns score with detailed verdicts and reasoning Dependencies Flexible language model integration"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#interpretation_guide","title":"Interpretation Guide","text":"Score Range Interpretation 0.0-0.2 Severely disorganized 0.2-0.4 Multiple structural issues 0.4-0.6 Acceptable with some inconsistency 0.6-0.8 Mostly coherent 0.8-1.0 Optimal structural organization"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#usage_example","title":"Usage Example","text":"<pre><code>from indoxJudge.metrics import StructureQuality\nfrom indoxJudge.pipelines import Evaluator\n\nsummary = \"Climate change is a global challenge. Rising temperatures affect ecosystems. Renewable energy offers solutions.\"\n\n# Initialize analyzer\nstructure_metric = StructureQuality(\n    summary=summary,\n    weights={\n        \"discourse_coherence\": 0.35,\n        \"logical_flow\": 0.25,\n        \"topic_consistency\": 0.25,\n        \"temporal_consistency\": 0.15\n    }\n)\n\n# Use in evaluation pipeline\nevaluator = Evaluator(\n    model=your_model,\n    metrics=[structure_metric]\n)\n\nresults = evaluator.judge()\n\n# Access comprehensive report\nprint(f\"\"\"\nStructure Score: {results['structure_quality']['score']:.2f}\nReasoning: {results['structure_quality']['reason']}\n\"\"\")\n</code></pre>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#configuration_options","title":"Configuration Options","text":"<p>| Parameter            | Effect                                                                                     | | -------------------- | ------------------------------------------------------------------------------------------ | --- | | weights              | Customize aspect importance (default: discourse=0.3, logical=0.3, topic=0.2, temporal=0.2) |     | | verbose_output=True  | Enable detailed aspect-specific explanations                                               | | logging_level=\"INFO\" | Set token usage and processing logging detail                                              |</p>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#best_practices","title":"Best Practices","text":"<ul> <li>Context Weighting: Adjust weights based on content type (narrative vs. technical)</li> <li>Model Selection: Choose appropriate language models for domain-specific analysis</li> <li>Multi-metric Analysis: Combine with Coherence and Clarity evaluations</li> <li>Iterative Refinement: Use verdict feedback to improve document structure</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#comparison_table","title":"Comparison Table","text":"Metric Focus Area Detection Method Output Granularity StructureQuality Overall structural integrity Multi-aspect analysis with LLM Score + Aspect verdicts Coherence Text flow Transition and reference analysis Coherence breakdown Organization Section arrangement Structural pattern detection Organizational suggestions"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#limitations","title":"Limitations","text":"<ul> <li>Content Specificity: Structure norms vary by document type</li> <li>Language Variation: Different discourse patterns across languages</li> <li>Model Dependence: Quality dependent on language model capabilities</li> <li>Genre Sensitivity: Different expectations for creative vs. technical text</li> </ul>"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#error_handling","title":"Error Handling","text":"Common Issues Recommended Action Model response parsing Activate fallback evaluation methods Invalid weight distribution Auto-normalize weight configurations Empty or minimal text Apply minimum content thresholds Complex nested structures Enable hierarchical analysis mode"},{"location":"indoxJudge/metrics/summary-metrics/StructureQuality/#flow_chart","title":"Flow Chart","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; EvaluateStructure: Input Summary\n\n    note right of EvaluateStructure\n        Analyzes summary across 4 key aspects:\n        - Discourse coherence\n        - Logical flow\n        - Topic consistency\n        - Temporal consistency\n    end note\n\n    EvaluateStructure --&gt; ScoreGeneration: Calculate Scores\n\n    state ScoreGeneration {\n        [*] --&gt; DiscourseCoherence\n        DiscourseCoherence --&gt; LogicalFlow\n        LogicalFlow --&gt; TopicConsistency\n        TopicConsistency --&gt; TemporalConsistency\n        TemporalConsistency --&gt; [*]\n    }\n\n    ScoreGeneration --&gt; FinalVerdictGeneration: Compile Scores\n\n    note right of FinalVerdictGeneration\n        Generates overall assessment\n        based on individual aspect scores\n    end note\n\n    FinalVerdictGeneration --&gt; JSONOutput: Return Results\n\n    JSONOutput --&gt; [*]</code></pre>"},{"location":"indoxJudge/overview/architecture/","title":"System Architecture","text":""},{"location":"indoxJudge/overview/architecture/#core_components","title":"Core Components","text":"<pre><code>graph TD\n    A[Input Text] --&gt; B(Pipeline)\n    B --&gt; C{Metrics}\n    C --&gt; D[Basic NLP]\n    C --&gt; E[Safety &amp; Ethics]\n    C --&gt; F[Bias &amp; Fairness]\n    C --&gt; G[Robustness]\n    C --&gt; H[Relevancy &amp; Context]\n    C --&gt; I[Quality &amp; Accuracy]\n    D --&gt; D1[BLEU/METEOR/Rouge/BertScore/Gruen/GEval]\n    E --&gt; E1[Toxicity/ToxicityDiscriminative/SafetyToxicity/Harmfulness/MachineEthics/Privacy]\n    F --&gt; F1[Bias/Fairness/StereotypeBias]\n    G --&gt; G1[AdversarialRobustness/OutOfDistributionRobustness/RobustnessToAdversarialDemonstrations]\n    H --&gt; H1[AnswerRelevancy/ContextualRelevancy/KnowledgeRetention/Faithfulness]\n    I --&gt; I1[Hallucination/Misinformation]\n    B --&gt; L[Analysis Tools]\n    L --&gt; M[EvaluationAnalyzer]\n    L --&gt; N[Visualization]</code></pre>"},{"location":"indoxJudge/overview/architecture/#component_breakdown","title":"Component Breakdown","text":""},{"location":"indoxJudge/overview/architecture/#1_evaluation_pipelines","title":"1. Evaluation Pipelines","text":"Component Responsibility Key Methods <code>Evaluator</code> Base evaluation workflow <code>judge()</code>, <code>validate()</code> <code>LLMEvaluator</code> General language model assessment <code>set_metrics()</code> <code>RagEvaluator</code> RAG system validation <code>check_retrieval()</code> <code>SafetyEvaluator</code> Content safety scanning <code>detect_harm()</code> <code>SummaryEvaluator</code> Summarization-specific checks <code>assess_coverage()</code> <code>EvaluationAnalyzer</code> Compare multiple LLMs <code>compare_models()</code>"},{"location":"indoxJudge/overview/architecture/#2_metric_categories","title":"2. Metric Categories","text":"Category Example Metrics Scoring Range Basic NLP BLEU, ROUGE, METEOR, BertScore, Gruen, GEval 0.0-1.0 Relevancy &amp; Context AnswerRelevancy, ContextualRelevancy, KnowledgeRetention, Faithfulness Binary + Score Safety &amp; Ethics Toxicity, ToxicityDiscriminative, SafetyToxicity, Harmfulness, MachineEthics, Privacy 0.0-1.0 Bias &amp; Fairness Bias, Fairness, StereotypeBias 0.0-1.0 Quality &amp; Accuracy Hallucination, Misinformation Multi-Dimensional Robustness AdversarialRobustness, OutOfDistributionRobustness, RobustnessToAdversarialDemonstrations 0.0-1.0 Summary-Specific SummaryBertScore, SummaryBleu, Conciseness, FactualConsistency, InformationCoverage Multi-Dimensional"},{"location":"indoxJudge/overview/architecture/#workflow_stages","title":"Workflow Stages","text":"<ol> <li> <p>Input Processing</p> </li> <li> <p>Text normalization</p> </li> <li>Context embedding</li> <li> <p>Dependency parsing</p> </li> <li> <p>Aggregated Analysis</p> </li> <li> <p>Cross-metric correlation</p> </li> <li>Confidence weighting</li> <li> <p>Threshold-based alerts</p> </li> <li> <p>Reporting</p> </li> <li> <p>Interactive visualizations</p> </li> <li>Exportable JSON/CSV</li> <li>Audit trails</li> </ol>"},{"location":"indoxJudge/overview/introduction/","title":"IndoxJudge Evaluation Framework","text":""},{"location":"indoxJudge/overview/introduction/#overview","title":"Overview","text":"<p>IndoxJudge is a comprehensive evaluation framework for language model applications, designed to systematically assess various aspects of AI-generated content through configurable pipelines and metrics.</p>"},{"location":"indoxJudge/overview/introduction/#key_capabilities","title":"Key Capabilities","text":"<ul> <li> <p>Multi-Dimensional Evaluation: Analyze outputs across 6 core dimensions:</p> </li> <li> <p>Basic NLP Quality</p> </li> <li>Contextual Relevance</li> <li>Safety &amp; Ethics</li> <li>Bias &amp; Fairness</li> <li>Informational Accuracy</li> <li> <p>Robustness</p> </li> <li> <p>Pipeline-Driven Architecture: Preconfigured evaluation workflows:</p> </li> </ul> <pre><code>__all__ = [\n    \"Evaluator\",          # Base Evaluator that takes custom list of metrics\n    \"LLMEvaluator\",       # General language model evaluation\n    \"RagEvaluator\",       # Retrieval-Augmented Generation systems\n    \"SafetyEvaluator\",    # Content safety analysis\n    \"SummaryEvaluator\",   # Summarization-specific checks\n    \"EvaluationAnalyzer\"  # Compare multiple LLMs\n]\n</code></pre> <ul> <li>Extensive Metric Library: 40+ specialized metrics across categories:</li> </ul> <pre><code># Basic NLP Metrics\n\"BLEU\", \"METEOR\", \"Rouge\", \"BertScore\", \"Gruen\", \"GEval\"\n\n# Relevancy and Context Metrics\n\"AnswerRelevancy\", \"ContextualRelevancy\", \"KnowledgeRetention\", \"Faithfulness\"\n\n# Safety and Ethics Metrics\n\"Toxicity\", \"ToxicityDiscriminative\", \"SafetyToxicity\", \"Harmfulness\",\n\"MachineEthics\", \"Privacy\"\n\n# Bias and Fairness Metrics\n\"Bias\", \"Fairness\", \"StereotypeBias\"\n\n# Quality and Accuracy Metrics\n\"Hallucination\", \"Misinformation\"\n\n# Robustness Metrics\n\"AdversarialRobustness\", \"OutOfDistributionRobustness\",\n\"RobustnessToAdversarialDemonstrations\"\n\n# Summary-Specific Metrics\n\"SummaryBertScore\", \"SummaryBleu\", \"Conciseness\", \"FactualConsistency\",\n\"SummaryGEval\", \"InformationCoverage\", \"SummaryMeteor\", \"Relevance\",\n\"SummaryRouge\", \"StructureQuality\", \"SummaryToxicity\"\n</code></pre>"},{"location":"indoxJudge/overview/introduction/#why_indoxjudge","title":"Why IndoxJudge?","text":"<ul> <li>Holistic Assessment: Combine lexical, semantic, and ethical evaluations</li> <li>Modular Design: Mix-and-match metrics for custom evaluation workflows</li> <li>Actionable Insights: Detailed scoring with explanatory feedback</li> <li>Enterprise-Ready: Scalable architecture for batch processing</li> </ul>"},{"location":"indoxJudge/overview/introduction/#getting_started","title":"Getting Started","text":"<pre><code>from indoxJudge.pipelines import Evaluator\nfrom indoxJudge.metrics import Toxicity, AnswerRelevancy\n\nevaluator = LLMEvaluator(\n    model=your_llm,\n    metrics=[Toxicity(), AnswerRelevancy()]\n)\n\nresults = evaluator.judge(\"Sample LLM output\")\n</code></pre>"},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/","title":"LLMComparison","text":""},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/#overview","title":"Overview","text":"<p>The <code>LLMComparison</code> class is designed to facilitate the comparison of multiple language models based on their evaluation metrics. This class allows for visual representation of the performance of different models, making it easier to analyze and compare their strengths and weaknesses.</p>"},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/#initialization","title":"Initialization","text":"<p>The <code>LLMComparison</code> class is initialized with a list of models, where each model is represented by a dictionary containing its name, overall score, and various evaluation metrics.</p>"},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/#example","title":"Example","text":"<pre><code>class LLMComparison:\n    def __init__(self, models):\n        \"\"\"\n        Initializes the LLMComparison with a list of models.\n\n        Args:\n            models (list): A list of dictionaries where each dictionary contains\n                           the model's name, score, and metrics.\n        \"\"\"\n</code></pre>"},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/#plotting_the_comparison","title":"Plotting the Comparison","text":"<p>The plot method generates a visual representation of the comparison between the models. It uses an external visualization library to create graphs that illustrate the performance of each model across various metrics. The mode parameter allows for different modes of visualization.</p>"},{"location":"indoxJudge/pipelines/EvaluationAnalyzer/#usage_example","title":"Usage Example","text":"<p>Below is an example of how to use the LLMComparison class to compare different language models and generate a plot.</p> <pre><code>from indoxJudge.pipelines import EvaluationAnalyzer\n\nmodels = [\n    {\n        'name': 'Model_1',\n        'score': 0.50,\n        'metrics': {\n            'Faithfulness': 0.55,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.45,\n            'Hallucination': 0.8,\n            'KnowledgeRetention': 0.0,\n            'Toxicity': 0.0,\n            'precision': 0.64,\n            'recall': 0.77,\n            'f1_score': 0.70,\n            'BLEU': 0.11\n        }\n    },\n    {\n        'name': 'Model_2',\n        'score': 0.61,\n        'metrics': {\n            'Faithfulness': 1.0,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.0,\n            'Hallucination': 0.8,\n            'KnowledgeRetention': 1.0,\n            'Toxicity': 0.0,\n            'precision': 0.667,\n            'recall': 0.77,\n            'f1_score': 0.71,\n            'BLEU': 0.14\n        }\n    },\n    {\n        'name': 'Model_3',\n        'score': 0.050,\n        'metrics': {\n            'Faithfulness': 1.0,\n            'AnswerRelevancy': 1.0,\n            'Bias': 0.0,\n            'Hallucination': 0.83,\n            'KnowledgeRetention': 0.0,\n            'Toxicity': 0.0,\n            'precision': 0.64,\n            'recall': 0.76,\n            'f1_score': 0.70,\n            'BLEU': 0.10\n        }\n    }\n]\n\nllm_comparison = EvaluationAnalyzer(models=models)\nllm_comparison.plot(mode=\"inline\")\n</code></pre> <p>In this example, three models are compared based on their metrics, and the results are plotted inline. The LLMComparison class facilitates the easy comparison and visualization of different language models' performances.</p>"},{"location":"indoxJudge/pipelines/Evaluator/","title":"Evaluator","text":""},{"location":"indoxJudge/pipelines/Evaluator/#overview","title":"Overview","text":"<p>The <code>Evaluator</code> class provides a comprehensive framework for evaluating language model outputs across multiple dimensions. It supports a wide range of metrics including Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, Hallucination, Knowledge Retention, Toxicity, and various text quality metrics like BertScore, BLEU, and METEOR. This class is essential for conducting thorough assessments of language model performance.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#initialization","title":"Initialization","text":"<p>The <code>Evaluator</code> class is initialized with the following components:</p> <ul> <li>Model: The language model to be evaluated.</li> <li>Metrics: A list of metric instances to evaluate the model.</li> </ul> <pre><code>class Evaluator:\n    def __init__(self, model, metrics: List):\n        \"\"\"\n        Initializes the Evaluator with a language model and a list of metrics.\n\n        Args:\n            model: The language model to be evaluated.\n            metrics (List): A list of metric instances to evaluate the model.\n        \"\"\"\n        self.model = model\n        self.metrics = metrics\n        logger.info(\"Evaluator initialized with model and metrics.\")\n        self.set_model_for_metrics()\n        self.evaluation_score = 0\n        self.metrics_score = {}\n        self.results = {}\n</code></pre>"},{"location":"indoxJudge/pipelines/Evaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that each metric has access to the language model for evaluation.</p> <pre><code>def set_model_for_metrics(self):\n    \"\"\"\n    Sets the language model for each metric that requires it.\n    \"\"\"\n    for metric in self.metrics:\n        if hasattr(metric, \"set_model\"):\n            metric.set_model(self.model)\n    logger.info(\"Model set for all metrics.\")\n</code></pre>"},{"location":"indoxJudge/pipelines/Evaluator/#evaluation_process","title":"Evaluation Process","text":"<p>The <code>judge</code> method performs the evaluation using all provided metrics and returns comprehensive results. It processes each metric individually, handling their specific evaluation requirements and aggregating the results.</p> <pre><code>def judge(self):\n    \"\"\"\n    Evaluates the language model using the provided metrics and returns the results.\n\n    Returns:\n        dict: A dictionary containing the evaluation results for each metric.\n    \"\"\"\n</code></pre>"},{"location":"indoxJudge/pipelines/Evaluator/#supported_metrics","title":"Supported Metrics","text":"<p>The Evaluator supports numerous metrics, each with specific evaluation approaches:</p>"},{"location":"indoxJudge/pipelines/Evaluator/#faithfulness","title":"Faithfulness","text":"<p>Evaluates factual accuracy by analyzing claims, truths, and providing verdicts with reasoning.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#answer_relevancy","title":"Answer Relevancy","text":"<p>Measures how relevant the model's responses are to the given queries.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#knowledge_retention","title":"Knowledge Retention","text":"<p>Assesses how well the model retains and applies information provided in context.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#hallucination","title":"Hallucination","text":"<p>Identifies when the model generates information not supported by the context.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#toxicity","title":"Toxicity","text":"<p>Detects harmful or offensive content in the model's outputs.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#bias","title":"Bias","text":"<p>Identifies various types of bias in the model's responses.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#quality_metrics","title":"Quality Metrics","text":"<p>Includes standard NLP evaluation metrics like BertScore, BLEU, and METEOR to assess text quality.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#safety_metrics","title":"Safety Metrics","text":"<p>Includes Fairness, Harmfulness, Privacy, and other safety-oriented evaluations.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#robustness_metrics","title":"Robustness Metrics","text":"<p>Evaluates model performance under adversarial conditions or with out-of-distribution inputs.</p>"},{"location":"indoxJudge/pipelines/Evaluator/#example_result_structure","title":"Example Result Structure","text":"<p>The results from the evaluation are structured as a dictionary:</p> <pre><code>{\n    \"Faithfulness\": {\n        \"claims\": [...],\n        \"truths\": [...],\n        \"verdicts\": [...],\n        \"score\": 0.85,\n        \"reason\": \"...\"\n    },\n    \"AnswerRelevancy\": {\n        \"score\": 0.92,\n        \"reason\": \"...\",\n        \"statements\": [...],\n        \"verdicts\": [...]\n    },\n    # Other metrics follow similar patterns\n}\n</code></pre>"},{"location":"indoxJudge/pipelines/Evaluator/#visualization","title":"Visualization","text":"<p>The Evaluator provides built-in visualization capabilities through the <code>plot</code> method:</p> <pre><code>def plot(self, mode=\"external\"):\n    \"\"\"\n    Visualizes the evaluation results.\n\n    Args:\n        mode (str): Visualization mode, either \"external\" or \"inline\".\n\n    Returns:\n        Visualization object: The visualization of evaluation results.\n    \"\"\"\n</code></pre> <p>This method creates a visual representation of the evaluation results, making it easier to interpret model performance across different metrics.</p>"},{"location":"indoxJudge/pipelines/LLMEvaluator/","title":"LLMEvaluator","text":""},{"location":"indoxJudge/pipelines/LLMEvaluator/#overview","title":"Overview","text":"<p>The <code>LLMEvaluator</code> class is designed to evaluate various aspects of language model outputs using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Bias, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, Toxicity, BertScore, BLEU, Rouge, and METEOR. This class provides a comprehensive assessment of different dimensions of model performance, making it ideal for thorough evaluations of language models.</p>"},{"location":"indoxJudge/pipelines/LLMEvaluator/#initialization","title":"Initialization","text":"<p>The <code>LLMEvaluator</code> class is initialized with four main components:</p> <ul> <li>llm_as_judge: The language model acting as the judge for the evaluation.</li> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context used for retrieval-based metrics.</li> <li>query: The query or prompt provided to the language model.</li> </ul>"},{"location":"indoxJudge/pipelines/LLMEvaluator/#example","title":"Example","text":"<pre><code>class LLMEvaluator:\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\n        \"\"\"\n        Initializes the Evaluator with a language model and a list of metrics.\n\n        Args:\n            llm_as_judge: The language model acting as the judge for the evaluation.\n            llm_response: The response generated by the language model.\n            retrieval_context: The context used for retrieval-based metrics.\n            query: The query or prompt provided to the language model.\n        \"\"\"\n        self.model = llm_as_judge\n        self.metrics = [\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\n            AnswerRelevancy(query=query, llm_response=llm_response),\n            Bias(llm_response=llm_response),\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            Toxicity(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\n            BLEU(llm_response=llm_response, retrieval_context=retrieval_context),\n        ]\n</code></pre>"},{"location":"indoxJudge/pipelines/LLMEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.</p>"},{"location":"indoxJudge/pipelines/LLMEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom indoxJudge.pipelines import LLMEvaluator\nfrom indoxJudge.models import OpenAi\n\nquery = \"What are the benefits of a Mediterranean diet?\"\nretrieval_context = [\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods, such as fruits and vegetables, whole grains, legumes, and nuts. It also includes moderate amounts of fish and poultry, and low consumption of red meat. Olive oil is the main source of fat, providing monounsaturated fats which are beneficial for heart health.\",\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease, stroke, and type 2 diabetes. It is also associated with improved cognitive function and a lower risk of Alzheimer's disease. The diet's high content of fiber, antioxidants, and healthy fats contributes to its numerous health benefits.\",\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases. It promotes healthy aging and weight management due to its emphasis on whole, unprocessed foods and balanced nutrition.\"\n]\n\n# Obtain the model's response\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease, stroke, and diabetes. It encourages the consumption of fruits, vegetables, whole grains, nuts, and olive oil, while limiting red meat. Additionally, this diet has been associated with better cognitive function and a reduced risk of Alzheimer's disease, promoting longevity and overall well-being.\"\n\n\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o\")\n\nevaluator = LLMEvaluator(llm_as_judge=llm_as_judge,llm_response=response,retrieval_context=retrieval_context,query=query)\nllm_results = evaluator.judge()\n\nevaluation_score = evaluator.evaluation_score\nevaluation_metrics_score = evaluator.metrics_score\n\n# For plot and visualize (choose inline for using in colab)\nevaluator.plot(mode=\"external\")\n</code></pre> <p>In this example, the LLMEvaluator class method judge initializes an empty dictionary called results. It iterates through each metric in the metrics list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. For instance, the Faithfulness metric involves evaluating claims, truths, and verdicts, which are then stored in the results dictionary under the 'faithfulness' key.</p> <p>If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.</p>"},{"location":"indoxJudge/pipelines/RagEvaluator/","title":"RagEvaluator","text":""},{"location":"indoxJudge/pipelines/RagEvaluator/#overview","title":"Overview","text":"<p>The <code>RagEvaluator</code> class is designed to evaluate various aspects of language model outputs in the context of Retrieval-Augmented Generation (RAG) using specified metrics. It supports metrics such as Faithfulness, Answer Relevancy, Contextual Relevancy, GEval, Hallucination, Knowledge Retention, BertScore, and METEOR. This class provides a comprehensive assessment of different dimensions of RAG model performance, making it ideal for thorough evaluations of retrieval-based language models.</p>"},{"location":"indoxJudge/pipelines/RagEvaluator/#initialization","title":"Initialization","text":"<p>The <code>RagEvaluator</code> class is initialized with four main components:</p> <ul> <li>llm_as_judge: The language model acting as the judge for the evaluation.</li> <li>llm_response: The response generated by the language model.</li> <li>retrieval_context: The context retrieved for the query.</li> <li>query: The query or prompt provided to the language model.</li> </ul>"},{"location":"indoxJudge/pipelines/RagEvaluator/#example","title":"Example","text":"<pre><code>class RagEvaluator:\n    def __init__(self, llm_as_judge, llm_response, retrieval_context, query):\n        \"\"\"\n        Initializes the RagEvaluator with a language model and a list of metrics.\n\n        Args:\n            llm_as_judge: The language model acting as the judge for the evaluation.\n            llm_response: The response generated by the language model.\n            retrieval_context: The context retrieved for the query.\n            query: The query or prompt provided to the language model.\n        \"\"\"\n        self.metrics = [\n            Faithfulness(llm_response=llm_response, retrieval_context=retrieval_context),\n            AnswerRelevancy(query=query, llm_response=llm_response),\n            ContextualRelevancy(query=query, retrieval_context=retrieval_context),\n            GEval(parameters=\"Rag Pipeline\", llm_response=llm_response, query=query, retrieval_context=retrieval_context),\n            Hallucination(llm_response=llm_response, retrieval_context=retrieval_context),\n            KnowledgeRetention(messages=[{\"query\": query, \"llm_response\": llm_response}]),\n            BertScore(llm_response=llm_response, retrieval_context=retrieval_context),\n            METEOR(llm_response=llm_response, retrieval_context=retrieval_context),\n        ]\n</code></pre>"},{"location":"indoxJudge/pipelines/RagEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the language model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model for evaluation purposes.</p>"},{"location":"indoxJudge/pipelines/RagEvaluator/#judging","title":"Judging","text":"<p>The <code>judge</code> method evaluates the language model using the provided metrics and returns the results. It processes each metric individually, handling specific evaluation logic for different metric types.</p>"},{"location":"indoxJudge/pipelines/RagEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\nfrom indoxJudge.pipelines import RagEvaluator\nfrom indoxJudge.models import OpenAi\n\nquery = \"What are the benefits of a Mediterranean diet?\"\nretrieval_context = [\n    \"The Mediterranean diet emphasizes eating primarily plant-based foods...\",\n    \"Research has shown that the Mediterranean diet can reduce the risk of heart disease...\",\n    \"A Mediterranean diet has been linked to a longer lifespan and a reduced risk of chronic diseases...\"\n]\n\nresponse = \"The Mediterranean diet is known for its health benefits, including reducing the risk of heart disease...\"\n\nllm_as_judge = OpenAi(api_key=OPENAI_API_KEY, model=\"gpt-4o\")\n\nevaluator = RagEvaluator(llm_as_judge=llm_as_judge, llm_response=response, retrieval_context=retrieval_context, query=query)\nrag_results = evaluator.judge()\n\nevaluation_score = evaluator.evaluation_score\nevaluation_metrics_score = evaluator.metrics_score\n\n# For plot and visualize (choose inline for using in colab)\nevaluator.plot(mode=\"external\")\n</code></pre> <p>In this example, the <code>RagEvaluator</code> class method <code>judge</code> initializes an empty dictionary called <code>results</code>. It iterates through each metric in the <code>metrics</code> list, retrieves the metric's name, and logs the start of the evaluation process. For each metric, the method checks its type and calls the appropriate evaluation method. The results are stored in the <code>results</code> dictionary under the corresponding metric key. If any exceptions are encountered during the evaluation process, they are caught and logged as errors, and the evaluation continues with the next metric. The final results are returned as a dictionary containing the outcomes of the evaluations for each metric.</p>"},{"location":"indoxJudge/pipelines/RagEvaluator/#visualization","title":"Visualization","text":"<p>The <code>plot</code> method allows for visualization of the evaluation results. It creates a graph representation of the RAG Evaluator's performance across different metrics.</p>"},{"location":"indoxJudge/pipelines/SafetyEvaluator/","title":"Safety Evaluator","text":""},{"location":"indoxJudge/pipelines/SafetyEvaluator/#safetyevaluator","title":"SafetyEvaluator","text":""},{"location":"indoxJudge/pipelines/SafetyEvaluator/#overview","title":"Overview","text":"<p>The <code>SafetyEvaluator</code> class is designed to assess various safety-related aspects of a given input using a set of predefined metrics. This class includes metrics such as Fairness, Harmfulness, Privacy, Misinformation, MachineEthics, and StereotypeBias. It provides a structured approach to evaluating these dimensions and visualizing the results, making it suitable for comprehensive safety evaluations of text inputs.</p>"},{"location":"indoxJudge/pipelines/SafetyEvaluator/#initialization","title":"Initialization","text":"<p>The <code>SafetyEvaluator</code> class is initialized with two main components:</p> <ul> <li>model: The model used for evaluating the safety metrics.</li> <li>input: The input text or sentence that will be evaluated against the safety metrics.</li> </ul>"},{"location":"indoxJudge/pipelines/SafetyEvaluator/#example","title":"Example","text":"<pre><code>class SafetyEvaluator:\n    def __init__(self, model, input):\n        \"\"\"\n        Initializes the SafetyEvaluator with a model and input sentence.\n\n        Args:\n            model: The model used for evaluating the metrics.\n            input: The input text that will be evaluated.\n        \"\"\"\n</code></pre>"},{"location":"indoxJudge/pipelines/SafetyEvaluator/#setting_the_model_for_metrics","title":"Setting the Model for Metrics","text":"<p>The <code>set_model_for_metrics</code> method ensures that the model is properly set for each metric that requires it. This step is crucial for metrics that need direct access to the model to perform their evaluations.</p>"},{"location":"indoxJudge/pipelines/SafetyEvaluator/#usage_example","title":"Usage Example","text":"<pre><code>import os\nfrom dotenv import load_dotenv\n\nfrom indoxJudge.pipelines import SafetyEvaluator\nfrom indoxJudge.models import OpenAi\n\nmodel = OpenAi(api_key=OPENAI_API_KEY,model=\"gpt-4o-mini\")\n\ninput_sentence = \"Most people believe that teamwork is essential for successful project completion, although some individuals may work better independently.\"\n\nevaluator = SafetyEvaluator(model=model, input=input_sentence)\n\n\neval_result = evaluator.judge()\n\nprint(evaluator.metrics_score)\n\n\n\nevaluator.plot(mode=\"external\",interpreter=model)\n</code></pre>"},{"location":"indoxMiner/classification/Classification_module/","title":"Overview","text":""},{"location":"indoxMiner/classification/Classification_module/#classification_module","title":"Classification Module","text":"<p>The classification module in <code>IndoxMiner</code> provides a robust framework for image classification, leveraging advanced deep learning models to deliver high accuracy and flexibility. Designed for scalability and ease of use, it supports both single-image and batch processing, custom label definitions, and intuitive result visualizations.</p>"},{"location":"indoxMiner/classification/Classification_module/#key_features","title":"\ud83c\udf1f Key Features","text":"<ul> <li>Multi-Model Support: Includes a variety of cutting-edge models, such as:</li> <li>SigCLIP: A semantic image classification model.</li> <li>ViT: Vision Transformer for general image classification tasks.</li> <li>MetaCLIP: Meta AI's enhanced CLIP for diverse applications.</li> <li>MobileCLIP: Lightweight CLIP optimized for mobile devices.</li> <li>BioCLIP: Designed for biological image analysis.</li> <li>AltCLIP: A powerful CLIP model alternative from BAAI.</li> <li> <p>RemoteCLIP: Specializes in remote sensing and satellite imagery classification.</p> </li> <li> <p>Custom Labels: Allows users to define custom label sets for tailored classification tasks.</p> </li> <li> <p>Batch Processing: Efficiently handles multiple images in a single call, enabling scalability for large datasets.</p> </li> <li> <p>Visualization: Automatically generates bar plots of predicted probabilities, providing an intuitive understanding of classification results.</p> </li> <li> <p>Flexible Integration: Seamlessly integrates into workflows requiring classification alongside data extraction or object detection.</p> </li> </ul>"},{"location":"indoxMiner/classification/Classification_module/#supported_models","title":"\ud83d\udcd6 Supported Models","text":"Model Description SigCLIP Semantic image classification model. ViT Vision Transformer for image classification. MetaCLIP Meta AI's advanced CLIP model. MobileCLIP Mobile-optimized CLIP. BioCLIP Specialized for biological images. AltCLIP Alternative CLIP from BAAI. RemoteCLIP Remote sensing-specific CLIP model. <p>Each model has unique strengths, enabling the classification module to cater to diverse use cases, from general-purpose image recognition to domain-specific tasks like biological image analysis or remote sensing.</p>"},{"location":"indoxMiner/classification/Classification_module/#quick_start","title":"\ud83d\ude80 Quick Start","text":"<p>Here's how you can get started with the SigCLIP model for image classification:</p>"},{"location":"indoxMiner/classification/Classification_module/#single_image_classification","title":"Single Image Classification","text":"<pre><code>from indoxminer.classification import SigCLIPClassifier\nfrom PIL import Image\n\n# Initialize SigCLIP classifier\nclassifier = SigCLIPClassifier()\n\n# Load an image\nimage = Image.open(\"/path/to/image.jpg\")\n\n# Classify the image with default labels\nclassifier.classify(image)\n</code></pre>"},{"location":"indoxMiner/classification/Classification_module/#batch_image_classification","title":"Batch Image Classification","text":"<pre><code># Load multiple images\nimages = [Image.open(\"/path/to/image1.jpg\"), Image.open(\"/path/to/image2.jpg\")]\n\n# Classify the batch of images\nclassifier.classify(images)\n</code></pre>"},{"location":"indoxMiner/classification/Classification_module/#custom_labels","title":"Custom Labels","text":"<pre><code># Define custom labels\nlabels = [\"a cat\", \"a dog\", \"a bird\"]\n\n# Classify the image with custom labels\nclassifier.classify(image, labels=labels)\n</code></pre>"},{"location":"indoxMiner/classification/Classification_module/#advanced_features","title":"\ud83d\udd0d Advanced Features","text":""},{"location":"indoxMiner/classification/Classification_module/#customizable_workflows","title":"Customizable Workflows","text":"<p>The module allows users to directly interact with its components (<code>preprocess</code>, <code>predict</code>, <code>visualize</code>) for complete control over the classification pipeline.</p> <pre><code># Preprocess the image and labels\ninputs = classifier.preprocess(image, labels)\n\n# Predict probabilities\nprobs = classifier.predict(inputs)\n\n# Visualize the results\nclassifier.visualize(image, labels, probs, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/Classification_module/#model-specific_capabilities","title":"Model-Specific Capabilities","text":"<p>Each classifier is optimized for its respective model, ensuring that unique features like specialized tokenization or domain-specific fine-tuning are fully utilized.</p>"},{"location":"indoxMiner/classification/Classification_module/#example_use_cases","title":"\ud83c\udf10 Example Use Cases","text":"<ol> <li>E-commerce: Automatically classify product images into predefined categories.</li> <li>Healthcare: Analyze biological images with BioCLIP for medical research.</li> <li>Remote Sensing: Identify features in satellite images using RemoteCLIP.</li> <li>Content Moderation: Classify images for compliance and moderation workflows.</li> <li>Mobile Applications: Leverage MobileCLIP for lightweight classification on mobile devices.</li> </ol>"},{"location":"indoxMiner/classification/Classification_module/#visualization","title":"\ud83d\udcca Visualization","text":"<p>The <code>classify</code> method generates bar plots that display the top predictions along with their probabilities. This feature aids in understanding the model's confidence and results.</p>"},{"location":"indoxMiner/classification/Classification_module/#model-specific_documentation","title":"\ud83d\udee0\ufe0f Model-Specific Documentation","text":"<p>Refer to the individual documentation pages for detailed information about each classifier:</p> <ul> <li>SigCLIPClassifier Documentation</li> <li>ViTClassifier Documentation</li> <li>MetaCLIPClassifier Documentation</li> <li>MobileCLIP</li> <li>BioCLIPClassifier Documentation</li> <li>AltCLIPClassifier Documentation</li> <li>RemoteCLIP</li> </ul>"},{"location":"indoxMiner/classification/Classification_module/#why_use_the_classification_module","title":"\ud83c\udf1f Why Use the Classification Module?","text":"<p>The classification module in <code>IndoxMiner</code></p>"},{"location":"indoxMiner/classification/MobileCLIP/","title":"MobileCLIP","text":""},{"location":"indoxMiner/classification/MobileCLIP/#mobileclipclassifier_documentation","title":"MobileCLIPClassifier Documentation","text":"<p>The <code>MobileCLIPClassifier</code> provides a lightweight and efficient solution for image classification, optimized for mobile environments. Built on MobileCLIP, it supports flexible classification tasks with batch processing and custom labels.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#installation","title":"\ud83d\udd27 Installation","text":"<p>Before using the <code>MobileCLIPClassifier</code>, ensure you have the required dependencies installed:</p>"},{"location":"indoxMiner/classification/MobileCLIP/#install_mobileclip","title":"Install MobileCLIP","text":"<pre><code>pip install git+https://github.com/apple/ml-mobileclip.git --no-deps\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#install_pytorch_and_openclip","title":"Install PyTorch and OpenCLIP","text":"<pre><code>pip install torch==2.1.0 torchvision==0.16.0\npip install open-clip-torch\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#downloading_pretrained_checkpoints","title":"\ud83d\udce5 Downloading Pretrained Checkpoints","text":"<p>You can download the pretrained MobileCLIP checkpoint from its official repository or another hosting location. Ensure the checkpoint is accessible at a known path.</p> <p>For example: - Download and save the <code>mobileclip_s0.pt</code> checkpoint to <code>/content/mobileclip_s0.pt</code>.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#using_mobileclipclassifier","title":"\ud83e\udde0 Using MobileCLIPClassifier","text":"<p>The <code>MobileCLIPClassifier</code> in the <code>IndoxMiner</code> library streamlines classification tasks by providing a simple API for single and batch image processing.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#initialization","title":"Initialization","text":"<pre><code>from indoxminer.classification import MobileCLIPClassifier\n\n# Initialize the classifier with the pretrained checkpoint\nclassifier = MobileCLIPClassifier(pretrained_path=\"/content/mobileclip_s0.pt\")\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"mobileclip_s0\"</code> Name of the MobileCLIP model variant. <code>pretrained_path</code> <code>str</code> <code>None</code> Path to the pretrained weights file."},{"location":"indoxMiner/classification/MobileCLIP/#usage","title":"\ud83d\ude80 Usage","text":""},{"location":"indoxMiner/classification/MobileCLIP/#single_image_classification","title":"Single Image Classification","text":"<pre><code>from PIL import Image\n\n# Load an image\nimage = Image.open(\"/path/to/image.jpg\")\n\n# Classify the image\nclassifier.classify(image, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#batch_image_classification","title":"Batch Image Classification","text":"<pre><code>from pathlib import Path\n\n# Load multiple images\nimage_paths = [Path(\"/path/to/image1.jpg\"), Path(\"/path/to/image2.jpg\")]\nimages = [Image.open(path) for path in image_paths]\n\n# Classify the batch of images\nclassifier.classify(images, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#custom_labels","title":"Custom Labels","text":"<pre><code>custom_labels = [\"a tiger\", \"a mountain\", \"a river\", \"a boat\", \"a forest\"]\nclassifier.classify(images, labels=custom_labels, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#visualization","title":"\ud83d\udd0d Visualization","text":"<p>The <code>classify</code> method generates visualizations for each image, including: 1. Input Image: Displays the input image. 2. Bar Chart: Shows the top predictions and their probabilities.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#advanced_usage","title":"\ud83d\udd2c Advanced Usage","text":"<p>For more control, you can directly use the classifier's methods:</p>"},{"location":"indoxMiner/classification/MobileCLIP/#preprocessing","title":"Preprocessing","text":"<pre><code>inputs = classifier.preprocess(images, labels=[\"a cat\", \"a dog\"])\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#prediction","title":"Prediction","text":"<pre><code>probs = classifier.predict(inputs)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#visualization_1","title":"Visualization","text":"<pre><code>classifier.visualize(images, labels, probs, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#example_workflow","title":"\ud83c\udf10 Example Workflow","text":"<pre><code>from pathlib import Path\nfrom PIL import Image\nfrom indoxminer.classification import MobileCLIPClassifier\n\n# Step 1: Load images\nimage_paths = [Path(\"/path/to/image1.jpg\"), Path(\"/path/to/image2.jpg\")]\nimages = [Image.open(path) for path in image_paths]\n\n# Step 2: Initialize the classifier\nclassifier = MobileCLIPClassifier(pretrained_path=\"/path/to/mobileclip_s0.pt\")\n\n# Step 3: Classify the images with default labels\nclassifier.classify(images, top=5)\n\n# Step 4: Classify the images with custom labels\ncustom_labels = [\"a forest\", \"a river\", \"a mountain\"]\nclassifier.classify(images, labels=custom_labels, top=5)\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#supported_features","title":"\ud83d\udd27 Supported Features","text":"<ul> <li>Custom Labels: Define specific categories for image classification.</li> <li>Batch Processing: Efficiently classify multiple images in a single call.</li> <li>Visualization: Automatically generates bar plots for results.</li> </ul>"},{"location":"indoxMiner/classification/MobileCLIP/#troubleshooting","title":"\ud83d\udee0\ufe0f Troubleshooting","text":""},{"location":"indoxMiner/classification/MobileCLIP/#1_missing_checkpoint","title":"1. Missing Checkpoint","text":"<ul> <li>Ensure the <code>pretrained_path</code> is correct and accessible.</li> </ul>"},{"location":"indoxMiner/classification/MobileCLIP/#2_cuda_not_available","title":"2. CUDA Not Available","text":"<ul> <li>Verify that your system supports CUDA and PyTorch is installed with CUDA capabilities.</li> </ul>"},{"location":"indoxMiner/classification/MobileCLIP/#3_dependency_errors","title":"3. Dependency Errors","text":"<ul> <li>Ensure all required dependencies (<code>torch</code>, <code>open-clip-torch</code>, and <code>mobileclip</code>) are installed.</li> </ul>"},{"location":"indoxMiner/classification/MobileCLIP/#summary","title":"\ud83c\udf1f Summary","text":"<p>The <code>MobileCLIPClassifier</code> provides a powerful, lightweight solution for mobile-friendly image classification. Its features include: - Support for custom labels and batch processing. - Clear and intuitive visualizations. - Seamless integration with <code>IndoxMiner</code> for broader use cases.</p> <p>Explore the full documentation for further details.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#mobileclip","title":"MobileCLIP","text":"<p>MobileCLIP is a lightweight version of the CLIP model, optimized for mobile and edge devices while maintaining strong performance on vision-language tasks.</p>"},{"location":"indoxMiner/classification/MobileCLIP/#features","title":"Features","text":"<ul> <li>Efficient architecture for mobile devices</li> <li>Multi-modal understanding (images and text)</li> <li>Low latency inference</li> <li>Reduced memory footprint</li> </ul>"},{"location":"indoxMiner/classification/MobileCLIP/#usage_1","title":"Usage","text":"<pre><code>from indoxMiner.classification import MobileCLIPClassifier\n\n# Initialize the classifier\nclassifier = MobileCLIPClassifier(\n    model_name=\"apple/mobileclip-base\",\n    device=\"cpu\"\n)\n\n# Classify with text prompts\nresult = classifier.classify_with_text(\n    image_path=\"path/to/image.jpg\",\n    text_prompts=[\"a dog\", \"a cat\", \"a bird\"]\n)\n\nprint(f\"Best match: {result.label} with confidence {result.confidence}\")\n</code></pre>"},{"location":"indoxMiner/classification/MobileCLIP/#configuration","title":"Configuration","text":"<ul> <li><code>model_name</code>: Model variant to use</li> <li><code>device</code>: Computing device</li> <li><code>batch_size</code>: Number of images to process at once</li> <li><code>cache_dir</code>: Directory for model caching</li> </ul>"},{"location":"indoxMiner/classification/RemoteCLIP/","title":"RemoteCLIP","text":""},{"location":"indoxMiner/classification/RemoteCLIP/#remoteclipclassifier_documentation","title":"RemoteCLIPClassifier Documentation","text":"<p>The <code>RemoteCLIPClassifier</code> is a specialized classifier based on the CLIP framework, tailored for remote sensing and general image classification tasks. It supports various model variants, including <code>RN50</code>, <code>ViT-B-32</code>, and <code>ViT-L-14</code>.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#installation","title":"\ud83d\udd27 Installation","text":"<p>Before using the <code>RemoteCLIPClassifier</code>, ensure you have the <code>open-clip-torch</code> library installed. This library provides the required framework for working with RemoteCLIP models.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#install_openclip","title":"Install OpenCLIP","text":"<pre><code>pip install open-clip-torch\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#downloading_pretrained_checkpoints","title":"\ud83d\udce5 Downloading Pretrained Checkpoints","text":"<p>The pretrained checkpoints for RemoteCLIP are available on Hugging Face. You can download them programmatically using the <code>huggingface_hub</code> library:</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#example_download_checkpoints","title":"Example: Download Checkpoints","text":"<pre><code>from huggingface_hub import hf_hub_download\n\n# Download checkpoints for all supported models\nfor model_name in ['RN50', 'ViT-B-32', 'ViT-L-14']:\n    checkpoint_path = hf_hub_download(\"chendelong/RemoteCLIP\", f\"RemoteCLIP-{model_name}.pt\", cache_dir='checkpoints')\n    print(f'{model_name} is downloaded to {checkpoint_path}.')\n</code></pre> <p>Alternatively, you can clone the repository using Git LFS to manually download the files.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#using_remoteclipclassifier","title":"\ud83e\udde0 Using RemoteCLIPClassifier","text":"<p>The <code>RemoteCLIPClassifier</code> in the <code>IndoxMiner</code> library simplifies the process of using RemoteCLIP models for image classification.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#initialization","title":"Initialization","text":"<pre><code>from indoxminer.classification import RemoteCLIPClassifier\n\n# Initialize the classifier with a specific model and checkpoint\nmodel_name = \"ViT-L-14\"  # Options: 'RN50', 'ViT-B-32', 'ViT-L-14'\ncheckpoint_path = \"/path/to/RemoteCLIP-ViT-L-14.pt\"\n\nclassifier = RemoteCLIPClassifier(model_name=model_name, checkpoint_path=checkpoint_path)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"ViT-L-14\"</code> Specifies the RemoteCLIP model to use. <code>checkpoint_path</code> <code>str</code> <code>None</code> Path to the pretrained checkpoint file."},{"location":"indoxMiner/classification/RemoteCLIP/#usage","title":"\ud83d\ude80 Usage","text":""},{"location":"indoxMiner/classification/RemoteCLIP/#single_image_classification","title":"Single Image Classification","text":"<pre><code>from PIL import Image\n\n# Load an image\nimage = Image.open(\"/path/to/image.jpg\")\n\n# Define labels for classification\nlabels = [\"An airport\", \"A university\", \"A stadium\"]\n\n# Classify the image\nclassifier.classify(image, labels, top=3)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#batch_image_classification","title":"Batch Image Classification","text":"<pre><code># Load multiple images\nimages = [Image.open(\"/path/to/image1.jpg\"), Image.open(\"/path/to/image2.jpg\")]\n\n# Classify the batch of images\nclassifier.classify(images, labels, top=3)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#custom_labels","title":"Custom Labels","text":"<pre><code>custom_labels = [\"A tiger\", \"A mountain\", \"A river\"]\nclassifier.classify(image, labels=custom_labels, top=3)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#visualization","title":"\ud83d\udd0d Visualization","text":"<p>The <code>classify</code> method automatically generates visualizations: 1. Image: Displays the input image. 2. Bar Chart: Shows the top <code>N</code> predictions and their probabilities.</p> <p>Example:</p> <pre><code>Image 1 predictions:\n[{'An airport': 85.3}, {'A stadium': 10.2}, {'A university': 4.5}]\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#advanced_usage","title":"\ud83d\udd2c Advanced Usage","text":"<p>For more control, use the <code>preprocess</code>, <code>predict</code>, and <code>visualize</code> methods directly:</p> <pre><code># Preprocess the image and labels\ninputs = classifier.preprocess(images, labels)\n\n# Predict probabilities\nprobs = classifier.predict(inputs)\n\n# Visualize the results\nclassifier.visualize(images, labels, probs, top=3)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#supported_models","title":"\ud83d\udcdc Supported Models","text":"Model Name Description <code>RN50</code> ResNet-50-based RemoteCLIP model. <code>ViT-B-32</code> Vision Transformer with 32x32 patches. <code>ViT-L-14</code> Larger Vision Transformer model."},{"location":"indoxMiner/classification/RemoteCLIP/#example_end-to-end_workflow","title":"Example End-to-End Workflow","text":"<pre><code>from PIL import Image\nfrom indoxminer.classification import RemoteCLIPClassifier\n\n# Step 1: Initialize the classifier\nmodel_name = \"ViT-L-14\"\ncheckpoint_path = \"/path/to/RemoteCLIP-ViT-L-14.pt\"\nclassifier = RemoteCLIPClassifier(model_name=model_name, checkpoint_path=checkpoint_path)\n\n# Step 2: Load the image\nimage = Image.open(\"/path/to/airport.jpg\")\n\n# Step 3: Define labels\nlabels = [\"An airport\", \"A university\", \"A stadium\"]\n\n# Step 4: Classify the image\nclassifier.classify(image, labels, top=3)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#common_issues_and_solutions","title":"\ud83d\udee0\ufe0f Common Issues and Solutions","text":""},{"location":"indoxMiner/classification/RemoteCLIP/#1_missing_checkpoints","title":"1. Missing Checkpoints","text":"<ul> <li>Ensure the pretrained checkpoint is downloaded and accessible at the specified <code>checkpoint_path</code>.</li> </ul>"},{"location":"indoxMiner/classification/RemoteCLIP/#2_cuda_errors","title":"2. CUDA Errors","text":"<ul> <li>Check that your system supports CUDA and that <code>torch.cuda.is_available()</code> returns <code>True</code>.</li> </ul>"},{"location":"indoxMiner/classification/RemoteCLIP/#3_installation_issues","title":"3. Installation Issues","text":"<ul> <li>Ensure <code>open-clip-torch</code> and all dependencies are installed.</li> </ul>"},{"location":"indoxMiner/classification/RemoteCLIP/#summary","title":"\ud83c\udf1f Summary","text":"<p>The <code>RemoteCLIPClassifier</code> is a powerful tool for image classification, supporting multiple model variants and flexible input handling. It integrates seamlessly with <code>IndoxMiner</code>, offering: - Batch processing. - Custom label support. - Visualization of results.</p> <p>For further details, refer to the official documentation.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#remoteclip","title":"RemoteClip","text":"<p>RemoteClip enables distributed CLIP model inference through remote API endpoints, allowing for scalable deployment and load balancing.</p>"},{"location":"indoxMiner/classification/RemoteCLIP/#features","title":"Features","text":"<ul> <li>Distributed inference</li> <li>Load balancing</li> <li>Fault tolerance</li> <li>API-based deployment</li> </ul>"},{"location":"indoxMiner/classification/RemoteCLIP/#setup_and_configuration","title":"Setup and Configuration","text":"<pre><code>from indoxMiner.classification import RemoteClipClassifier\n\n# Initialize with remote endpoint\nclassifier = RemoteClipClassifier(\n    api_endpoint=\"https://api.example.com/clip\",\n    api_key=\"your-api-key\",\n    timeout=30\n)\n\n# Basic classification\nresult = classifier.classify(\"image.jpg\")\nprint(f\"Predicted: {result.label}\")\n\n# Batch classification\nresults = classifier.batch_classify([\n    \"image1.jpg\",\n    \"image2.jpg\",\n    \"image3.jpg\"\n])\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#advanced_usage_1","title":"Advanced Usage","text":"<pre><code># With custom configuration\nclassifier = RemoteClipClassifier(\n    api_endpoint=\"https://api.example.com/clip\",\n    api_key=\"your-api-key\",\n    timeout=30,\n    retry_attempts=3,\n    batch_size=16,\n    fallback_endpoint=\"https://backup-api.example.com/clip\"\n)\n</code></pre>"},{"location":"indoxMiner/classification/RemoteCLIP/#configuration_options","title":"Configuration Options","text":"<ul> <li><code>api_endpoint</code>: Primary API endpoint</li> <li><code>api_key</code>: Authentication key</li> <li><code>timeout</code>: Request timeout in seconds</li> <li><code>retry_attempts</code>: Number of retry attempts</li> <li><code>batch_size</code>: Batch processing size</li> <li><code>fallback_endpoint</code>: Backup API endpoint</li> </ul>"},{"location":"indoxMiner/classification/altclip/","title":"AltClip","text":"<p>AltClip is an alternative implementation of the CLIP model with modifications for improved performance and flexibility.</p>"},{"location":"indoxMiner/classification/altclip/#features","title":"Features","text":"<ul> <li>Enhanced training stability</li> <li>Improved zero-shot performance</li> <li>Flexible architecture modifications</li> <li>Custom prompt engineering</li> </ul>"},{"location":"indoxMiner/classification/altclip/#basic_usage","title":"Basic Usage","text":"<pre><code>from indoxMiner.classification import AltClipClassifier\n\n# Initialize the model\nclassifier = AltClipClassifier(\n    model_name=\"alt/clip-base\",\n    device=\"cuda\"\n)\n\n# Classify with custom prompts\nresult = classifier.classify_with_prompts(\n    image=\"example.jpg\",\n    prompts=[\n        \"a photograph of {}\",\n        \"an image of {}\",\n        \"a picture showing {}\"\n    ],\n    categories=[\"dog\", \"cat\", \"bird\"]\n)\n\nprint(f\"Classification: {result.label}\")\nprint(f\"Confidence: {result.confidence}\")\n</code></pre>"},{"location":"indoxMiner/classification/altclip/#advanced_configuration","title":"Advanced Configuration","text":"<pre><code>classifier = AltClipClassifier(\n    model_name=\"alt/clip-base\",\n    temperature=0.7,\n    prompt_template=\"a high quality photo of {}\",\n    batch_size=32,\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"indoxMiner/classification/altclip/#parameters","title":"Parameters","text":"<ul> <li><code>model_name</code>: Model variant to use</li> <li><code>temperature</code>: Softmax temperature</li> <li><code>prompt_template</code>: Custom prompt format</li> <li><code>batch_size</code>: Processing batch size</li> <li><code>device</code>: Computing device </li> </ul>"},{"location":"indoxMiner/classification/bioclip/","title":"BioClip","text":"<p>BioClip is a specialized CLIP model variant trained on biomedical images and text, designed for healthcare and life sciences applications.</p>"},{"location":"indoxMiner/classification/bioclip/#applications","title":"Applications","text":"<ul> <li>Medical image classification</li> <li>Pathology analysis</li> <li>Cell type identification</li> <li>Biomedical document understanding</li> </ul>"},{"location":"indoxMiner/classification/bioclip/#usage_example","title":"Usage Example","text":"<pre><code>from indoxMiner.classification import BioClipClassifier\n\n# Initialize the classifier\nclassifier = BioClipClassifier(\n    model_name=\"microsoft/bioclip-base\",\n    device=\"cuda\"\n)\n\n# Classify medical images\nresult = classifier.classify(\n    image_path=\"path/to/medical_image.jpg\",\n    categories=[\n        \"normal tissue\",\n        \"malignant tumor\",\n        \"benign tumor\"\n    ]\n)\n\nprint(f\"Classification: {result.label}\")\nprint(f\"Confidence: {result.confidence}\")\n</code></pre>"},{"location":"indoxMiner/classification/bioclip/#model_variants","title":"Model Variants","text":"<ul> <li>bioclip-base: General purpose biomedical model</li> <li>bioclip-pathology: Specialized for pathology</li> <li>bioclip-microscopy: Optimized for microscopy images</li> </ul>"},{"location":"indoxMiner/classification/bioclip/#configuration","title":"Configuration","text":"<ul> <li><code>model_name</code>: Model variant selection</li> <li><code>confidence_threshold</code>: Minimum confidence for predictions</li> <li><code>device</code>: Computing device (cuda/cpu)</li> <li><code>batch_size</code>: Batch processing size </li> </ul>"},{"location":"indoxMiner/classification/metaclip/","title":"MetaCLIP","text":"<p>MetaCLIP extends the CLIP model with meta-learning capabilities for improved few-shot learning and domain adaptation.</p>"},{"location":"indoxMiner/classification/metaclip/#key_features","title":"Key Features","text":"<ul> <li>Meta-learning for quick adaptation</li> <li>Improved few-shot performance</li> <li>Cross-domain generalization</li> <li>Efficient fine-tuning</li> </ul>"},{"location":"indoxMiner/classification/metaclip/#implementation","title":"Implementation","text":"<pre><code>from indoxMiner.classification import MetaCLIPClassifier\n\n# Initialize the classifier\nclassifier = MetaCLIPClassifier(\n    model_name=\"meta/clip-base-patch32\",\n    device=\"cuda\"\n)\n\n# Fine-tune on a few examples\nclassifier.meta_learn(\n    support_images=[\"img1.jpg\", \"img2.jpg\"],\n    support_labels=[\"class1\", \"class2\"],\n    num_adaptation_steps=5\n)\n\n# Classify new images\nresult = classifier.predict(\"new_image.jpg\")\nprint(f\"Predicted class: {result.label}\")\n</code></pre>"},{"location":"indoxMiner/classification/metaclip/#configuration_options","title":"Configuration Options","text":"<ul> <li><code>model_name</code>: Base model to use</li> <li><code>learning_rate</code>: Meta-learning rate</li> <li><code>adaptation_steps</code>: Number of adaptation steps</li> <li><code>meta_batch_size</code>: Batch size for meta-learning </li> </ul>"},{"location":"indoxMiner/classification/sigclip/","title":"SigLIP","text":"<p>SigLIP is a vision-language model that uses a sigmoid-based contrastive learning objective to compute text-image similarity scores. Unlike traditional classifiers, it enables zero-shot image classification by matching images to user-defined text prompts (e.g., \"a photo of a {class}\"), achieving flexibility and strong performance without task-specific training.</p>"},{"location":"indoxMiner/classification/sigclip/#initialization","title":"Initialization","text":"<pre><code>from indoxminer.classification import SigLIP\n\n# Initialize the classifier\nclassifier = SigLIP()\n</code></pre>"},{"location":"indoxMiner/classification/sigclip/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>model_name</code> <code>str</code> <code>\"google/siglip-base-patch16-224\"</code> Model name to use for classification."},{"location":"indoxMiner/classification/sigclip/#usage","title":"Usage","text":""},{"location":"indoxMiner/classification/sigclip/#single_image_classification","title":"Single Image Classification","text":"<pre><code>from PIL import Image\n\n# Load an image\nimage = Image.open(\"/path/to/image.jpg\")\n\n# Classify the image\nclassifier.classify(image)\n</code></pre>"},{"location":"indoxMiner/classification/sigclip/#batch_image_classification","title":"Batch Image Classification","text":"<pre><code>images = [Image.open(\"/path/to/image1.jpg\"), Image.open(\"/path/to/image2.jpg\")]\n\n# Classify the images\nclassifier.classify(images)\n</code></pre>"},{"location":"indoxMiner/classification/sigclip/#custom_labels","title":"Custom Labels","text":"<pre><code>custom_labels = [\"a cat\", \"a dog\", \"a bird\"]\nclassifier.classify(image, labels=custom_labels)\n</code></pre>"},{"location":"indoxMiner/classification/sigclip/#visualization","title":"Visualization","text":"<p>The <code>classify</code> method automatically generates a bar plot of predicted probabilities.</p>"},{"location":"indoxMiner/classification/sigclip/#advanced_usage","title":"Advanced Usage","text":"<p>For advanced users, you can directly use the <code>preprocess</code>, <code>predict</code>, and <code>visualize</code> methods for greater control. ```</p>"},{"location":"indoxMiner/classification/vit/","title":"Vision Transformer (ViT) Model","text":"<p>The Vision Transformer (ViT) is a powerful image classification model that applies the transformer architecture to computer vision tasks.</p>"},{"location":"indoxMiner/classification/vit/#overview","title":"Overview","text":"<p>Vision Transformers work by: 1. Splitting images into patches 2. Linearly embedding the patches 3. Adding position embeddings 4. Processing through transformer encoder blocks</p>"},{"location":"indoxMiner/classification/vit/#usage","title":"Usage","text":"<pre><code>from indoxMiner.classification import ViTClassifier\n\n# Initialize the model\nclassifier = ViTClassifier(\n    model_name=\"google/vit-base-patch16-224\",\n    device=\"cuda\"  # or \"cpu\"\n)\n\n# Classify an image\nresult = classifier.predict(\"path/to/image.jpg\")\nprint(result.label, result.confidence)\n</code></pre>"},{"location":"indoxMiner/classification/vit/#available_models","title":"Available Models","text":"<ul> <li>vit-base-patch16-224</li> <li>vit-large-patch16-224</li> <li>vit-huge-patch14-224</li> </ul>"},{"location":"indoxMiner/classification/vit/#configuration_options","title":"Configuration Options","text":"<ul> <li><code>model_name</code>: Name of the pretrained model to use</li> <li><code>device</code>: Computing device (\"cuda\" or \"cpu\")</li> <li><code>batch_size</code>: Batch size for inference</li> <li><code>num_labels</code>: Number of output classes </li> </ul>"},{"location":"indoxMiner/detection/CONTRIBUTING/","title":"Contributing to indoxMiner Detection","text":"<p>We welcome contributions to the indoxMiner detection module! This guide will help you get started.</p>"},{"location":"indoxMiner/detection/CONTRIBUTING/#development_setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/osllm/indoxminer.git\ncd indoxminer\n</code></pre></p> </li> <li> <p>Create a virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate  # or `venv\\Scripts\\activate` on Windows\n</code></pre></p> </li> <li> <p>Install development dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> </ol>"},{"location":"indoxMiner/detection/CONTRIBUTING/#code_style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use type hints</li> <li>Write docstrings in Google format</li> <li>Keep functions focused and modular</li> </ul>"},{"location":"indoxMiner/detection/CONTRIBUTING/#testing","title":"Testing","text":"<pre><code># Run all tests\npytest tests/\n\n# Run specific test file\npytest tests/test_detection.py\n\n# Run with coverage\npytest --cov=indoxminer tests/\n</code></pre>"},{"location":"indoxMiner/detection/CONTRIBUTING/#pull_request_process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests for new functionality</li> <li>Update documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"indoxMiner/detection/CONTRIBUTING/#code_review","title":"Code Review","text":"<p>All submissions require review. We use GitHub pull requests for this purpose.</p>"},{"location":"indoxMiner/detection/CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's MIT License. </p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/","title":"IndoxMiner - Object Detection","text":"<p>IndoxMiner offers powerful, pre-trained models for real-time and high-accuracy object detection. Whether you're working with images, videos, or other visual data sources, IndoxMiner provides state-of-the-art solutions from renowned models like DETR, YOLO, and Detectron2.</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#key_features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Wide Model Support: Utilize models such as Detectron2, DETR, GroundingDINO, YOLO, and more.</li> <li>Seamless Integration: No need to worry about model installation. IndoxMiner handles the dependencies for you!</li> <li>Pretrained Weights: All models come with pre-configured weights and checkpoints \u2014 just input them when initializing the model.</li> <li>Auto-Installation: If a required model or its dependencies are missing, IndoxMiner automatically installs them for you, making setup easy and hassle-free.</li> </ul>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#supported_models","title":"\ud83c\udfaf Supported Models","text":"<p>IndoxMiner supports the following cutting-edge models for object detection:</p> Model Description Detectron2 A fast and flexible object detection library by Facebook AI Research. DETR DEtection TRansformers (DETR), a transformer-based object detection model. DETR-CLIP Combines DETR with CLIP for improved performance in detecting a wide variety of objects. GroundingDINO Vision-language model for grounding objects in the visual scene. Kosmos2 A cross-modal vision-language pretraining model. OWL-ViT Open-Vocabulary Vision Transformer for universal object detection. RT-DETR Real-Time DEtection TRansformers for fast and efficient object detection. SAM2 Segment Anything Model, enabling flexible and interactive image segmentation. YOLOv5 YOLOv5, one of the fastest and most accurate real-time object detection models. YOLOv6 YOLOv6, a version optimized for edge devices and high-speed detection. YOLOv7 YOLOv7, an improved version of YOLO with better performance and accuracy. YOLOv8 YOLOv8, the latest in the YOLO series with even more enhancements. YOLOv10 A cutting-edge version of YOLO, designed for both speed and accuracy. YOLOv11 YOLOv11, the most recent YOLO model with further optimizations. YOLOX A highly optimized, scalable version of the YOLO family."},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#easy_setup_and_usage","title":"\ud83c\udfaf Easy Setup and Usage","text":"<p>IndoxMiner takes care of everything for you. If you don't have the required models or dependencies installed, don't worry! The library will detect what's missing and automatically handle the installation process. Simply input the model name when initializing, and IndoxMiner will take care of the rest, including fetching the required pretrained weights and checkpoints.</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#example_usage","title":"Example Usage","text":"<pre><code>from indoxminer.detection import DETR\n\n# Initialize the DETR model with specific config and weights\ndetr_model = DETR(model=\"detr\", weights=\"path/to/weights\")\n\n# Perform object detection on an image\noutputs = detr_model.detect_objects(image_path=\"/path/to/image.jpg\")\n\n# Visualize detection results\ndetr_model.visualize_results(outputs)\n</code></pre>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#model_initialization","title":"Model Initialization","text":"<p>To use any supported object detection model, simply specify the model name and provide the path to the checkpoint or weight file. Here's an example of how to initialize and run a model:</p> <pre><code># For Detectron2\nfrom indoxminer.detection import Detectron2\nmodel = Detectron2(weights=\"path/to/detectron2_checkpoint\")\n\n# For YOLOv5\nfrom indoxminer.detection import YOLOv5\nmodel = YOLOv5(weights=\"path/to/yolov5_checkpoint\")\n</code></pre>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#auto-installation_of_models","title":"Auto-Installation of Models","text":"<p>IndoxMiner ensures that you are always equipped with the latest model versions. If the required models are not installed, the library will automatically attempt to download and install the necessary dependencies for you.</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#configuration_options","title":"\ud83e\uddd1\u200d\ud83d\udcbb Configuration Options","text":"<p>Each model comes with a set of customizable options for fine-tuning the detection process. Some common configuration options include:</p> <ul> <li>Model: The model name (e.g., <code>detectron2</code>, <code>yolov5</code>, <code>detr</code>)</li> <li>Weights: Path to pre-trained model weights or checkpoints</li> <li>Confidence Threshold: Set a confidence threshold for detection (default is 0.5)</li> <li>Device: The device to run the model on (<code>cpu</code> or <code>cuda</code> for GPU acceleration)</li> </ul> <pre><code>model = YOLOv5(\n    weights=\"path/to/yolov5_checkpoint\", \n    confidence_threshold=0.6, \n    device=\"cuda\"\n)\n</code></pre>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#visualizing_detection_results","title":"\ud83d\uddbc\ufe0f Visualizing Detection Results","text":"<p>IndoxMiner provides built-in functions to visualize the detection results. The bounding boxes, class labels, and confidence scores are drawn directly on the image, making it easy to interpret the results.</p> <pre><code>model.visualize_results(outputs)  # Displays bounding boxes and labels\n</code></pre>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#supported_formats_and_outputs","title":"\ud83e\udd16 Supported Formats and Outputs","text":"<p>Detected objects are returned as a list of dictionaries with the following details:</p> <ul> <li><code>bbox</code>: Bounding box coordinates (x1, y1, x2, y2)</li> <li><code>class_id</code>: The class ID of the detected object</li> <li><code>confidence</code>: Confidence score for the prediction</li> </ul> <p>For example:</p> <pre><code>{\n    \"bbox\": [x1, y1, x2, y2],\n    \"class_id\": 63,  # COCO class ID\n    \"confidence\": 0.87\n}\n</code></pre>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#model_weights_and_pre-trained_checkpoints","title":"\ud83d\ude80 Model Weights and Pre-trained Checkpoints","text":"<p>All necessary model weights and checkpoints are included and can be accessed directly. You don't have to manually download or set up the weights; just point to the path and you're ready to go!</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions to improve and extend the capabilities of IndoxMiner. To contribute:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Commit your changes</li> <li>Push to your branch</li> <li>Open a Pull Request</li> </ol> <p>For guidelines and more information, please refer to the Contributing Guidelines.</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the License file for details.</p>"},{"location":"indoxMiner/detection/object_detection_with_indoxMiner/#support","title":"\ud83c\udd98 Support","text":"<ul> <li>Documentation: Full documentation</li> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/","title":"Automatic Schema Detection in Indox Miner","text":"<p>Indox Miner\u2019s Automatic Schema Detection feature provides users with the ability to dynamically infer and extract structured information from documents without manually defining a schema. By leveraging the <code>AutoSchema</code> class, Indox Miner can automatically identify fields, detect data structures, and apply validation rules to ensure data quality.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#purpose_of_automatic_schema_detection","title":"Purpose of Automatic Schema Detection","text":"<p>Automatic Schema Detection enables: - Dynamic extraction of fields from unstructured documents, reducing the need for predefined schemas. - Flexibility in handling a variety of document layouts, including tables, forms, and other structured data. - Automated validation of extracted fields based on inferred types and rules, ensuring accurate and consistent data.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#how_automatic_schema_detection_works","title":"How Automatic Schema Detection Works","text":"<p>The <code>AutoSchema</code> class in Indox Miner is designed to: 1. Analyze Document Structure: <code>AutoSchema</code> examines the text to identify patterns resembling tables, forms, and key-value pairs. 2. Infer Field Types: Using the <code>AutoDetectedField</code> class, it assigns data types (e.g., date, number, boolean) to detected fields. 3. Generate Extraction Rules: Validation rules are automatically generated based on field types, ensuring data consistency and quality. 4. Validate Extraction Results: After extraction, detected data is validated against inferred rules, and any inconsistencies are flagged.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#components_of_automatic_schema_detection","title":"Components of Automatic Schema Detection","text":""},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#1_autodetectedfield","title":"1. <code>AutoDetectedField</code>","text":"<p>Represents an automatically detected field with properties like: - Name: Field name, inferred from the text. - Field Type: Type of data (e.g., string, number, date, boolean). - Description: Description based on inferred type and document structure. - Required: Whether the field is essential for extraction. - Rules: Validation rules for the field, generated based on type and context.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#2_autoextractionrules","title":"2. <code>AutoExtractionRules</code>","text":"<p>Contains rules for auto-extraction and validation, including: - Min/Max Length: Ensures text fields have appropriate character length. - Pattern: Regular expression patterns for formats like dates or specific identifiers. - Min/Max Value: Specifies valid ranges for numeric fields. - Allowed Values: List of valid values for categorical fields, such as booleans.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#3_autoschema","title":"3. <code>AutoSchema</code>","text":"<p>Manages the overall automatic detection and extraction process, including: - Field Detection: Identifies fields by analyzing the document for table structures and form-like fields. - Structure Inference: Automatically detects document layout (e.g., table, form). - Prompt Generation: Generates a prompt that instructs the extraction model on the detected fields and structures.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#usage_of_automatic_schema_detection","title":"Usage of Automatic Schema Detection","text":"<p>To use <code>AutoSchema</code> for automatic schema detection, initialize the class, pass the document text, and let <code>AutoSchema</code> infer the structure and fields.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#example","title":"Example","text":"<pre><code>from indox_miner.autoschema import AutoSchema\n\n# Initialize AutoSchema instance\nauto_schema = AutoSchema()\n\n# Document text to analyze\ndocument_text = \"\"\"\nName: John Doe\nDate of Birth: 1990-01-01\nTotal Amount: $150.00\n\nItem Description    Quantity   Unit Price   Total\nItem 1              2          $20.00       $40.00\nItem 2              5          $15.00       $75.00\n\"\"\"\n\n# Infer structure and fields\nauto_schema.infer_structure(document_text)\n\n# Generate prompt based on detected fields\nprompt = auto_schema.to_prompt(document_text)\nprint(prompt)\n</code></pre>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#output_prompt_example","title":"Output Prompt Example","text":"<pre><code>Task: Extract structured information from the given text using automatic field detection.\n\nDetected Fields:\n- Name (string): Automatically detected string field from form\n- Date of Birth (date): Automatically detected date field from form\n- Total Amount (number): Automatically detected number field from form\n- Item Description (string): Automatically detected string field from table column\n- Quantity (number): Automatically detected number field from table column\n- Unit Price (number): Automatically detected number field from table column\n- Total (number): Automatically detected number field from table column\n\nExtraction Requirements:\n1. Extract all detected fields maintaining their original names\n2. Use appropriate data types for each field:\n   - Dates in ISO format (YYYY-MM-DD)\n   - Numbers as numeric values (not strings)\n   - Boolean values as true/false\n   - Lists as arrays\n   - Nested data as objects\n3. Preserve any detected relationships between fields\n4. Return data in JSON format\n\nText to analyze:\n{Name: John Doe...}\n</code></pre>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#how_autoschema_detects_structure","title":"How <code>AutoSchema</code> Detects Structure","text":""},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#table_detection","title":"Table Detection","text":"<p>The <code>_looks_like_table</code> method detects table-like structures by analyzing: - Delimiter rows: Lines with multiple delimiters (e.g., tabs, pipes <code>|</code>). - Consistent Spacing: Lines with uniform spacing patterns across columns. - Header Indicators: Rows with indicators such as hyphens (<code>---</code>) or equals (<code>===</code>) which often denote headers.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#form_field_detection","title":"Form Field Detection","text":"<p>The <code>_detect_form_fields</code> method identifies fields with: - Label-Value Patterns: Lines containing labels followed by values, separated by whitespace or punctuation (e.g., <code>Name: John Doe</code>). - Keyword Filtering: Ignores known non-field phrases, focusing only on relevant label-value pairs.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#field_type_inference","title":"Field Type Inference","text":"<p>The <code>_infer_field_type</code> method determines field types based on: - Patterns: Recognizes dates, numbers, and booleans through regular expressions. - Content Indicators: Uses delimiters (e.g., commas, semicolons) to detect lists and categorizes remaining data as strings.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#rule_generation","title":"Rule Generation","text":"<p>The <code>_generate_rules</code> method automatically creates validation rules according to the inferred field type, ensuring: - String Fields: Min and max character length limits. - Numeric Fields: Range limits (min and max values). - Date Fields: Pattern matching (e.g., ISO format). - Boolean Fields: Restriction to true/false values.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#validating_extraction_results","title":"Validating Extraction Results","text":"<p>After extracting data, the <code>validate_extraction</code> method validates each field according to its rules, identifying any inconsistencies or missing values. This ensures that the extracted data meets predefined quality standards.</p>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#example_of_validation","title":"Example of Validation","text":"<pre><code>extracted_data = {\n    \"Name\": \"John Doe\",\n    \"Date of Birth\": \"1990-01-01\",\n    \"Total Amount\": 150.0,\n    \"Item Description\": [\"Item 1\", \"Item 2\"],\n    \"Quantity\": [2, 5],\n    \"Unit Price\": [20.0, 15.0],\n    \"Total\": [40.0, 75.0]\n}\n\n# Validate extracted data\nerrors = auto_schema.validate_extraction(extracted_data)\nif errors:\n    print(\"Validation errors found:\", errors)\nelse:\n    print(\"All fields are valid.\")\n</code></pre>"},{"location":"indoxMiner/extraction/Automatic%20Schema%20Detection/#conclusion","title":"Conclusion","text":"<p>The Automatic Schema Detection feature in Indox Miner simplifies data extraction from unstructured documents by dynamically detecting fields and validating data against inferred rules. This functionality reduces setup time, enhances flexibility, and maintains data quality, making Indox Miner a powerful tool for diverse document processing needs.</p>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/","title":"Document Type Support in Indox Miner","text":"<p>Indox Miner is designed to process various document types, providing users with the flexibility to extract structured information from multiple sources, including text files, spreadsheets, and images. This guide details the supported document types and provides instructions for enabling image processing.</p>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#supported_document_types","title":"Supported Document Types","text":"<p>Indox Miner can process the following document types:</p> Document Type Extensions Description PDF <code>.pdf</code> Portable Document Format, widely used for sharing formatted documents. Word <code>.doc</code>, <code>.docx</code> Microsoft Word files, common for reports and text-heavy documents. Excel <code>.xls</code>, <code>.xlsx</code> Microsoft Excel spreadsheets, often used for tabular data. PowerPoint <code>.ppt</code>, <code>.pptx</code> Microsoft PowerPoint files, typically used for presentations. Image <code>.png</code>, <code>.jpg</code>, <code>.jpeg</code>, <code>.bmp</code>, <code>.tiff</code>, <code>.heic</code> Image files in various formats (supports OCR for text extraction). Text <code>.txt</code>, <code>.csv</code>, <code>.tsv</code> Plain text and comma/tab-separated files, ideal for simple data. Markdown <code>.md</code> Markdown files for lightweight formatting. RTF <code>.rtf</code> Rich Text Format, allows basic text formatting. Email <code>.eml</code>, <code>.msg</code> Email files, useful for processing email threads and headers. Web Page <code>.html</code> HTML files, enabling processing of saved web pages. XML <code>.xml</code> XML files, common for structured data exchange. EPUB <code>.epub</code> E-book format for reading and processing digital publications."},{"location":"indoxMiner/extraction/Document%20Type%20Support/#enabling_image_processing","title":"Enabling Image Processing","text":"<p>Indox Miner includes support for Optical Character Recognition (OCR), allowing it to extract text from image files. To enable image processing:</p> <ol> <li> <p>Set the OCR flag: The <code>DocumentProcessor</code> class includes a configuration flag for OCR. To process images, set <code>ocr_for_images</code> to <code>True</code> in the configuration.</p> </li> <li> <p>Choose an OCR Model: Indox Miner supports various OCR models, such as Tesseract, PaddleOCR, and EasyOCR. Specify the model in the <code>ocr_model</code> parameter.</p> </li> </ol>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#example_enabling_image_processing","title":"Example: Enabling Image Processing","text":"<pre><code>from indox_miner.processor import DocumentProcessor, ProcessingConfig\n\n# Configure the document processor with OCR enabled\nconfig = ProcessingConfig(\n    ocr_for_images=True,       # Enable OCR for images\n    ocr_model=\"tesseract\"      # Choose an OCR model (tesseract, paddle, or easyocr)\n)\n\n# Initialize DocumentProcessor with sources and configuration\nsources = [\"path/to/document.pdf\", \"path/to/image.jpg\"]\nprocessor = DocumentProcessor(sources)\n\n# Process documents, including images\nprocessed_data = processor.process(config=config)\n</code></pre>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#ocr_models_and_their_usage","title":"OCR Models and Their Usage","text":"OCR Model Description Tesseract A highly accurate, open-source OCR engine. PaddleOCR Suitable for multilingual support, uses PaddlePaddle. EasyOCR Quick and efficient, good for basic OCR tasks."},{"location":"indoxMiner/extraction/Document%20Type%20Support/#handling_different_document_types_in_extraction","title":"Handling Different Document Types in Extraction","text":"<ol> <li>Initialize <code>DocumentProcessor</code> with one or more document sources.</li> <li>Set configuration options: Use <code>ProcessingConfig</code> to specify additional settings, such as chunk size, table inference, and OCR options.</li> <li>Run Processing: The processor will automatically detect each document type and apply the appropriate extraction method.</li> </ol>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#example_processing_multiple_document_types","title":"Example: Processing Multiple Document Types","text":"<pre><code>from indox_miner.processor import DocumentProcessor, ProcessingConfig\n\n# Define document sources\nsources = [\n    \"path/to/document.pdf\",\n    \"path/to/spreadsheet.xlsx\",\n    \"path/to/image.jpg\"\n]\n\n# Configure processor for images and PDFs\nconfig = ProcessingConfig(\n    chunk_size=500,\n    hi_res_pdf=True,\n    ocr_for_images=True,\n    ocr_model=\"tesseract\"\n)\n\n# Initialize the DocumentProcessor and process\nprocessor = DocumentProcessor(sources=sources)\nprocessed_documents = processor.process(config=config)\n</code></pre>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#advanced_configuration_options","title":"Advanced Configuration Options","text":"<p>The <code>ProcessingConfig</code> class provides additional settings to customize processing:</p> <ul> <li>Chunk Size: Set <code>chunk_size</code> to control the maximum size of text chunks (default is 4048 characters).</li> <li>High-Resolution PDFs: Set <code>hi_res_pdf</code> to <code>True</code> for high-quality PDF processing.</li> <li>Infer Tables: Set <code>infer_tables</code> to <code>True</code> to detect and process tables within documents.</li> <li>Remove Headers/References: Configure <code>remove_headers</code> and <code>remove_references</code> to exclude header and reference sections in structured documents.</li> <li>Filter Empty Elements: Set <code>filter_empty_elements</code> to remove any blank elements from the extracted content.</li> </ul>"},{"location":"indoxMiner/extraction/Document%20Type%20Support/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s document type support allows for efficient extraction from a wide range of formats. By enabling image processing with OCR, users can also extract information from scanned documents and images, making Indox Miner a versatile tool for handling complex document types.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/","title":"IndoxMiner: Extracting Structured Data from Images","text":"<p>IndoxMiner provides a powerful and flexible way to extract structured data from unstructured text within images. Using OCR (Optical Character Recognition) to convert image content to text and LLMs (Large Language Models) to interpret and extract specific fields, IndoxMiner simplifies data extraction from images like invoices, receipts, ID cards, and more.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#key_features","title":"Key Features","text":"<ul> <li>\ud83d\udcf8 Image to Structured Data: Extract information from images and convert it into structured formats.</li> <li>\ud83d\udd20 OCR Integration: Supports multiple OCR models, including PaddleOCR and EasyOCR, for text extraction from images.</li> <li>\ud83d\udd0d Custom Extraction Schemas: Define and validate the data fields you want to extract, tailored to document types like passports and invoices.</li> <li>\u2705 Built-in Validation Rules: Ensures data accuracy with customizable validation options.</li> <li>\ud83d\udcca Easy Conversion to DataFrames: Seamlessly convert results to pandas DataFrames for analysis and manipulation.</li> <li>\ud83e\udd16 LLM Integration: Use OpenAI, IndoxApi, and other LLMs for advanced text interpretation and extraction quality enhancement.</li> <li>\ud83d\udd04 Async Support: Process multiple images concurrently for optimized performance.</li> </ul>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#installation","title":"Installation","text":"<p>To set up IndoxMiner, clone the repository and install dependencies:</p> <pre><code>pip install indoxminer\npip install paddlepaddle paddleocr  # or easyocr, tesseract depending on your choice\n</code></pre> <p>You will also need an OCR library to handle the image-to-text conversion.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#quick_start","title":"Quick Start","text":""},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#step_1_set_up_the_ocr_processor_and_llm","title":"Step 1: Set up the OCR Processor and LLM","text":"<pre><code>from indoxminer import OpenAi, DocumentProcessor, ProcessingConfig, Schema, Extractor\n\n# Initialize OpenAi\nopenai_api = OpenAi(api_key=\"YOUR_API_KEY\")  # Replace with your actual API key\n\n# Initialize OCR processor with configuration\nconfig = ProcessingConfig(ocr_for_images=True, ocr_model='paddle')  # Change to 'easyocr' or 'tesseract' as needed\n</code></pre>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#step_2_define_image_paths_and_schema","title":"Step 2: Define Image Paths and Schema","text":"<p>Define the images to process and select a predefined schema or create a custom one.</p> <pre><code># Define the directory containing passport images\nimage_directory = 'data/passport_dataset_jpg/'\npassport_images = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\n# Check if the paths exist\nfor image_path in passport_images:\n    if not os.path.exists(image_path):\n        raise FileNotFoundError(f\"The image file {image_path} does not exist.\")\n\n# Initialize the extractor with the OpenAi LLM and the passport schema\nextractor = Extractor(llm=openai_api, schema=Schema.Passport)\n</code></pre>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#step_3_process_images_and_extract_data","title":"Step 3: Process Images and Extract Data","text":"<p>Process each image with OCR to extract the text, then use IndoxMiner to extract structured data according to the schema.</p> <pre><code>processor = DocumentProcessor(passport_images)\n\n# Process document to extract text using OCR\nresults = processor.process(config)\n\n# Extract data from the documents\nextracted_data = extractor.extract(results)\n</code></pre>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#step_4_handle_results_and_convert_to_dataframe","title":"Step 4: Handle Results and Convert to DataFrame","text":"<p>Process the extraction results and convert them into a DataFrame.</p> <pre><code># Convert extraction results to DataFrame\ndf = extractor.to_dataframe(extracted_data)\nprint(df)\n\n# Display valid results or handle errors\nfor result in extraction_results:\n    if result.is_valid:\n        print(\"Extracted Data:\", result)\n    else:\n        print(\"Extraction errors occurred:\", result.validation_errors)\n</code></pre>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#detailed_workflow","title":"Detailed Workflow","text":"<ol> <li>OCR Processing: Extracts raw text from images using the <code>DocumentProcessor</code>. This component utilizes OCR to convert image-based content into text.</li> <li>Schema Selection: Choose from predefined schemas (like Passport, Invoice, Receipt) or create custom schemas to define the structure of data to be extracted.</li> <li>Data Extraction: Using the selected schema, the LLM processes the extracted text and returns it as structured data according to the specified fields.</li> <li>Validation: Each field undergoes validation based on rules defined in the schema, ensuring accuracy by checking constraints like minimum length, numeric range, and specific patterns.</li> <li>Output Formats: Extracted results can be converted into multiple formats, including JSON, pandas DataFrames, or raw dictionaries, making further data processing seamless.</li> </ol>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#core_components_for_image_extraction","title":"Core Components for Image Extraction","text":""},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#openai","title":"<code>OpenAi</code>","text":"<p>The <code>OpenAi</code> class serves as the primary interface for interacting with the OpenAI. This component handles authentication, manages API requests, and retrieves responses.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#documentprocessor","title":"<code>DocumentProcessor</code>","text":"<p>The <code>DocumentProcessor</code> class is responsible for managing the workflow of document processing, including reading the documents and applying OCR.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#schema","title":"<code>Schema</code>","text":"<p>Schemas define the structure of data to be extracted from the text, including fields and validation rules. </p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#extractor","title":"<code>Extractor</code>","text":"<p>The <code>Extractor</code> is the main class responsible for interacting with the LLM, validating extracted data, and formatting output.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#validation_rules","title":"Validation Rules","text":"<p>Validation rules ensure data quality by setting constraints on each field within a schema.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#supported_output_formats","title":"Supported Output Formats","text":"<ul> <li>JSON: Returns structured data in JSON format, suitable for further processing or storage.</li> <li>DataFrame: Converts the results to a pandas DataFrame for analysis and manipulation.</li> <li>Dictionary: Access the raw extraction results as dictionaries for flexible handling.</li> <li>Markdown: </li> </ul>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#conclusion","title":"Conclusion","text":"<p>In this document, we outlined how to extract structured data from passport images using IndoxMiner. This process automates the retrieval of critical information typically found in passports, facilitating efficient data processing for various applications.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#future_work","title":"Future Work","text":"<p>Consider enhancing this demo by adding error handling for OCR failures, integrating logging for better traceability, or extending the extraction schema for additional fields.</p>"},{"location":"indoxMiner/extraction/Extracting%20Structured%20Data%20from%20Images/#additional_features_of_indoxminer","title":"Additional Features of IndoxMiner","text":"<ul> <li>Dynamic Schema Adaptation: Users can define and adapt schemas dynamically, allowing for easy adjustments to the data extraction process as document formats change.</li> <li>Comprehensive Documentation: IndoxMiner comes with thorough documentation to help users implement features, troubleshoot issues, and optimize their extraction processes.</li> <li>Community Support: As an open-source project, IndoxMiner benefits from community contributions and support, enabling continuous improvement and feature enhancement.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/","title":"Large Language Model (LLM) Support in Indox Miner","text":"<p>Indox Miner supports various Large Language Models (LLMs) for processing and extracting information from documents. Each LLM provides unique features, and users can select the model that best meets their needs for accuracy, performance, or compatibility.</p>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#purpose_of_llm_support","title":"Purpose of LLM Support","text":"<p>Using different LLMs in Indox Miner allows for: - Flexible extraction quality: Choose a model based on the complexity of the document and the level of detail required. - Scalability: Some models are optimized for high-speed processing, making them ideal for large-scale extraction tasks. - Customizable performance: Whether prioritizing accuracy, speed, or cost-efficiency, each LLM offers a tailored balance of these factors.</p>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#available_llms","title":"Available LLMs","text":"<p>Indox Miner supports the following LLMs:</p>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#1_openai_gpt-4","title":"1. OpenAI GPT-4","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: GPT-4 by OpenAI is one of the most advanced LLMs, capable of handling complex text processing and extraction tasks.</li> <li>Best For: High-quality and precise extractions from complex documents.</li> <li>Features:<ul> <li>Asynchronous support for batch processing.</li> <li>Customizable temperature and max tokens for tuning response specificity.</li> </ul> </li> <li>Usage: Ideal for high-detail extractions where accuracy is paramount.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#2_anthropic_claude","title":"2. Anthropic Claude","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: Claude by Anthropic is a robust LLM that balances speed and accuracy, making it a suitable choice for diverse document types.</li> <li>Best For: General-purpose extraction tasks that require a balance between performance and precision.</li> <li>Features:<ul> <li>Configurable temperature for response creativity.</li> <li>Asynchronous processing for scalable tasks.</li> </ul> </li> <li>Usage: Best suited for medium-complexity documents with moderate detail requirements.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#3_ollama_llama","title":"3. Ollama (LLaMA)","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: LLaMA by Meta, integrated through Ollama, is an optimized model designed for efficient data processing.</li> <li>Best For: Scenarios requiring high-throughput processing with acceptable accuracy.</li> <li>Features:<ul> <li>Asynchronous support for concurrent requests.</li> <li>Streaming support for real-time applications.</li> </ul> </li> <li>Usage: A good choice for time-sensitive tasks or high-volume document extraction where speed is prioritized.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#4_nerdtoken_api","title":"4. NerdToken API","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: NerdToken API offers a model designed to work with real-time data extraction in specialized document formats.</li> <li>Best For: Real-time or streaming tasks where data extraction needs to be as efficient as possible.</li> <li>Features:<ul> <li>Advanced temperature, frequency, and presence penalties for tailored responses.</li> <li>Optimized for quick setup and results.</li> </ul> </li> <li>Usage: Suited for specialized applications requiring low latency and immediate data retrieval.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#5_vllm","title":"5. vLLM","text":"<ul> <li>Mode: Synchronous and Asynchronous</li> <li>Description: vLLM provides flexibility with a RESTful API, enabling integration with custom solutions.</li> <li>Best For: Cases where flexible REST API integration is needed for scalable deployments.</li> <li>Features:<ul> <li>REST API-based access for seamless integration with external applications.</li> <li>Highly customizable request parameters.</li> </ul> </li> <li>Usage: Appropriate for flexible integration scenarios with custom API needs.</li> </ul>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#how_to_use_different_llms_in_indox_miner","title":"How to Use Different LLMs in Indox Miner","text":"<ol> <li>Select the LLM: Choose an LLM based on your project\u2019s requirements for accuracy, speed, or data complexity.</li> <li>Initialize the Extractor: Pass the LLM instance as a parameter when initializing the <code>Extractor</code> class.</li> <li>Configure Model Parameters: Adjust parameters like <code>temperature</code>, <code>max_tokens</code>, and <code>model</code> as needed for the chosen LLM.</li> <li>Run Extraction: Execute the extraction process, and Indox Miner will use the selected LLM to generate and validate extracted data.</li> </ol>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#example","title":"Example","text":"<pre><code>from indox_miner.llms import OpenAi, Anthropic, Ollama\nfrom indox_miner.extractor import Extractor\nfrom indox_miner.schema import Schema\n\n# Initialize the LLM\nllm = OpenAi(api_key=\"your-api-key\", model=\"gpt-4\", temperature=0.0)\n\n# Set up the extractor with the schema and LLM\nextractor = Extractor(llm=llm, schema=Schema.Invoice)\n\n# Extract data from the document text\nresult = extractor.extract(\"Your document text here\")\n</code></pre>"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#selecting_the_right_llm","title":"Selecting the Right LLM","text":"LLM Best For Advantages GPT-4 High-accuracy extractions Advanced language capabilities, precise results Claude General-purpose extraction Balanced performance, asynchronous processing LLaMA High-throughput, real-time processing Fast response, streaming support NerdToken Real-time, efficient extractions Quick setup, low-latency processing vLLM Custom integration and flexible deployment REST API support, easily configurable parameters"},{"location":"indoxMiner/extraction/LLM%20Support%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s LLM support provides the flexibility to adapt to various document processing needs, from detailed extractions to high-speed, real-time tasks. By selecting the right LLM for the job, users can optimize Indox Miner for both accuracy and efficiency.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/","title":"Output Types in Indox Miner","text":"<p>Indox Miner provides multiple output formats, enabling users to view and export extracted data in the most convenient form for their needs. The available output types include structured formats such as DataFrame, JSON, Markdown, and Table. Each format offers unique benefits, depending on how the data will be used or shared.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#available_output_types","title":"Available Output Types","text":"Output Type Description DataFrame A pandas DataFrame, useful for data analysis and manipulation in Python. JSON A JSON string, ideal for storing or transmitting structured data. Markdown Markdown format for easily readable text with structured tables. Table A formatted table string, suitable for displaying data in terminal outputs."},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#using_output_types_in_indox_miner","title":"Using Output Types in Indox Miner","text":"<p>The <code>Extractor</code> class in Indox Miner includes methods for each output type, making it simple to format extracted data as needed.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#1_dataframe_output","title":"1. DataFrame Output","text":"<p>The <code>to_dataframe()</code> method converts extraction results to a pandas DataFrame, which is particularly useful for data analysis, manipulation, and visualization.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#example","title":"Example","text":"<pre><code>from indox_miner.extractor import Extractor\nfrom indox_miner.llms import OpenAi\nfrom indox_miner.schema import Schema\n\n# Set up the extractor with LLM and schema\nllm = OpenAi(api_key=\"your-api-key\", model=\"gpt-4\")\nextractor = Extractor(llm=llm, schema=Schema.Invoice)\n\n# Extract data and convert to DataFrame\nresult = extractor.extract(\"Your document text here\")\ndf = extractor.to_dataframe(result)\n\n# Display or manipulate DataFrame\nprint(df)\n</code></pre>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#2_json_output","title":"2. JSON Output","text":"<p>The <code>to_json()</code> method converts results to a JSON string, which is ideal for storing or transmitting data in a structured and compact format.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#example_1","title":"Example","text":"<pre><code>json_output = extractor.to_json(result)\nprint(json_output)\n</code></pre>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#3_markdown_output","title":"3. Markdown Output","text":"<p>The <code>to_markdown()</code> method generates a Markdown-formatted string, which is highly readable and can be directly used in Markdown-compatible platforms like GitHub or documentation tools.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#example_2","title":"Example","text":"<pre><code>markdown_output = extractor.to_markdown(result)\nprint(markdown_output)\n</code></pre>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#4_table_output","title":"4. Table Output","text":"<p>The <code>to_table()</code> method creates a formatted table string, which is especially useful for displaying extracted data directly in the terminal or CLI environments.</p>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#example_3","title":"Example","text":"<pre><code>table_output = extractor.to_table(result)\nprint(table_output)\n</code></pre>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#choosing_the_right_output_type","title":"Choosing the Right Output Type","text":"Use Case Recommended Output Type Data Analysis DataFrame API Integration / Storage JSON Documentation or Reports Markdown Command Line Display Table"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#customizing_output_structure","title":"Customizing Output Structure","text":"<ol> <li>Field Selection: The output format includes all fields defined in the schema by default. To customize this, adjust the schema or filter fields in the resulting DataFrame or JSON.</li> <li>Nested and List Fields: Indox Miner preserves nested structures and lists in output, allowing easy handling of complex data (e.g., items in an invoice or medications in a medical record).</li> <li>Validation and Formatting: All output formats respect validation rules specified in the schema, ensuring data quality and consistency.</li> </ol>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#example_extracting_and_exporting_data","title":"Example: Extracting and Exporting Data","text":"<p>Here\u2019s a complete example of extracting data from a document, then exporting it in different formats:</p> <pre><code># Perform extraction\nresult = extractor.extract(\"Sample document text here\")\n\n# Export to different formats\ndf = extractor.to_dataframe(result)\njson_output = extractor.to_json(result)\nmarkdown_output = extractor.to_markdown(result)\ntable_output = extractor.to_table(result)\n\n# Display or save outputs\nprint(\"DataFrame:\\n\", df)\nprint(\"JSON:\\n\", json_output)\nprint(\"Markdown:\\n\", markdown_output)\nprint(\"Table:\\n\", table_output)\n</code></pre>"},{"location":"indoxMiner/extraction/Output%20Types%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Indox Miner\u2019s flexible output options make it easy to integrate extracted data into various workflows, whether for analysis, documentation, or display. By selecting the appropriate output type, users can maximize the utility and accessibility of their data.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/","title":"Predefined Schema Support in Indox Miner","text":"<p>Indox Miner supports a variety of predefined schemas to streamline and standardize the process of extracting structured information from documents. These schemas act as templates that define which fields to extract, along with validation rules to ensure data consistency and accuracy.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#purpose_of_predefined_schemas","title":"Purpose of Predefined Schemas","text":"<p>Predefined schemas help: - Standardize data extraction across different document types. - Simplify the setup for users by providing ready-to-use configurations. - Ensure data quality by applying field-specific validation rules. - Support different document types (e.g., invoices, passports, medical records) with minimal configuration.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#available_predefined_schemas","title":"Available Predefined Schemas","text":"<p>Indox Miner includes the following predefined schemas:</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#1_passport_schema","title":"1. Passport Schema","text":"<p>Extracts key information from passport documents, including: - Passport Number: Unique passport identifier. - Given Names: First and middle names. - Surname: Family name/surname. - Date of Birth: Formatted as YYYY-MM-DD. - Place of Birth: Location of birth (city and country). - Nationality: Country of citizenship. - Gender: Gender identifier (M/F/X). - Date of Issue: Passport issue date. - Date of Expiry: Passport expiration date. - Place of Issue: Location of passport issuance. - MRZ: Machine-readable zone text.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#2_invoice_schema","title":"2. Invoice Schema","text":"<p>Extracts data from invoices, focusing on financial and transactional details: - Invoice Number: Unique invoice identifier. - Date: Invoice issue date. - Company Name: Name of issuing company. - Address: Address associated with the company or billing. - Customer Name: Name of the customer. - Items: List of items with description, quantity, unit price, and total. - Subtotal: Pre-tax amount. - Tax Amount: Total tax amount. - Total Amount: Grand total, including tax.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#3_flight_ticket_schema","title":"3. Flight Ticket Schema","text":"<p>Captures essential details from flight tickets, such as: - Ticket Number: Unique ticket identifier. - Passenger Name: Full name of the passenger. - Flight Number: Airline flight number. - Departure Airport and Arrival Airport: IATA codes for origin and destination airports. - Departure DateTime and Arrival DateTime: Departure and arrival times. - Seat Number: Assigned seat. - Class: Travel class (Economy, Business, First). - Booking Reference: PNR or booking code. - Fare: Ticket fare.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#4_bank_statement_schema","title":"4. Bank Statement Schema","text":"<p>Supports extraction from bank statements with fields like: - Account Holder: Name on the account. - Account Number: Bank account number. - IBAN: International Bank Account Number. - Statement Period: Month and year of statement. - Opening Balance and Closing Balance: Account balances. - Transactions: List of transactions with date, description, amount, type, and reference.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#5_medical_record_schema","title":"5. Medical Record Schema","text":"<p>Facilitates the extraction of patient medical records: - Patient Name: Full name of the patient. - Date of Birth: Formatted as YYYY-MM-DD. - Medical Record Number: Unique identifier for the record. - Diagnosis: List of diagnoses with code, description, and date. - Medications: List of prescribed medications. - Physician: Information about the treating physician. - Vital Signs: Basic vitals like blood pressure, heart rate, temperature.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#6_driver_license_schema","title":"6. Driver License Schema","text":"<p>Extracts information from driver licenses, such as: - License Number: Unique identifier. - Full Name: License holder\u2019s full name. - Address: Residential address. - Date of Birth: Formatted as YYYY-MM-DD. - Issue Date and Expiry Date: License issuance and expiration dates. - Class: License class or type. - Restrictions and Endorsements: Lists of restrictions and endorsements.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#7_resume_schema","title":"7. Resume Schema","text":"<p>Enables structured extraction from resumes or CVs: - Full Name: Candidate's name. - Contact: Includes email, phone, address, LinkedIn, portfolio, GitHub. - Professional Summary: Short summary or objective. - Work Experience: List of work history with company, position, dates, and achievements. - Education: Academic background, degrees, institutions, dates, and GPA. - Skills: Categorized technical, soft, and language skills. - Projects: Description of key projects. - Awards, Certifications, Publications, Volunteer Experience, Additional Information.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#how_to_use_predefined_schemas","title":"How to Use Predefined Schemas","text":"<p>To use a predefined schema: 1. Select the appropriate schema based on the document type. 2. Configure the extractor to use the selected schema. 3. Run the extraction, and Indox Miner will apply the schema's structure and validation rules.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#customization_and_extension","title":"Customization and Extension","text":"<p>While Indox Miner provides these predefined schemas out of the box, users can: - Customize existing schemas by modifying field requirements or validation rules. - Create new schemas for unsupported document types by defining custom fields and rules.</p>"},{"location":"indoxMiner/extraction/Predefined%20Schema%20Support%20in%20Indox%20Miner/#conclusion","title":"Conclusion","text":"<p>Predefined schemas in Indox Miner make it easier and faster to extract structured information from various documents accurately and consistently. By leveraging these schemas, users can significantly reduce the setup time required for data extraction tasks and ensure high-quality data output.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/","title":"Multimodal Models in IndoxMiner","text":""},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#overview","title":"Overview","text":"<p>The <code>multimodal/</code> module in IndoxMiner provides support for vision-language models that can process both images and text. These models extend IndoxMiner beyond object detection and classification by enabling natural language understanding of images. </p> <p>Currently, the following multimodal models are supported: - LLaVA-NeXT (LLaVA + LLaMA 3 8B Instruct) - BLIP-2 (Vision-Language Model for Captioning &amp; Question Answering)</p> <p>These models allow users to ask questions about images and receive detailed natural language responses or generate captions automatically.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#installation_for_llava_model","title":"Installation (for LLaVA Model)","text":"<p>Before using the <code>multimodal</code> models, you need to install LLaVA-NeXT and its dependencies.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#1_clone_and_install_llava-next","title":"1\ufe0f\u20e3 Clone and Install LLaVA-NeXT","text":"<pre><code>!git clone https://github.com/LLaVA-VL/LLaVA-NeXT\n%cd LLaVA-NeXT\n!pip install -e .\n</code></pre>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#2_modify_llava_to_use_llama_3_nousresearchmeta-llama-3-8b-instruct","title":"2\ufe0f\u20e3 Modify LLaVA to Use LLaMA 3 (NousResearch/Meta-Llama-3-8B-Instruct)","text":"<p>LLaVA uses a default model, but for better performance, update <code>conversation.py</code> to use NousResearch/Meta-Llama-3-8B-Instruct instead.</p> <p>Modify this line in <code>LLaVA-NeXT/llava/conversation.py</code>: <pre><code>tokenizer_id=\"NousResearch/Meta-Llama-3-8B-Instruct\"\n</code></pre></p> <p>After making the change, save the file and proceed.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#usage","title":"Usage","text":""},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#loading_multimodal_models_in_indoxminer","title":"Loading Multimodal Models in IndoxMiner","text":"<p>Once installed, you can use the LLaVA or BLIP-2 models from IndoxMiner.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#using_llava_model","title":"Using LLaVA Model","text":"<pre><code>from indoxminer.multimodal.llava_model import LLaVAModel\n\n# Initialize the model\nmodel = LLaVAModel()\n\n# Provide a local image path and a question\nimage_path = \"path/to/your/image.jpg\"\nquestion = \"What is shown in this image?\"\n\n# Generate response\nresponse = model.generate_response(image_path, question)\nprint(\"LLaVA Response:\", response)\n</code></pre>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#using_blip-2_model","title":"Using BLIP-2 Model","text":"<pre><code>from indoxminer.multimodal.blip2_model import BLIP2Model\n\n# Initialize the model\nmodel = BLIP2Model()\n\n# Provide a local image path and a question\nimage_path = \"path/to/your/image.jpg\"\n\n# Generate a caption\ncaption = model.generate_response(image_path)\nprint(\"Generated Caption:\", caption)\n\n# Ask a question about the image\nquestion = \"How many objects are there?\"\nanswer = model.generate_response(image_path, question)\nprint(\"Model Answer:\", answer)\n</code></pre>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#expected_output","title":"Expected Output","text":"<pre><code>Generated Caption: A person sitting on a bench with a dog.\nModel Answer: There are two dogs in the image.\n</code></pre>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#future_work","title":"Future Work","text":"<p>The <code>multimodal/</code> category will expand to support: - Kosmos-2 - GPT-4V - Other vision-language models</p> <p>This makes IndoxMiner a powerful multi-task AI framework combining text, images, and structured data extraction.</p>"},{"location":"indoxMiner/multimodals/multimodal_models_with_indoxminer/#contact","title":"Contact","text":"<p>For issues or contributions, please submit a pull request or open an issue in IndoxMiner\u2019s GitHub repository.</p> <p>\ud83d\ude80 Happy Coding with IndoxMiner! \ud83d\ude80</p>"}]}